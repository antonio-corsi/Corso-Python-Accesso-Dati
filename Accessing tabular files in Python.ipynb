{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d5a227-e219-418b-8d04-f62224e656c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:35:39.679553Z",
     "iopub.status.busy": "2025-10-21T08:35:39.679228Z",
     "iopub.status.idle": "2025-10-21T08:35:39.683523Z",
     "shell.execute_reply": "2025-10-21T08:35:39.683015Z",
     "shell.execute_reply.started": "2025-10-21T08:35:39.679537Z"
    }
   },
   "source": [
    "<font size=\"6\">**Leggere file tabellari in Python**</font><br>\n",
    "\n",
    "> (c) 2025 Antonio Piemontese"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b6f42-42b8-4cc7-94ea-92ec013eb6a5",
   "metadata": {},
   "source": [
    "# I dati in Python (nella Data Science)\n",
    "Nella Data Science spesso siamo interessati:\n",
    "- all'analisi dei **dati passati**, non in tempo reale\n",
    "- all'analisi di un **singolo file**, non del DB\n",
    "\n",
    "Questi dati sono in genere **file tabellari**, cos√¨ chiamati perch√® <u>composti da righe e colonne</u> (2 dimensioni). Sono in genere **creati dagli **utenti**, ricevuti **da altre aziende** o **semplicemente estratti da un DB** (come export).\n",
    "\n",
    "Uno dei formati tabellari pi√π diffusi per l'import di dati in Python √® il formato [**CSV**](https://it.wikipedia.org/wiki/Comma-separated_values). E' un formato praticamente **ubiquo a tutti gli ambienti e tool di *data management* (tabellari)**: excel, google sheet, tutti i DB relazionali, ecc.\n",
    "\n",
    "Ancorch√® in python si possa caricare in memoria **qualsiasi tipo di file** (xml, json, PDF, txt, ecc) ed accedere a **qualsiasi tipo di database** (Oracle, SQLServer, MySQL, ecc), il formato pi√π semplice ed efficiente da caricare in memoria (in un `dataframe` *pandas*) √® il CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae679e73-95ba-4316-836a-6186867428c8",
   "metadata": {},
   "source": [
    "> ‚ÄúUn file CSV o Excel (**file *tabellare***) pu√≤ sembrare una tabella come nel database, ma √® solo un contenitore di dati grezzi.\n",
    "Una tabella SQL invece √® una struttura controllata, con regole, tipi e relazioni, che il database gestisce in modo coerente, sicuro e transazionale.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e0dab-91fb-4180-90ef-6a9ad3acbba1",
   "metadata": {},
   "source": [
    "# Leggere file tabellari tramite librerie specifiche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fabb53-a6c8-4127-902f-e4e40e57bdaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T14:10:19.887968Z",
     "iopub.status.busy": "2025-10-21T14:10:19.887715Z",
     "iopub.status.idle": "2025-10-21T14:10:19.891475Z",
     "shell.execute_reply": "2025-10-21T14:10:19.890966Z",
     "shell.execute_reply.started": "2025-10-21T14:10:19.887952Z"
    }
   },
   "source": [
    "Per leggere file tabellari <u>csv</u> una **prima possibilit√†** √® **la libreria `csv`**, cio√® il **CSV nativo di Python**.<br>\n",
    "La ragione di questa scelta, che tuttavia presenta molti limiti, √® di evitare di caricare in memoria tutto il package *pandas* (pesante) ed evitare il problema delle dipendenze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09fef0bb-3656-4cab-b542-baeef33d366c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T18:36:54.862762Z",
     "iopub.status.busy": "2025-10-21T18:36:54.862549Z",
     "iopub.status.idle": "2025-10-21T18:36:54.866892Z",
     "shell.execute_reply": "2025-10-21T18:36:54.866524Z",
     "shell.execute_reply.started": "2025-10-21T18:36:54.862749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'ID', 'Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education', 'Gender', 'Student', 'Married', 'Ethnicity', 'Balance']\n",
      "['1', '1', '14.891', '3606', '283', '2', '34', '11', ' Male', 'No', 'Yes', 'Caucasian', '333']\n",
      "['2', '2', '106.025', '6645', '483', '3', '82', '15', 'Female', 'Yes', 'Yes', 'Asian', '903']\n",
      "['3', '3', '104.593', '7075', '514', '4', '71', '11', ' Male', 'No', 'No', 'Asian', '580']\n",
      "['4', '4', '148.924', '9504', '681', '3', '36', '11', 'Female', 'No', 'No', 'Asian', '964']\n",
      "['5', '5', '55.882', '4897', '357', '2', '68', '16', ' Male', 'No', 'Yes', 'Caucasian', '331']\n",
      "['6', '6', '80.18', '8047', '569', '4', '77', '10', ' Male', 'No', 'No', 'Caucasian', '1151']\n",
      "['7', '7', '20.996', '3388', '259', '2', '37', '12', 'Female', 'No', 'No', 'African American', '203']\n",
      "['8', '8', '71.408', '7114', '512', '2', '87', '9', ' Male', 'No', 'No', 'Asian', '872']\n",
      "['9', '9', '15.125', '3300', '266', '5', '66', '13', 'Female', 'No', 'No', 'Caucasian', '279']\n",
      "['10', '10', '71.061', '6819', '491', '3', '41', '19', 'Female', 'Yes', 'Yes', 'African American', '1350']\n",
      "['11', '11', '63.095', '8117', '589', '4', '30', '14', ' Male', 'No', 'Yes', 'Caucasian', '1407']\n",
      "['12', '12', '15.045', '1311', '138', '3', '64', '16', ' Male', 'No', 'No', 'Caucasian', '0']\n",
      "['13', '13', '80.616', '5308', '394', '1', '57', '7', 'Female', 'No', 'Yes', 'Asian', '204']\n",
      "['14', '14', '43.682', '6922', '511', '1', '49', '9', ' Male', 'No', 'Yes', 'Caucasian', '1081']\n",
      "['15', '15', '19.144', '3291', '269', '2', '75', '13', 'Female', 'No', 'No', 'African American', '148']\n",
      "['16', '16', '20.089', '2525', '200', '3', '57', '15', 'Female', 'No', 'Yes', 'African American', '0']\n",
      "['17', '17', '53.598', '3714', '286', '3', '73', '17', 'Female', 'No', 'Yes', 'African American', '0']\n",
      "['18', '18', '36.496', '4378', '339', '3', '69', '15', 'Female', 'No', 'Yes', 'Asian', '368']\n",
      "['19', '19', '49.57', '6384', '448', '1', '28', '9', 'Female', 'No', 'Yes', 'Asian', '891']\n",
      "['20', '20', '42.079', '6626', '479', '2', '44', '9', ' Male', 'No', 'No', 'Asian', '1048']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"Credit_ISLR.csv\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        print(row)\n",
    "        if i >= 20:   # stampa solo le prime 20 righe (0‚Äì19)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf0b1aa-8afa-40e2-8549-a4a916c5d04d",
   "metadata": {},
   "source": [
    "E' **leggerissimo**, ma si deve gestire **tutto ‚Äúa mano‚Äù** (tipi, header, encoding...):\n",
    "- gestione dei tipi (tutto √® stringa) - il modulo `csv` non converte automaticamente i tipi: tutto ci√≤ che legge √® una stringa.\n",
    "- header da gestire manualmente - il modulo `csv` non sa da solo se la prima riga √® intestazione o dati.\n",
    "- problemi di encoding-se il file non √® in UTF-8 (es. √® in `latin-1` o `windows-1252`), `open()` dar√† errore o caratteri strani.\n",
    "- separatori diversi (`,` oppure `;` oppure `\\t`) -i file CSV non sono sempre separati da virgole ‚Äî in Italia spesso da `;` o tabulazioni.\n",
    "- gestione di virgolette e caratteri speciali - se un campo contiene una virgola o un ritorno a capo, il parsing pu√≤ rompersi se non si usano i parametri giusti.\n",
    "- dati mancanti (celle vuote) - non esiste un concetto di `NaN`\n",
    "- con milioni di righe, `csv.reader` √® pi√π veloce di pandas in lettura pura, ma non si pu√≤ filtrare, unire, o fare operazioni sui dati facilmente.\n",
    "\n",
    "**In sintesi**:<br>\n",
    "Usare `csv` puro √® come leggere il file ‚Äúa mano‚Äù: abbiamo il pieno controllo ma anche tutto il lavoro √® a carico nostro. *pandas* (o *Polars*) invece capiscono header, tipi, separatori, encoding, missing, ecc. automaticamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c553916-4ccd-46c2-b099-4dd448006f6a",
   "metadata": {},
   "source": [
    "Una **seconda possibilit√†** √® il modulo **`openpyxl`** (per file Excel **.xlsx**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "447e03bf-b3ed-4816-bfd1-830e2031f9b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T18:40:45.158784Z",
     "iopub.status.busy": "2025-10-21T18:40:45.158589Z",
     "iopub.status.idle": "2025-10-21T18:40:45.212259Z",
     "shell.execute_reply": "2025-10-21T18:40:45.211753Z",
     "shell.execute_reply.started": "2025-10-21T18:40:45.158771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Column1', 'ID', 'Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education', 'Gender', 'Student', 'Married', 'Ethnicity', 'Balance')\n",
      "(1, 1, 14891, 3606, 283, 2, 34, 11, ' Male', 'No', 'Yes', 'Caucasian', 333)\n",
      "(2, 2, 106025, 6645, 483, 3, 82, 15, 'Female', 'Yes', 'Yes', 'Asian', 903)\n",
      "(3, 3, 104593, 7075, 514, 4, 71, 11, ' Male', 'No', 'No', 'Asian', 580)\n",
      "(4, 4, 148924, 9504, 681, 3, 36, 11, 'Female', 'No', 'No', 'Asian', 964)\n",
      "(5, 5, 55882, 4897, 357, 2, 68, 16, ' Male', 'No', 'Yes', 'Caucasian', 331)\n",
      "(6, 6, 8018, 8047, 569, 4, 77, 10, ' Male', 'No', 'No', 'Caucasian', 1151)\n",
      "(7, 7, 20996, 3388, 259, 2, 37, 12, 'Female', 'No', 'No', 'African American', 203)\n",
      "(8, 8, 71408, 7114, 512, 2, 87, 9, ' Male', 'No', 'No', 'Asian', 872)\n",
      "(9, 9, 15125, 3300, 266, 5, 66, 13, 'Female', 'No', 'No', 'Caucasian', 279)\n",
      "(10, 10, 71061, 6819, 491, 3, 41, 19, 'Female', 'Yes', 'Yes', 'African American', 1350)\n",
      "(11, 11, 63095, 8117, 589, 4, 30, 14, ' Male', 'No', 'Yes', 'Caucasian', 1407)\n",
      "(12, 12, 15045, 1311, 138, 3, 64, 16, ' Male', 'No', 'No', 'Caucasian', 0)\n",
      "(13, 13, 80616, 5308, 394, 1, 57, 7, 'Female', 'No', 'Yes', 'Asian', 204)\n",
      "(14, 14, 43682, 6922, 511, 1, 49, 9, ' Male', 'No', 'Yes', 'Caucasian', 1081)\n",
      "(15, 15, 19144, 3291, 269, 2, 75, 13, 'Female', 'No', 'No', 'African American', 148)\n",
      "(16, 16, 20089, 2525, 200, 3, 57, 15, 'Female', 'No', 'Yes', 'African American', 0)\n",
      "(17, 17, 53598, 3714, 286, 3, 73, 17, 'Female', 'No', 'Yes', 'African American', 0)\n",
      "(18, 18, 36496, 4378, 339, 3, 69, 15, 'Female', 'No', 'Yes', 'Asian', 368)\n",
      "(19, 19, 4957, 6384, 448, 1, 28, 9, 'Female', 'No', 'Yes', 'Asian', 891)\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "wb = load_workbook(\"Credit_ISLR.xlsx\")\n",
    "ws = wb.active\n",
    "\n",
    "for i, row in enumerate(ws.iter_rows(values_only=True)):\n",
    "    print(row)\n",
    "    if i >= 19:    # indice parte da 0 ‚Üí 0‚Äì19 = 20 righe\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e637d41-8fbc-4d8e-a75d-5525141c1167",
   "metadata": {},
   "source": [
    "Una **terza** possibilit√† √® il modulo `xlrd` o `xlwt` - sempre per file excel\n",
    "- `xlrd` ‚Üí lettura di vecchi .xls (Excel 97-2003)\n",
    "- `xlwt` ‚Üí scrittura di .xls<br>\n",
    "\n",
    "‚ö†Ô∏è Deprecati per `.xlsx`, quindi oggi meno consigliati."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e830e7-b719-4de9-a97a-833f78a85cd2",
   "metadata": {},
   "source": [
    "![](sintesi_formati_tabellari.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4687b0-f8d9-414a-a71e-05b9fc4226a6",
   "metadata": {},
   "source": [
    "# Il caricamento in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d825a892-c123-438e-83f0-ef3b519f894c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T14:46:39.234929Z",
     "iopub.status.busy": "2025-10-21T14:46:39.234603Z",
     "iopub.status.idle": "2025-10-21T14:46:39.238351Z",
     "shell.execute_reply": "2025-10-21T14:46:39.237877Z",
     "shell.execute_reply.started": "2025-10-21T14:46:39.234914Z"
    }
   },
   "source": [
    "Se si vuole solo leggere/scrivere file (tabellari) senza installare grandi librerie, si pu√≤ usare `csv` o `openpyxl`, come visto prima.\n",
    "\n",
    "Altrimenti, si usano i [**dataframe**](https://en.wikipedia.org/wiki/Dataframe) che sono la **struttura dati pi√π utilizzata nella Data Science**.\n",
    "\n",
    "Per caricare un file tabellare in un dataframe si usano queste due funzioni *pandas*:\n",
    "```python\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('dati.csv)\n",
    "    df = pd.read_excel('dati.xlsx')\n",
    "```\n",
    "\n",
    "Se invece si vogliono alternative **pi√π performanti**, oggi si usa molto `cuDF` (con GPU) oppure `Polars`.\n",
    "\n",
    "\n",
    "<p style=\"color:red; font-size:18px; font-weight:bold;\">\n",
    "üö® I file tabellari (ed anche le tabelle SQL che vedremo pi√π avanti) sono in genere caricati in un dataframe pandas üö®\n",
    "</p>\n",
    "Pandas √® comodo ma non √® l‚Äôunico modo per importare file tabellari in python.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad194c6-cfc5-4629-acff-81f2ed49d549",
   "metadata": {},
   "source": [
    "# Excel o csv?\n",
    "Qual √® il formato migliore per importare file tabellari? Excel o csv?<br>\n",
    "Dipende dallo scopo e dal contesto, ma **nella maggior parte dei casi CSV √® pi√π efficiente, trasparente e robusto, mentre Excel √® pi√π comodo per l‚Äôutente umano**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d64c30a-84f9-4f1d-a18f-842a6a4b31d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Facciamo un confronto **CSV vs Excel** dal punto di vista <u>tecnico</u> e da quello <u>umano</u>.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Dal punto di vista tecnico (*pandas*)**\n",
    "\n",
    "üü© **CSV: √® il formato ‚Äúnativo‚Äù per pandas**\n",
    "- √® pi√π leggero da leggere e scrivere:\n",
    "```python\n",
    "    df = pd.read_csv(\"dati.csv\")\n",
    "```\n",
    "- pandas lo apre molto pi√π velocemente (soprattutto file grandi).\n",
    "- nessuna dipendenza esterna (solo Python standard).\n",
    "- perfetto per scambi tra sistemi o integrazione con altri linguaggi.\n",
    "- √® testuale e trasparente ‚Üí lo si pu√≤ aprire anche con un editor o versionare su Git.\n",
    "\n",
    "‚ö†Ô∏è Svantaggi:\n",
    "- perde formattazioni, formule, fogli multipli (ad esempio se convertito da excel)\n",
    "- non ha metadati (tipi, date, ecc.), quindi pandas deve ‚Äúindovinarli‚Äù (**inferirli**).\n",
    "\n",
    "üü® **Excel (XLS / XLSX): comodo ma pi√π complesso**\n",
    "- supporta fogli multipli, celle formattate, tipi pi√π espliciti.\n",
    "- ottimo per file provenienti da utenti umani o report aziendali.\n",
    "\n",
    "```python\n",
    "    df = pd.read_excel(\"dati.xlsx\")\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è Svantaggi:\n",
    "- meno stabile su grandi volumi (>100 000 righe).\n",
    "- usa librerie esterne (`openpyxl`, `xlrd`, `pyxlsb` ecc.) - come visto prima\n",
    "- pi√π lento da leggere e scrivere.\n",
    "- gli errori di formattazione (celle unite, righe vuote, formule, ecc.) spesso creano problemi.\n",
    "- meno adatto all‚Äôautomazione massiva (batch ETL, pipeline, ecc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5d500-d1ee-4b7d-bb33-87f8d95e15d5",
   "metadata": {},
   "source": [
    "---\n",
    "**\"Librerie esterne\"**: cosa si intende?<br>\n",
    "La funzione `pd.read_excel()` √® integrata in pandas, ma non fa tutto da sola: per funzionare **ha bisogno di librerie esterne** che gestiscono concretamente il formato Excel.<br>\n",
    "Cio√®, come funziona davvero `pd.read_excel()`?<br>\n",
    "Quando si chiama:\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_excel(\"dati.xlsx\")\n",
    "```\n",
    "pandas:\n",
    "- riconosce il formato del file (es. `.xls`, `.xlsx`, `.xlsb`)\n",
    "- usa un ‚Äúmotore‚Äù esterno (engine) per leggere i dati\n",
    "- trasforma ci√≤ che legge in un `DataFrame`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693a008-8fd0-47b2-ba08-c65f68a81034",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T15:06:22.273393Z",
     "iopub.status.busy": "2025-10-21T15:06:22.273077Z",
     "iopub.status.idle": "2025-10-21T15:06:22.291626Z",
     "shell.execute_reply": "2025-10-21T15:06:22.291290Z",
     "shell.execute_reply.started": "2025-10-21T15:06:22.273373Z"
    }
   },
   "source": [
    "**2. Confronto dal punto di vista del ‚Äúdata pipeline‚Äù**:\n",
    "\n",
    "![](cfr_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e55ed59-708f-4da5-913c-3c84dd371dde",
   "metadata": {},
   "source": [
    "**3. Performance: confronto indicativo**:<br>\n",
    "![](performance_csv_excel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cfda21-e530-405e-8f22-4d3672241961",
   "metadata": {},
   "source": [
    "**4. In pratica**:\n",
    "üëâ Se il file **arriva da un sistema o un processo automatico** --> scegliere CSV.\n",
    "üëâ Se il file **arriva da un collega o un cliente che lavora in Excel** --> usa Excel, pulirlo e poi convertirlo in CSV o Parquet.\n",
    "\n",
    "üí° Molti flussi aziendali fanno proprio cos√¨:\n",
    "1. `read_excel()`<br>\n",
    "2.  **pulizia dati in *pandas***<br>\n",
    "3. `to_csv()` o `to_parquet()` per uso interno / storage efficiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e00a387",
   "metadata": {},
   "source": [
    "# L'importanza e la diffusione del formato *csv*\n",
    "Vediamo pi√π in dettaglio **perch√® il formato CSV √® cos√¨ ubiquo nel mondo dei dati**. Non c'√® praticamente ambiente di *data management* (excel, google sheet, DB relazionali, ecc) che non permetta import ed export di file csv.\n",
    "\n",
    "![](importanza_csv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4810542f",
   "metadata": {},
   "source": [
    "# Ragioni tecniche della diffusione di csv\n",
    "Ci sono diversi **motivi tecnici molto concreti** che spiegano perch√© il formato CSV √® cos√¨ onnipresente nel mondo dei dati.<br>\n",
    "Ecco una tabella riassuntiva chiara e tecnica:\n",
    "\n",
    "![](diffusione_csv.png)\n",
    "\n",
    "üí° In breve:<br>\n",
    "Il CSV √® il **‚Äúminimo comune denominatore‚Äù dei dati tabellari**: semplice, interoperabile, senza dipendenze e compatibile con tutto ‚Äî da Excel a Spark.<br>\n",
    "Non √® perfetto (niente tipi, schema, compressione o metadati), ma proprio **la sua povert√† strutturale √® la sua forza**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d45255-be27-4a5d-9932-790901b179e2",
   "metadata": {},
   "source": [
    "# Leggere file CSV in pandas\n",
    "\n",
    "Come detto, nella Data Science, spesso, NON siamo interessati alla fotografia dei dati in tempo reale. In genere sia le analisi (EDA) che i modelli (predittivi) - i due obiettivi tipici del ML/AI - sono fatti su dati **passati (congelati)**, sia perch√® manca l'interesse sui dati recentissimi sia perch√® i dati in input ai modelli devono essere preprocessati (controllo qualit√†, gestione outlier, gestione MV, gestione duplicati, standardizzazione, ecc). Quindi, sebbene Python/pandas siano in gradi di accedere **direttamente** a tabelle SQL remote (tramite i metodi `pd.read_sql_query` per le query e `pd.read_sql_table` per il download dell'intera tabella), tuttavia √® molto pi√π veloce caricare i dati da un file esterno locale, che pu√≤ essere di vari formati (csv, json, parquest, ecc).\n",
    "\n",
    "## 3 note tecniche sul formato CSV\n",
    "\n",
    "* i due argomenti principali del metodo pandas `read_csv` sono  `sep`, che indica il carattere usato nel file per \"separare\" le colonne (in genere √® \",\" oppure il \";\") e `header`, che indica la presenza (e l'eventuale numero) di righe di heading (intestazione).\n",
    "* ci sono diversi formati csv disponibili da excel; occorre scegliere quello indicato in figura sottostante con la freccia rossa\n",
    "  \n",
    "  ![](tipi_CSV.png)\n",
    "* [pro e contro](https://towardsdatascience.com/why-i-stopped-dumping-dataframes-to-a-csv-and-why-you-should-too-c0954c410f8f) del formato csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae4ec6c-ed02-4ecc-b78c-b8feb1efda58",
   "metadata": {},
   "source": [
    "Carichiamo in *pandas* il famoso file bancario `Credit_ISLR`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f8f076-dc8b-488e-8659-e63b4e9f43d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:43:33.738874Z",
     "iopub.status.busy": "2025-10-21T08:43:33.738657Z",
     "iopub.status.idle": "2025-10-21T08:43:33.761516Z",
     "shell.execute_reply": "2025-10-21T08:43:33.761072Z",
     "shell.execute_reply.started": "2025-10-21T08:43:33.738860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.891</td>\n",
       "      <td>3606</td>\n",
       "      <td>283</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>106.025</td>\n",
       "      <td>6645</td>\n",
       "      <td>483</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Asian</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>104.593</td>\n",
       "      <td>7075</td>\n",
       "      <td>514</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>148.924</td>\n",
       "      <td>9504</td>\n",
       "      <td>681</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>55.882</td>\n",
       "      <td>4897</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>396</td>\n",
       "      <td>396</td>\n",
       "      <td>12.096</td>\n",
       "      <td>4100</td>\n",
       "      <td>307</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>397</td>\n",
       "      <td>397</td>\n",
       "      <td>13.364</td>\n",
       "      <td>3838</td>\n",
       "      <td>296</td>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "      <td>17</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>African American</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>398</td>\n",
       "      <td>398</td>\n",
       "      <td>57.872</td>\n",
       "      <td>4171</td>\n",
       "      <td>321</td>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>399</td>\n",
       "      <td>399</td>\n",
       "      <td>37.728</td>\n",
       "      <td>2525</td>\n",
       "      <td>192</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>13</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>18.701</td>\n",
       "      <td>5524</td>\n",
       "      <td>415</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows √ó 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0   ID   Income  Limit  Rating  Cards  Age  Education  Gender  \\\n",
       "0             1    1   14.891   3606     283      2   34         11    Male   \n",
       "1             2    2  106.025   6645     483      3   82         15  Female   \n",
       "2             3    3  104.593   7075     514      4   71         11    Male   \n",
       "3             4    4  148.924   9504     681      3   36         11  Female   \n",
       "4             5    5   55.882   4897     357      2   68         16    Male   \n",
       "..          ...  ...      ...    ...     ...    ...  ...        ...     ...   \n",
       "395         396  396   12.096   4100     307      3   32         13    Male   \n",
       "396         397  397   13.364   3838     296      5   65         17    Male   \n",
       "397         398  398   57.872   4171     321      5   67         12  Female   \n",
       "398         399  399   37.728   2525     192      1   44         13    Male   \n",
       "399         400  400   18.701   5524     415      5   64          7  Female   \n",
       "\n",
       "    Student Married         Ethnicity  Balance  \n",
       "0        No     Yes         Caucasian      333  \n",
       "1       Yes     Yes             Asian      903  \n",
       "2        No      No             Asian      580  \n",
       "3        No      No             Asian      964  \n",
       "4        No     Yes         Caucasian      331  \n",
       "..      ...     ...               ...      ...  \n",
       "395      No     Yes         Caucasian      560  \n",
       "396      No      No  African American      480  \n",
       "397      No     Yes         Caucasian      138  \n",
       "398      No     Yes         Caucasian        0  \n",
       "399      No      No             Asian      966  \n",
       "\n",
       "[400 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_credit = pd.read_csv(\"Credit_ISLR.csv\",header=0)\n",
    "df_credit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f89a5-9591-40a8-8bfb-75d1643f41ae",
   "metadata": {},
   "source": [
    "Come si vede, la funzione *pandas* `read_csv` ha creato automaticamente un **indice** numerico, sicch√® l'indice originario `ID` √® ora <u>ridondante</u>, ed ha aggiunto una colonna `Unnamed: 0` (vedremo pi√π avanti perch√®). E' bene cancellarle entrambe perch√® inutili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adedb20e-72fa-4c93-95ed-4a48b2853d83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:51:08.184431Z",
     "iopub.status.busy": "2025-10-21T08:51:08.183921Z",
     "iopub.status.idle": "2025-10-21T08:51:08.191669Z",
     "shell.execute_reply": "2025-10-21T08:51:08.191323Z",
     "shell.execute_reply.started": "2025-10-21T08:51:08.184414Z"
    }
   },
   "outputs": [],
   "source": [
    "df_credit.drop(columns=['Unnamed: 0', 'ID'], inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64d44fbb-2b10-4079-9d36-05411f3ce003",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:51:24.310829Z",
     "iopub.status.busy": "2025-10-21T08:51:24.310609Z",
     "iopub.status.idle": "2025-10-21T08:51:24.318242Z",
     "shell.execute_reply": "2025-10-21T08:51:24.317874Z",
     "shell.execute_reply.started": "2025-10-21T08:51:24.310813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.891</td>\n",
       "      <td>3606</td>\n",
       "      <td>283</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106.025</td>\n",
       "      <td>6645</td>\n",
       "      <td>483</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Asian</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104.593</td>\n",
       "      <td>7075</td>\n",
       "      <td>514</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148.924</td>\n",
       "      <td>9504</td>\n",
       "      <td>681</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.882</td>\n",
       "      <td>4897</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Income  Limit  Rating  Cards  Age  Education  Gender Student Married  \\\n",
       "0   14.891   3606     283      2   34         11    Male      No     Yes   \n",
       "1  106.025   6645     483      3   82         15  Female     Yes     Yes   \n",
       "2  104.593   7075     514      4   71         11    Male      No      No   \n",
       "3  148.924   9504     681      3   36         11  Female      No      No   \n",
       "4   55.882   4897     357      2   68         16    Male      No     Yes   \n",
       "\n",
       "   Ethnicity  Balance  \n",
       "0  Caucasian      333  \n",
       "1      Asian      903  \n",
       "2      Asian      580  \n",
       "3      Asian      964  \n",
       "4  Caucasian      331  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e32b2-e3fd-48df-a65f-1fd81333037d",
   "metadata": {},
   "source": [
    "## pandas √® pesante?\n",
    "\n",
    "‚öôÔ∏è Cosa significa che ‚Äúpandas √® pesante‚Äù? Lo √® in vari sensi:\n",
    "\n",
    "1Ô∏è‚É£ **Dimensione e complessit√†**\n",
    "- Pandas non √® una piccola libreria:<br>\n",
    "    - l‚Äôinstallazione porta con s√© molte dipendenze:\n",
    "    - `numpy`, `dateutil`, `pytz`, `tzdata`, `matplotlib`, `openpyxl`, `xlrd`, ecc.\n",
    "- La dimensione del pacchetto √® di **decine di MB**.\n",
    "- Il caricamento in memoria all‚Äôavvio √® **pi√π lento** rispetto a un semplice `import csv`.\n",
    "\n",
    "üëâ Quindi per uno script che deve solo leggere un file CSV e stampare 10 righe, importare tutto pandas √® come usare un camion per consegnare una lettera.\n",
    "\n",
    "2Ô∏è‚É£ **Dipendenze esterne**<br>\n",
    "\n",
    "Pandas, per funzionare bene con molti formati, usa librerie esterne (come detto prima):\n",
    "- `openpyxl` per i file *.xlsx*\n",
    "- `xlrd` per i *.xls*\n",
    "- `pyarrow` per i file *.parquet*\n",
    "- `numexpr` per operazioni numeriche\n",
    "- `matplotlib per .plot()`<br>\n",
    "\n",
    "üëâ Queste dipendenze sono comode in un ambiente data science,\n",
    "ma eccessive in uno script di sistema o un microservizio.\n",
    "\n",
    "3Ô∏è‚É£ **Impatto su ambienti piccoli**<br>\n",
    "\n",
    "In contesti come:\n",
    "- microservizi Docker\n",
    "- script CLI leggeri\n",
    "- funzioni serverless (AWS Lambda, GCP Functions)\n",
    "- sistemi con vincoli di memoria\n",
    "\n",
    "importare pandas pu√≤:\n",
    "- rallentare l‚Äôavvio dello script,\n",
    "- aumentare l‚Äôimmagine Docker di decine o centinaia di MB,\n",
    "- portare a incompatibilit√† o tempi di cold start lunghi.\n",
    "\n",
    "\n",
    "**Ecco quindi perch√© a volte si vuole ‚Äúevitare pandas‚Äù**:\n",
    "- Non ci servono le sue funzioni di analisi avanzata.\n",
    "- Si vuole solo leggere un file e scorrerne le righe.\n",
    "- Si vuole ridurre dipendenze e tempo di startup.\n",
    "\n",
    "In quel caso ha pi√π senso usare:\n",
    "```python\n",
    "import csv      # per file CSV\n",
    "import openpyxl # per file Excel moderni\n",
    "```\n",
    "\n",
    "che sono moduli molto pi√π leggeri.\n",
    "\n",
    "üß† Metafora<br>\n",
    "Pandas √® come Excel o un gestionale completo.<br>\n",
    "Se ti serve solo aprire un file di testo e stampare due colonne, ti basta il Blocco Note."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf41df-1eaf-473d-849a-d7eddca92f06",
   "metadata": {},
   "source": [
    "## Convertire file Excel in formato CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d281f-0e94-42e6-956f-122e767c9aaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:43:41.545329Z",
     "iopub.status.busy": "2025-10-21T08:43:41.545074Z",
     "iopub.status.idle": "2025-10-21T08:43:41.549360Z",
     "shell.execute_reply": "2025-10-21T08:43:41.548809Z",
     "shell.execute_reply.started": "2025-10-21T08:43:41.545312Z"
    }
   },
   "source": [
    "Al contrario, per visualizzare un file CSV **nel formato Excel standard** si pu√≤ fare cos√¨ (ci sono anche altri modi):\n",
    "* aprire un **nuovo file**\n",
    "* scheda `Dati`\n",
    "* bottone in alto a sx `Recupera dati`\n",
    "* `Da file` --> `Da testo/CSV`\n",
    "* nella preview fare le eventuali modifiche (al caricamento) e poi premere il bottone \"Carica\" in basso a dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b439110",
   "metadata": {},
   "source": [
    "> Se un file nasce in excel e poi viene convertito in csv, salvato e in un secondo momento riaperto in excel, excel lo visualizza come excel (cio√® con la griglia). Come mai?<br>\n",
    "\n",
    "Excel non apre il CSV come file Excel vero e proprio ‚Äî lo interpreta come una tabella testuale, e lo visualizza nella stessa interfaccia grafica (celle, righe, colonne).<br>\n",
    "Questo fa s√¨ che ‚Äúsembri Excel‚Äù, ma in realt√† il file non contiene nessuna delle informazioni tipiche del formato `.xlsx`:\n",
    "- niente formattazione,\n",
    "- niente formule,\n",
    "- niente fogli multipli,\n",
    "- niente tipi di dato complessi.\n",
    "\n",
    "Excel semplicemente mostra **una griglia sopra un file di testo**.<br>\n",
    "√à un po‚Äô come aprire un file `.txt` in Word: il contenuto √® testo, ma l‚Äôambiente √® Word, con tutto il suo aspetto ‚Äúricco‚Äù.\n",
    "\n",
    "üí° Riassunto in una frase:<br>\n",
    "> Excel riconosce l‚Äôestensione `.csv`, ne interpreta i dati in forma tabellare e li visualizza nella sua interfaccia ‚Äî ma il file rimane puro testo strutturato, non un vero foglio Excel.\n",
    "\n",
    "**Obiezione**: se apro in excel un file csv generato indipendentemente da excel, la griglia non √® applicata.<br>\n",
    "Vero, Excel non legge il CSV ‚Äúintelligentemente‚Äù in base al contenuto, ma usa **impostazioni locali** del sistema operativo (la ‚Äúimpostazione paese‚Äù o ‚Äúseparatore di elenco‚Äù di Windows).<br>\n",
    "Per esempio:\n",
    "- in Italia, il separatore di elenco predefinito √® `;` (punto e virgola);\n",
    "- in USA/UK, √® `,` (virgola).\n",
    "\n",
    "Quindi:\n",
    "- se il CSV viene da Excel, esso usa lo stesso separatore della tua configurazione regionale ‚Üí Excel lo ‚Äúriconosce‚Äù e mostra la tabella.\n",
    "- Se invece il CSV viene da un altro programma (es. Python to_csv(), MySQL, o sistemi internazionali) che usa la virgola, Excel non sa che √® un separatore e ti mette tutto in un‚Äôunica cella.\n",
    "\n",
    "L‚Äôutente pu√≤ correggere manualmente:\n",
    "\n",
    "    `Dati ‚Üí Da testo/CSV ‚Üí scegli delimitatore corretto (virgola, punto e virgola, tab)`\n",
    "\n",
    "oppure cambiare l‚Äôimpostazione regionale del sistema.\n",
    "\n",
    "üí° In sintesi:\n",
    "\n",
    "> Excel ‚Äúinterpreta‚Äù come tabella solo i CSV che rispettano le sue convenzioni regionali (separatore e encoding).<br>\n",
    "> Se il file √® generato altrove con altri standard, Excel lo mostra come testo in una colonna.\n",
    "\n",
    "![](apertura_csv.png)\n",
    "\n",
    "üí¨ **Spiegazione breve**\n",
    "\n",
    "Quando Excel mostra tutto in una sola colonna, √® perch√©:\n",
    "- il delimitatore del file (es. `,`)\n",
    "‚â†\n",
    "- dal separatore di elenco di sistema (es. `;` in Italia).\n",
    "\n",
    "üëâ Soluzione: usare l‚Äôimportazione guidata, che ti fa scegliere manualmente il delimitatore e l‚Äôencoding (UTF-8 consigliato).<br>\n",
    "Dopo questa scelta, Excel mostra subito la griglia corretta e puoi salvare come .xlsx se vuoi mantenerla stabile.\n",
    "\n",
    "üí° Consiglio pratico per chi lavora spesso con Python o CSV esterni:\n",
    "- usa `df.to_csv('file.csv', sep=';', encoding='utf-8-sig')`\n",
    "‚Üí cos√¨ Excel (versione italiana) lo apre gi√† ‚Äúa griglia‚Äù senza interventi manuali.\n",
    "- `utf-8-sig` √® una variante dell‚Äôencoding UTF-8 molto usata proprio per far s√¨ che Excel riconosca correttamente i CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c5549a-b79e-44d5-b83a-39cad9e4ea90",
   "metadata": {},
   "source": [
    "## Gli argomenti di input della funzione `pd.read_csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0853b04-c48a-41cb-a7b4-4ffa761e149a",
   "metadata": {},
   "source": [
    "Abbiamo gi√† citato i due **fondamentali argomenti in input** della funzione pandas `read_csv`: `sep` e `header`. Sono **critici:\n",
    "- `header=1` dice a pandas di saltare la prima riga del file e usare la seconda riga come intestazione.<br>\n",
    "Se i nostri CSV non hanno due righe di intestazione, oppure se il primo file ha un formato leggermente diverso dagli altri (spazi, separatore, BOM, ecc.), pandas **interpreter√† in modo sbagliato le colonne**\n",
    "- `sep=';'` scassa tutti i dati (se il file √® effettivamente CSV!<br>\n",
    "Attenzione: molti \"file CSV\" hanno sep=';'!\n",
    "\n",
    "In realt√† la funzione `read_csv` ha **molti altri argomenti in input**, come si evince dall'help della cella successiva - pi√π avanti approfondiremo i **principali**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1ea1ed4-0b0b-42df-a17a-6561d85f48b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:43:37.520188Z",
     "iopub.status.busy": "2025-10-21T08:43:37.519710Z",
     "iopub.status.idle": "2025-10-21T08:43:37.524355Z",
     "shell.execute_reply": "2025-10-21T08:43:37.524052Z",
     "shell.execute_reply.started": "2025-10-21T08:43:37.520167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_csv in module pandas.io.parsers.readers:\n",
      "\n",
      "read_csv(\n",
      "    filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]',\n",
      "    *,\n",
      "    sep: 'str | None | lib.NoDefault' = <no_default>,\n",
      "    delimiter: 'str | None | lib.NoDefault' = None,\n",
      "    header: \"int | Sequence[int] | None | Literal['infer']\" = 'infer',\n",
      "    names: 'Sequence[Hashable] | None | lib.NoDefault' = <no_default>,\n",
      "    index_col: 'IndexLabel | Literal[False] | None' = None,\n",
      "    usecols: 'UsecolsArgType' = None,\n",
      "    dtype: 'DtypeArg | None' = None,\n",
      "    engine: 'CSVEngine | None' = None,\n",
      "    converters: 'Mapping[Hashable, Callable] | None' = None,\n",
      "    true_values: 'list | None' = None,\n",
      "    false_values: 'list | None' = None,\n",
      "    skipinitialspace: 'bool' = False,\n",
      "    skiprows: 'list[int] | int | Callable[[Hashable], bool] | None' = None,\n",
      "    skipfooter: 'int' = 0,\n",
      "    nrows: 'int | None' = None,\n",
      "    na_values: 'Hashable | Iterable[Hashable] | Mapping[Hashable, Iterable[Hashable]] | None' = None,\n",
      "    keep_default_na: 'bool' = True,\n",
      "    na_filter: 'bool' = True,\n",
      "    verbose: 'bool | lib.NoDefault' = <no_default>,\n",
      "    skip_blank_lines: 'bool' = True,\n",
      "    parse_dates: 'bool | Sequence[Hashable] | None' = None,\n",
      "    infer_datetime_format: 'bool | lib.NoDefault' = <no_default>,\n",
      "    keep_date_col: 'bool | lib.NoDefault' = <no_default>,\n",
      "    date_parser: 'Callable | lib.NoDefault' = <no_default>,\n",
      "    date_format: 'str | dict[Hashable, str] | None' = None,\n",
      "    dayfirst: 'bool' = False,\n",
      "    cache_dates: 'bool' = True,\n",
      "    iterator: 'bool' = False,\n",
      "    chunksize: 'int | None' = None,\n",
      "    compression: 'CompressionOptions' = 'infer',\n",
      "    thousands: 'str | None' = None,\n",
      "    decimal: 'str' = '.',\n",
      "    lineterminator: 'str | None' = None,\n",
      "    quotechar: 'str' = '\"',\n",
      "    quoting: 'int' = 0,\n",
      "    doublequote: 'bool' = True,\n",
      "    escapechar: 'str | None' = None,\n",
      "    comment: 'str | None' = None,\n",
      "    encoding: 'str | None' = None,\n",
      "    encoding_errors: 'str | None' = 'strict',\n",
      "    dialect: 'str | csv.Dialect | None' = None,\n",
      "    on_bad_lines: 'str' = 'error',\n",
      "    delim_whitespace: 'bool | lib.NoDefault' = <no_default>,\n",
      "    low_memory: 'bool' = True,\n",
      "    memory_map: 'bool' = False,\n",
      "    float_precision: \"Literal['high', 'legacy'] | None\" = None,\n",
      "    storage_options: 'StorageOptions | None' = None,\n",
      "    dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>\n",
      ") -> 'DataFrame | TextFileReader'\n",
      "    Read a comma-separated values (csv) file into DataFrame.\n",
      "\n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "\n",
      "    Additional help can be found in the online docs for\n",
      "    `IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : str, path object or file-like object\n",
      "        Any valid string path is acceptable. The string could be a URL. Valid\n",
      "        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
      "        expected. A local file could be: file://localhost/path/to/table.csv.\n",
      "\n",
      "        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "\n",
      "        By file-like object, we refer to objects with a ``read()`` method, such as\n",
      "        a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
      "    sep : str, default ','\n",
      "        Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n",
      "        C engine cannot automatically detect\n",
      "        the separator, but the Python parsing engine can, meaning the latter will\n",
      "        be used and automatically detect the separator from only the first valid\n",
      "        row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n",
      "        In addition, separators longer than 1 character and different from\n",
      "        ``'\\s+'`` will be interpreted as regular expressions and will also force\n",
      "        the use of the Python parsing engine. Note that regex delimiters are prone\n",
      "        to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
      "    delimiter : str, optional\n",
      "        Alias for ``sep``.\n",
      "    header : int, Sequence of int, 'infer' or None, default 'infer'\n",
      "        Row number(s) containing column labels and marking the start of the\n",
      "        data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n",
      "        are passed the behavior is identical to ``header=0`` and column\n",
      "        names are inferred from the first line of the file, if column\n",
      "        names are passed explicitly to ``names`` then the behavior is identical to\n",
      "        ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "        replace existing names. The header can be a list of integers that\n",
      "        specify row locations for a :class:`~pandas.MultiIndex` on the columns\n",
      "        e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n",
      "        skipped (e.g. 2 in this example is skipped). Note that this\n",
      "        parameter ignores commented lines and empty lines if\n",
      "        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
      "        data rather than the first line of the file.\n",
      "    names : Sequence of Hashable, optional\n",
      "        Sequence of column labels to apply. If the file contains a header row,\n",
      "        then you should explicitly pass ``header=0`` to override the column names.\n",
      "        Duplicates in this list are not allowed.\n",
      "    index_col : Hashable, Sequence of Hashable or False, optional\n",
      "      Column(s) to use as row label(s), denoted either by column labels or column\n",
      "      indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n",
      "      will be formed for the row labels.\n",
      "\n",
      "      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
      "      column as the index, e.g., when you have a malformed file with delimiters at\n",
      "      the end of each line.\n",
      "    usecols : Sequence of Hashable or Callable, optional\n",
      "        Subset of columns to select, denoted either by column labels or column indices.\n",
      "        If list-like, all elements must either\n",
      "        be positional (i.e. integer indices into the document columns) or strings\n",
      "        that correspond to column names provided either by the user in ``names`` or\n",
      "        inferred from the document header row(s). If ``names`` are given, the document\n",
      "        header row(s) are not taken into account. For example, a valid list-like\n",
      "        ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
      "        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "        To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n",
      "        preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n",
      "        for columns in ``['foo', 'bar']`` order or\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "        for ``['bar', 'foo']`` order.\n",
      "\n",
      "        If callable, the callable function will be evaluated against the column\n",
      "        names, returning names where the callable function evaluates to ``True``. An\n",
      "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "        parsing time and lower memory usage.\n",
      "    dtype : dtype or dict of {Hashable : dtype}, optional\n",
      "        Data type(s) to apply to either the whole dataset or individual columns.\n",
      "        E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n",
      "        Use ``str`` or ``object`` together with suitable ``na_values`` settings\n",
      "        to preserve and not interpret ``dtype``.\n",
      "        If ``converters`` are specified, they will be applied INSTEAD\n",
      "        of ``dtype`` conversion.\n",
      "\n",
      "        .. versionadded:: 1.5.0\n",
      "\n",
      "            Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n",
      "            the default determines the ``dtype`` of the columns which are not explicitly\n",
      "            listed.\n",
      "    engine : {'c', 'python', 'pyarrow'}, optional\n",
      "        Parser engine to use. The C and pyarrow engines are faster, while the python engine\n",
      "        is currently more feature-complete. Multithreading is currently only supported by\n",
      "        the pyarrow engine.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "            The 'pyarrow' engine was added as an *experimental* engine, and some features\n",
      "            are unsupported, or may not work correctly, with this engine.\n",
      "    converters : dict of {Hashable : Callable}, optional\n",
      "        Functions for converting values in specified columns. Keys can either\n",
      "        be column labels or column indices.\n",
      "    true_values : list, optional\n",
      "        Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\n",
      "    false_values : list, optional\n",
      "        Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\n",
      "    skipinitialspace : bool, default False\n",
      "        Skip spaces after delimiter.\n",
      "    skiprows : int, list of int or Callable, optional\n",
      "        Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n",
      "        at the start of the file.\n",
      "\n",
      "        If callable, the callable function will be evaluated against the row\n",
      "        indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n",
      "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "    skipfooter : int, default 0\n",
      "        Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\n",
      "    nrows : int, optional\n",
      "        Number of rows of file to read. Useful for reading pieces of large files.\n",
      "    na_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n",
      "        Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n",
      "        per-column ``NA`` values.  By default the following values are interpreted as\n",
      "        ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n",
      "        \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n",
      "        \"n/a\", \"nan\", \"null \".\n",
      "\n",
      "    keep_default_na : bool, default True\n",
      "        Whether or not to include the default ``NaN`` values when parsing the data.\n",
      "        Depending on whether ``na_values`` is passed in, the behavior is as follows:\n",
      "\n",
      "        * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n",
      "          is appended to the default ``NaN`` values used for parsing.\n",
      "        * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n",
      "          the default ``NaN`` values are used for parsing.\n",
      "        * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n",
      "          the ``NaN`` values specified ``na_values`` are used for parsing.\n",
      "        * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n",
      "          strings will be parsed as ``NaN``.\n",
      "\n",
      "        Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n",
      "        ``na_values`` parameters will be ignored.\n",
      "    na_filter : bool, default True\n",
      "        Detect missing value markers (empty strings and the value of ``na_values``). In\n",
      "        data without any ``NA`` values, passing ``na_filter=False`` can improve the\n",
      "        performance of reading a large file.\n",
      "    verbose : bool, default False\n",
      "        Indicate number of ``NA`` values placed in non-numeric columns.\n",
      "\n",
      "        .. deprecated:: 2.2.0\n",
      "    skip_blank_lines : bool, default True\n",
      "        If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\n",
      "    parse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n",
      "        The behavior is as follows:\n",
      "\n",
      "        * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n",
      "          ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n",
      "        * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n",
      "          each as a separate date column.\n",
      "        * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n",
      "          as a single date column. Values are joined with a space before parsing.\n",
      "        * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n",
      "          result 'foo'. Values are joined with a space before parsing.\n",
      "\n",
      "        If a column or index cannot be represented as an array of ``datetime``,\n",
      "        say because of an unparsable value or a mixture of timezones, the column\n",
      "        or index will be returned unaltered as an ``object`` data type. For\n",
      "        non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n",
      "        :func:`~pandas.read_csv`.\n",
      "\n",
      "        Note: A fast-path exists for iso8601-formatted dates.\n",
      "    infer_datetime_format : bool, default False\n",
      "        If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n",
      "        format of the ``datetime`` strings in the columns, and if it can be inferred,\n",
      "        switch to a faster method of parsing them. In some cases this can increase\n",
      "        the parsing speed by 5-10x.\n",
      "\n",
      "        .. deprecated:: 2.0.0\n",
      "            A strict version of this argument is now the default, passing it has no effect.\n",
      "\n",
      "    keep_date_col : bool, default False\n",
      "        If ``True`` and ``parse_dates`` specifies combining multiple columns then\n",
      "        keep the original columns.\n",
      "    date_parser : Callable, optional\n",
      "        Function to use for converting a sequence of string columns to an array of\n",
      "        ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "        conversion. pandas will try to call ``date_parser`` in three different ways,\n",
      "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "        (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n",
      "        string values from the columns defined by ``parse_dates`` into a single array\n",
      "        and pass that; and 3) call ``date_parser`` once for each row using one or\n",
      "        more strings (corresponding to the columns defined by ``parse_dates``) as\n",
      "        arguments.\n",
      "\n",
      "        .. deprecated:: 2.0.0\n",
      "           Use ``date_format`` instead, or read in as ``object`` and then apply\n",
      "           :func:`~pandas.to_datetime` as-needed.\n",
      "    date_format : str or dict of column -> format, optional\n",
      "        Format to use for parsing dates when used in conjunction with ``parse_dates``.\n",
      "        The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n",
      "        `strftime documentation\n",
      "        <https://docs.python.org/3/library/datetime.html\n",
      "        #strftime-and-strptime-behavior>`_ for more information on choices, though\n",
      "        note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n",
      "        You can also pass:\n",
      "\n",
      "        - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n",
      "            time string (not necessarily in exactly the same format);\n",
      "        - \"mixed\", to infer the format for each element individually. This is risky,\n",
      "            and you should probably use it along with `dayfirst`.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "    dayfirst : bool, default False\n",
      "        DD/MM format dates, international and European format.\n",
      "    cache_dates : bool, default True\n",
      "        If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n",
      "        conversion. May produce significant speed-up when parsing duplicate\n",
      "        date strings, especially ones with timezone offsets.\n",
      "\n",
      "    iterator : bool, default False\n",
      "        Return ``TextFileReader`` object for iteration or getting chunks with\n",
      "        ``get_chunk()``.\n",
      "    chunksize : int, optional\n",
      "        Number of lines to read from the file per chunk. Passing a value will cause the\n",
      "        function to return a ``TextFileReader`` object for iteration.\n",
      "        See the `IO Tools docs\n",
      "        <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "        for more information on ``iterator`` and ``chunksize``.\n",
      "\n",
      "    compression : str or dict, default 'infer'\n",
      "        For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n",
      "        path-like, then detect compression from the following extensions: '.gz',\n",
      "        '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      "        (otherwise no compression).\n",
      "        If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n",
      "        Set to ``None`` for no decompression.\n",
      "        Can also be a dict with key ``'method'`` set\n",
      "        to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
      "        other key-value pairs are forwarded to\n",
      "        ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      "        ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n",
      "        ``tarfile.TarFile``, respectively.\n",
      "        As an example, the following could be passed for Zstandard decompression using a\n",
      "        custom compression dictionary:\n",
      "        ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n",
      "\n",
      "        .. versionadded:: 1.5.0\n",
      "            Added support for `.tar` files.\n",
      "\n",
      "        .. versionchanged:: 1.4.0 Zstandard support.\n",
      "\n",
      "    thousands : str (length 1), optional\n",
      "        Character acting as the thousands separator in numerical values.\n",
      "    decimal : str (length 1), default '.'\n",
      "        Character to recognize as decimal point (e.g., use ',' for European data).\n",
      "    lineterminator : str (length 1), optional\n",
      "        Character used to denote a line break. Only valid with C parser.\n",
      "    quotechar : str (length 1), optional\n",
      "        Character used to denote the start and end of a quoted item. Quoted\n",
      "        items can include the ``delimiter`` and it will be ignored.\n",
      "    quoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n",
      "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n",
      "        ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n",
      "        characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n",
      "        or ``lineterminator``.\n",
      "    doublequote : bool, default True\n",
      "       When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n",
      "       whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n",
      "       field as a single ``quotechar`` element.\n",
      "    escapechar : str (length 1), optional\n",
      "        Character used to escape other characters.\n",
      "    comment : str (length 1), optional\n",
      "        Character indicating that the remainder of line should not be parsed.\n",
      "        If found at the beginning\n",
      "        of a line, the line will be ignored altogether. This parameter must be a\n",
      "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "        fully commented lines are ignored by the parameter ``header`` but not by\n",
      "        ``skiprows``. For example, if ``comment='#'``, parsing\n",
      "        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n",
      "        treated as the header.\n",
      "    encoding : str, optional, default 'utf-8'\n",
      "        Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n",
      "        standard encodings\n",
      "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
      "\n",
      "    encoding_errors : str, optional, default 'strict'\n",
      "        How encoding errors are treated. `List of possible values\n",
      "        <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "    dialect : str or csv.Dialect, optional\n",
      "        If provided, this parameter will override values (default or not) for the\n",
      "        following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n",
      "        ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n",
      "        override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n",
      "        documentation for more details.\n",
      "    on_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n",
      "        Specifies what to do upon encountering a bad line (a line with too many fields).\n",
      "        Allowed values are :\n",
      "\n",
      "        - ``'error'``, raise an Exception when a bad line is encountered.\n",
      "        - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n",
      "        - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "            - Callable, function with signature\n",
      "              ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n",
      "              bad line. ``bad_line`` is a list of strings split by the ``sep``.\n",
      "              If the function returns ``None``, the bad line will be ignored.\n",
      "              If the function returns a new ``list`` of strings with more elements than\n",
      "              expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n",
      "              Only supported when ``engine='python'``\n",
      "\n",
      "        .. versionchanged:: 2.2.0\n",
      "\n",
      "            - Callable, function with signature\n",
      "              as described in `pyarrow documentation\n",
      "              <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n",
      "              #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n",
      "\n",
      "    delim_whitespace : bool, default False\n",
      "        Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n",
      "        used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n",
      "        is set to ``True``, nothing should be passed in for the ``delimiter``\n",
      "        parameter.\n",
      "\n",
      "        .. deprecated:: 2.2.0\n",
      "            Use ``sep=\"\\s+\"`` instead.\n",
      "    low_memory : bool, default True\n",
      "        Internally process the file in chunks, resulting in lower memory use\n",
      "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "        types either set ``False``, or specify the type with the ``dtype`` parameter.\n",
      "        Note that the entire file is read into a single :class:`~pandas.DataFrame`\n",
      "        regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n",
      "        chunks. (Only valid with C parser).\n",
      "    memory_map : bool, default False\n",
      "        If a filepath is provided for ``filepath_or_buffer``, map the file object\n",
      "        directly onto memory and access the data directly from there. Using this\n",
      "        option can improve performance because there is no longer any I/O overhead.\n",
      "    float_precision : {'high', 'legacy', 'round_trip'}, optional\n",
      "        Specifies which converter the C engine should use for floating-point\n",
      "        values. The options are ``None`` or ``'high'`` for the ordinary converter,\n",
      "        ``'legacy'`` for the original lower precision pandas converter, and\n",
      "        ``'round_trip'`` for the round-trip converter.\n",
      "\n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "        are forwarded to ``urllib.request.Request`` as header options. For other\n",
      "        URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      "        forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      "        details, and for more examples on storage options refer `here\n",
      "        <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      "        highlight=storage_options#reading-writing-remote-files>`_.\n",
      "\n",
      "    dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n",
      "        Back-end data type applied to the resultant :class:`DataFrame`\n",
      "        (still experimental). Behaviour is as follows:\n",
      "\n",
      "        * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n",
      "          (default).\n",
      "        * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n",
      "          DataFrame.\n",
      "\n",
      "        .. versionadded:: 2.0\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or TextFileReader\n",
      "        A comma-separated values (csv) file is returned as two-dimensional\n",
      "        data structure with labeled axes.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "    read_table : Read general delimited file into DataFrame.\n",
      "    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b128b8-189d-4145-a0f1-9bf4d943e1a9",
   "metadata": {},
   "source": [
    "> La lettura di un file *csv* rappresenta spesso una delle **prime attivit√†** di un Data Scientist e di un notebook. Sebbene essa possa sembrare banale, spesso invece rappresenta uno **scoglio iniziale**, fonte di **non poche frustrazioni**, per due ragioni:\n",
    "> - i molti e non banali argomenti della funzione `read_csv`\n",
    "> - le **irregoilarit√†** nei dati dei file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5394f7a9-8061-400e-a1af-3a9a87099765",
   "metadata": {},
   "source": [
    "## Il mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c16f43-e516-48ac-95b9-1534a96430a2",
   "metadata": {},
   "source": [
    "La funzione `pd.read_csv` fa un **mapping** automatico dei dati CSV in pandas, come qui descritto:\n",
    "\n",
    "![](how_pandas_infers_CSV_datatypes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e93810-ba9e-41ed-be16-49f71508751d",
   "metadata": {},
   "source": [
    "C'√® un problema, non citato nella slide: la funzione `read_csv` **non riesce spesso a inferire le variabili categoriche** (se presenti nel file CSV come stringhe), che vengono perci√≤ importate come `object`, il generico data type *stringa* di pandas. Come si vede, infatti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9292db96-52aa-4873-ae2b-79451efae594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T09:32:27.750380Z",
     "iopub.status.busy": "2025-10-21T09:32:27.750139Z",
     "iopub.status.idle": "2025-10-21T09:32:27.754749Z",
     "shell.execute_reply": "2025-10-21T09:32:27.754389Z",
     "shell.execute_reply.started": "2025-10-21T09:32:27.750364Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Income       float64\n",
       "Limit          int64\n",
       "Rating         int64\n",
       "Cards          int64\n",
       "Age            int64\n",
       "Education      int64\n",
       "Gender        object\n",
       "Student       object\n",
       "Married       object\n",
       "Ethnicity     object\n",
       "Balance        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_credit.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99baa579-39ee-4f63-9bfc-4e8c39b72841",
   "metadata": {},
   "source": [
    "Le variabili `Gender`, `Student`, `Married` e `Ethnicity` sono **categoriche** perch√®, al di l√† del loro formato, possono assumere un numero finito e piccolo di valori, a differenza delle variabili **numeriche**, che possono assumere (almeno potenzialmente) un numero infinito di valori.\n",
    "\n",
    "Ogni cella della variabile, se importata come `object`, **punta a una stringa in memoria, spesso duplicata pi√π volte**.\n",
    "\n",
    "Occorre dunque **convertire** queste variabili in formato `category`, un data type atomico reso disponibile da *pandas* (non c'√® in Python base), nel seguente modo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65df6bda-6ddc-4f71-a4f3-c5374269103f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T09:40:41.996556Z",
     "iopub.status.busy": "2025-10-21T09:40:41.996253Z",
     "iopub.status.idle": "2025-10-21T09:40:42.001137Z",
     "shell.execute_reply": "2025-10-21T09:40:42.000829Z",
     "shell.execute_reply.started": "2025-10-21T09:40:41.996542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Income        float64\n",
       "Limit           int64\n",
       "Rating          int64\n",
       "Cards           int64\n",
       "Age             int64\n",
       "Education       int64\n",
       "Gender       category\n",
       "Student        object\n",
       "Married        object\n",
       "Ethnicity      object\n",
       "Balance         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_credit['Gender'] = df_credit['Gender'].astype('category')\n",
    "df_credit.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b894adb0-45e6-47ef-b6d2-993f4163a6de",
   "metadata": {},
   "source": [
    "Il metodo *pandas* `astype('category')`:\n",
    "- crea una **tabella di codifica interna** (i ‚Äúlevels‚Äù o ‚Äúcategories‚Äù),\n",
    "- rappresenta la colonna come **interi** interni (0, 1, 2, ‚Ä¶) invece che stringhe ripetute.\n",
    "\n",
    "Il funzionamento di `category` √® simile a quello del **fattore** di R.\n",
    "\n",
    "üöÄ <font size=\"4\">**Vantaggi principali** (di `category`)</font><br>\n",
    "\n",
    "üîπ **Efficienza in memoria**<br>\n",
    "Ogni valore diventa **un intero**, e la stringa viene **memorizzata una sola volta nella tabella delle categorie**.<br>\n",
    "üëâ Su grandi dataset, il risparmio pu√≤ arrivare al 70‚Äì90% di RAM.\n",
    "Esempio:\n",
    "```python\n",
    "df['citt√†'].memory_usage(deep=True)\n",
    "df['citt√†'].astype('category').memory_usage(deep=True)\n",
    "```\n",
    "La seconda occupa molto meno spazio.\n",
    "\n",
    "üîπ **Velocit√† di elaborazione**<br>\n",
    "Molte operazioni pandas (`groupby`, `sort`, `value_counts`, `merges`) diventano **molto pi√π veloci**; infatti:\n",
    "- confrontare interi √® pi√π rapido che confrontare stringhe,\n",
    "- gli algoritmi di raggruppamento e join lavorano sui codici numerici.\n",
    "üí° Tipico: `df.groupby('categoria').agg(...)` √® molto pi√π rapido se categoria √® `category`.\n",
    "\n",
    "üîπ **Significato semantico**<br>\n",
    "Una variabile categorica ha **un numero finito e noto di livelli**.<br>\n",
    "Questo √® utile per:\n",
    "- garantire che non compaiano valori ‚Äúfuori lista‚Äù (es. ‚ÄòFemmina‚Äô vs ‚ÄòF‚Äô),\n",
    "- mantenere l‚Äôordine logico o gerarchico (es. Basso < Medio < Alto).<br>\n",
    "\n",
    "Si pu√≤ anche definire esplicitamente l‚Äôordine, in questo modo:\n",
    "```python\n",
    "df['livello'] = pd.Categorical(df['livello'], categories=['basso','medio','alto'], ordered=True)\n",
    "```\n",
    "\n",
    "‚Üí utile per confronti, ordinamenti o encoding nel machine learning.\n",
    "\n",
    "üîπ **Compatibilit√† ML e preprocessing**<br>\n",
    "Molti algoritmi di machine learning o encoder (es. `sklearn.preprocessing.OrdinalEncoder`, `OneHotEncoder`) riconoscono category e la trattano subito come variabile discreta, **senza doverla prima convertire da object**.\n",
    "\n",
    "---\n",
    "\n",
    "‚ö†Ô∏è **Quando `category` non conviene?\n",
    "- se la colonna ha **molti valori unici** (es. un codice univoco o un ID cliente), la conversione non porta benefici: la tabella delle categorie sarebbe grande quanto la colonna stessa.\n",
    "- se si modificano spesso i valori (aggiungendo nuove categorie), il tipo `category` √® meno flessibile.\n",
    "\n",
    "üîç **Esempio pratico**:\n",
    "```\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'sesso': ['M','F','M','F','F']*100000\n",
    "})\n",
    "print(df['sesso'].memory_usage(deep=True))   # object\n",
    "df['sesso'] = df['sesso'].astype('category')\n",
    "print(df['sesso'].memory_usage(deep=True))   # category (molto meno!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "![](sintesi_category.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefdf89b-2214-4ed7-8b59-49939285ecfe",
   "metadata": {},
   "source": [
    "## Gli argomenti della funzione `pd.read_csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1fd02f-b8fe-4d70-9bbe-235863bef8d5",
   "metadata": {},
   "source": [
    "Ecco un ottimo notebook di [illustrazione dei vari argomenti](https://github.com/nikitaprasad21/ML-Cheat-Codes/blob/main/Data-Gathering/CSV-(Comma-Separated-Values)-Files/csv_file_cheatcodes.ipynb) per `pd.read_csv` - **scaricato** nella directory di questo notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0786d-2382-40f2-8247-5b828130b23d",
   "metadata": {},
   "source": [
    "## La colonna `Unnamed: 0`\n",
    "Vedi [questa chat](https://chatgpt.com/share/68f74bca-554c-8012-a844-7260ce18391d) di chatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36889a3b-f5e6-406e-8905-83d23e15a66b",
   "metadata": {},
   "source": [
    "## Problemi frequenti nel caricamento dei file CSV in pandas.\n",
    "\n",
    "Ecco un elenco dei **problemi** pi√π comuni che si incontrano caricando **file CSV** con `pandas.read_csv()`, insieme a **cause** e **soluzioni tipiche**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea061b7-be45-4711-af0a-04cbac19db08",
   "metadata": {},
   "source": [
    "üß© **1. Colonne ‚ÄúUnnamed: 0‚Äù o ‚ÄúUnnamed: n‚Äù** - gi√† visto prima\n",
    "\n",
    "<u>Problema</u>: appare una colonna indesiderata chiamata `Unnamed: 0`.<br>\n",
    "<u>Causa</u>: spesso il CSV include un indice salvato da un precedente `DataFrame.to_csv()` (cio√® `index=True` di default).<br>\n",
    "<u>Soluzione</u>:\n",
    "```python\n",
    "pd.read_csv(\"file.csv\", index_col=0)\n",
    "# oppure\n",
    "pd.read_csv(\"file.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ca7d0b-38e8-4a23-99b8-5a8ff7749428",
   "metadata": {},
   "source": [
    "‚öôÔ∏è **2. Delimitatori sbagliati** - gi√† visto prima\n",
    "\n",
    "<u>Problema</u>: il file non viene separato correttamente (tutte le colonne finiscono in una sola).<br>\n",
    "<u>Causa</u>: il separatore non √® la virgola ma punto e virgola ;, tab \\t o altro.<br>\n",
    "<u>Soluzione</u>:\n",
    "```python\n",
    "pd.read_csv(\"file.csv\", sep=\";\")      # per CSV europei\n",
    "pd.read_csv(\"file.csv\", sep=\"\\t\")     # per file TSV\n",
    "```\n",
    "\n",
    "---\n",
    "Un TSV (*Tab-Separated Values*) √® praticamente un CSV, ma invece del separatore `,` oppure `;`, usa il tabulatore `\\t` come separatore di campo.\n",
    "Si pu√† anche rilevare automaticamente:\n",
    "```python\n",
    "pd.read_csv(\"file.csv\", sep=None, engine=\"python\")\n",
    "```\n",
    "\n",
    "üß© Quando si usa un TSV?\n",
    "- quando i dati contengono molte virgole o punti e virgola (es. descrizioni di testo).\n",
    "- quando il file √® esportato da sistemi Unix o database (es. PostgreSQL COPY TO, Excel ‚Üí ‚ÄúTesto con tabulazioni‚Äù).\n",
    "- quando si vuole evitare ambiguit√† tra separatori decimali e di campo.\n",
    "\n",
    "---\n",
    "\n",
    "Il parametro `engine` in `pandas.read_csv()` serve a dire quale ‚Äúmotore di parsing‚Äù usare per leggere e interpretare il file CSV.<brr>\n",
    "In pratica, Pandas ha **due diversi ‚Äúparser engine‚Äù** che fanno lo stesso lavoro (cio√® leggere il file e trasformarlo in DataFrame), ma **con caratteristiche e prestazioni diverse**.\n",
    "\n",
    "1Ô∏è‚É£ **`engine=\"c\"`** ‚Üí il parser ‚Äúveloce‚Äù (default)\n",
    "- scritto in linguaggio C ‚Üí molto veloce\n",
    "- √® quello usato di default in quasi tutti i casi.\n",
    "- √® ottimo per file puliti e regolari.\n",
    "- ma... √® meno flessibile: non supporta tutte le opzioni, e pu√≤ fallire su CSV ‚Äúsporchi‚Äù o complessi.\n",
    "\n",
    "Esempio di uso:\n",
    "```python\n",
    "pd.read_csv(\"file.csv\", engine=\"c\")\n",
    "```\n",
    "\n",
    "2Ô∏è‚É£ **`engine=\"python\"`** ‚Üí il parser ‚Äúrobusto‚Äù\n",
    "- scritto in puro Python ‚Üí pi√π lento, ma pi√π tollerante.\n",
    "- supporta opzioni che il parser C non gestisce bene, come:\n",
    "    - `sep=None` (cio√® **il rilevamento automatico del separatore**),\n",
    "    - delimitatori multipli o irregolari,\n",
    "    - linee malformate (on_bad_lines),\n",
    "    - quote e caratteri speciali complessi.\n",
    "\n",
    "Esempio d'uso:\n",
    "```python\n",
    "pd.read_csv(\"file.csv\", sep=None, engine=\"python\")\n",
    "```\n",
    "\n",
    "üëâ Qui Pandas prova automaticamente a indovinare il separatore (',', ';', '\\t', ecc.) analizzando le prime righe.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a39c444-17bf-457e-aed9-9ba843517807",
   "metadata": {},
   "source": [
    "Torniamo all'elenco dei  problemi e soluzioni di `read_csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee6a66-6045-429c-abfc-2e04eae898e9",
   "metadata": {},
   "source": [
    "üî§ **3. Encoding errato**\n",
    "\n",
    "<u>Problema</u>: caratteri accentati o simboli speciali appaiono come ÔøΩ o danno errore `UnicodeDecodeError`.<br>\n",
    "<u>Causa</u>: il file non √® in `UTF-8` ma in `latin1`, `cp1252`, ecc.<br>\n",
    "<u>Soluzione</u>:\n",
    "```python\n",
    "pd.read_csv(\"file.csv\", encoding=\"latin1\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e36a52b-1484-4355-9b4d-c85f6a1d80b6",
   "metadata": {},
   "source": [
    "üìâ **4. Tipo di dato non corretto**\n",
    "\n",
    "<u>Problema</u>: colonne numeriche importate come stringhe (object).<br>\n",
    "<u>Causa</u>: presenza di separatori migliaia, simboli, o celle vuote.<br>\n",
    "<u>Soluzione</u>:\n",
    "```python\n",
    "pd.read_csv(\"file.csv\", thousands=\".\", decimal=\",\")\n",
    "```\n",
    "oppure successivamente:\n",
    "```python\n",
    "df[\"col\"] = pd.to_numeric(df[\"col\"], errors=\"coerce\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84574a93-10bb-434d-8295-e9164b1e51a5",
   "metadata": {},
   "source": [
    "üßæ **5. Header non nella prima riga** - gi√† visto prima\n",
    "\n",
    "<u>Problema</u>: i nomi di colonna non vengono letti correttamente.<br>\n",
    "<u>Causa</u>: il file ha righe descrittive o metadati iniziali.<br>\n",
    "<u>Soluzione</u>:\n",
    "```python\n",
    "pd.read_csv(\"file.csv\", header=2)   # se l‚Äôintestazione √® alla terza riga\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a34b8c9-0a5b-4333-82b7-1d87f085d1a5",
   "metadata": {},
   "source": [
    "ü™ì **6. File troppo grande**\n",
    "\n",
    "<u>Problema</u>: `MemoryError` o caricamento lentissimo.<br>\n",
    "<u>Causa</u>: CSV molto grande rispetto alla RAM.<br>\n",
    "<u>Soluzioni</u>:\n",
    "\n",
    "- Caricamento a chunk:\n",
    "```python\n",
    "for chunk in pd.read_csv(\"file.csv\", chunksize=100000):\n",
    "    process(chunk)\n",
    "```\n",
    "- Oppure usare **Dask** o **Polars** per big data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac0d14-6fb4-4909-9c4d-1935bc98df71",
   "metadata": {},
   "source": [
    "üßÆ **7. Colonne con valori mancanti o disallineati**\n",
    "\n",
    "<u>Problema</u>: righe con numero diverso di colonne, errore tipo `ParserError: Error tokenizing data`.<br>\n",
    "<u>Causa</u>: virgolette non chiuse o separatori dentro i campi.<br>\n",
    "<u>Soluzione</u>:\n",
    "```python\n",
    "pd.read_csv(\"file.csv\", on_bad_lines=\"skip\", quoting=csv.QUOTE_NONE)\n",
    "```\n",
    "\n",
    "Oppure controllare i delimitatori."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab5064-1469-451a-8732-786123690fa4",
   "metadata": {},
   "source": [
    "üß† **8. Date non interpretate correttamente**\n",
    "\n",
    "<u>Problema</u>: le date restano stringhe o sono nel formato errato.<br>\n",
    "<u>Soluzione</u>:\n",
    "```python\n",
    "pd.read_csv(\"file.csv\", parse_dates=[\"data\"])\n",
    "```\n",
    "\n",
    "oppure\n",
    "```python\n",
    "df[\"data\"] = pd.to_datetime(df[\"data\"], dayfirst=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6f29c-cd33-4d11-b3e1-0b6c97d9b118",
   "metadata": {},
   "source": [
    "ü™™ **9. Duplicati o whitespace nei nomi colonna**\n",
    "\n",
    "<u>Problema</u>: nomi con spazi o duplicati (`'Nome '` ‚â† `'Nome'`).<br>\n",
    "<u>Soluzione</u>:\n",
    "```python\n",
    "df.columns = df.columns.str.strip()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e5542-b3f5-483c-ab63-bc570b80c2c1",
   "metadata": {},
   "source": [
    "üß± **10. Quote e caratteri speciali**\n",
    "\n",
    "<u>Problema</u>: CSV con virgolette interne, doppie virgolette, ecc.<br>\n",
    "<u>Soluzione</u>:\n",
    "```python\n",
    "pd.read_csv(\"file.csv\", quotechar='\"', escapechar='\\\\')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a303a02e-92b9-43e7-8b9f-39df6858cc3f",
   "metadata": {},
   "source": [
    "![](problemi_tipici_read_csv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a31fa67-aee3-4f8d-b74c-fdcd27295ea4",
   "metadata": {},
   "source": [
    "Vediamo ora:<br>\n",
    "1Ô∏è‚É£ un file CSV **‚Äúsporco‚Äù** con vari **errori e inconsistenze reali**;<br>\n",
    "2Ô∏è‚É£ e il codice Python completo per leggerlo correttamente con `pandas.read_csv()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb74fea9-7ab2-4c45-816a-2e5c326fe786",
   "metadata": {},
   "source": [
    "Contenuto del file `dati_sporchi.csv`.<br>\n",
    "üëâ Questo file contiene:\n",
    "- delimitatore `;` invece di `,`\n",
    "- encoding misto (accenti e caratteri speciali)\n",
    "- separatori decimali confusi (`,`, `.`)\n",
    "- valori mancanti o `N/A`\n",
    "- riga con virgolette interne e una virgola nel nome\n",
    "- header con spazi\n",
    "- colonna `Unnamed: 0` inutile\n",
    "- righe con colonne disallineate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6285db6a-1d17-412c-900e-25bbad202383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T12:04:25.200361Z",
     "iopub.status.busy": "2025-10-21T12:04:25.200077Z",
     "iopub.status.idle": "2025-10-21T12:04:25.204300Z",
     "shell.execute_reply": "2025-10-21T12:04:25.203949Z",
     "shell.execute_reply.started": "2025-10-21T12:04:25.200345Z"
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1. CREA IL FILE CSV \"SPORCO\"\n",
    "# =========================\n",
    "csv_content = \"\"\"# Dati di esempio esportati da sistema legacy\n",
    "# Contengono errori di formato, encoding e separatori\n",
    "ID; Nome ; Et√† ; Data_nascita ; Stipendio ; Note\n",
    "0; \"Mario Rossi\"; 35 ; 12/05/1989 ; \"2.500,50\" ; \"Lavora a Roma, ottimo rendimento\"\n",
    "1; \"Anna Bianchi\"; 29 ; 01/09/1995 ; \"3.200,00\" ; \"Milano, nuovi progetti\"\n",
    "2; \"Jos√© √Ålvarez\"; 40 ; 15/02/1984 ; \"4.000,75\" ; \"Problemi di encoding √†√®√¨√≤√π\"\n",
    "3; \"Luigi Verdi\"; \"?\" ; 03/11/1990 ; \"2,800.00\" ; \"Errore nei separatori decimali\"\n",
    "4; \"Giulia Rossi\" ; 27 ; 31-08-1997 ; \"3.000,00\" ; \"Riga OK\"\n",
    "5; \"Paolo Bianchi\" ; 33 ; 02/04/1991 ; \"N/A\" ; \"Valore mancante stipendio\"\n",
    "6; \"Marco, Test\"; 38 ; 07/07/1986 ; \"2.900,00\" ; \"Virgola nel nome\"\n",
    "7 \"Sara Neri\" ; 31 ; 10/10/1993 ; \"3.200,00\" ; Riga con separatore mancante\n",
    "8; \"Laura Verdi\"; 25 ; 21/06/1999 ; \"3.000,00\"\n",
    "9; \"Andrea Neri\" ; ; ; ; \"Campi mancanti\"\n",
    "Unnamed: 0; \"Extra colonna inutile\"; ; ; ;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe4599f6-d8f0-4fdf-ba16-11c669be76a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T12:04:26.023843Z",
     "iopub.status.busy": "2025-10-21T12:04:26.023528Z",
     "iopub.status.idle": "2025-10-21T12:04:26.030795Z",
     "shell.execute_reply": "2025-10-21T12:04:26.030115Z",
     "shell.execute_reply.started": "2025-10-21T12:04:26.023824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File 'dati_sporchi.csv' creato.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"dati_sporchi.csv\", \"w\", encoding=\"latin1\") as f:\n",
    "    f.write(csv_content)\n",
    "\n",
    "print(\"‚úÖ File 'dati_sporchi.csv' creato.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f7bc6d9-36b7-443d-9fd2-1c8485f44be5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T12:04:31.824870Z",
     "iopub.status.busy": "2025-10-21T12:04:31.824652Z",
     "iopub.status.idle": "2025-10-21T12:04:31.831028Z",
     "shell.execute_reply": "2025-10-21T12:04:31.830405Z",
     "shell.execute_reply.started": "2025-10-21T12:04:31.824854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonne originali: ['ID', 'Nome ', 'Et√† ', 'Data_nascita ', 'Stipendio ', 'Note'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 2. LETTURA ROBUSTA\n",
    "# =========================\n",
    "df = pd.read_csv(\n",
    "    \"dati_sporchi.csv\",\n",
    "    sep=\";\",                     # separatore europeo\n",
    "    comment=\"#\",                 # ignora righe di commento\n",
    "    engine=\"python\",             # parser pi√π flessibile\n",
    "    encoding=\"latin1\",           # gestisce accenti\n",
    "    on_bad_lines=\"skip\",         # salta righe errate\n",
    "    skip_blank_lines=True,       # ignora righe vuote\n",
    "    skipinitialspace=True        # rimuove spazi dopo ;\n",
    ")\n",
    "\n",
    "print(\"Colonne originali:\", df.columns.tolist(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b471617-f89e-476b-835a-271212a655cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T12:04:39.705683Z",
     "iopub.status.busy": "2025-10-21T12:04:39.705409Z",
     "iopub.status.idle": "2025-10-21T12:04:39.713430Z",
     "shell.execute_reply": "2025-10-21T12:04:39.712972Z",
     "shell.execute_reply.started": "2025-10-21T12:04:39.705656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Nome</th>\n",
       "      <th>Et√†</th>\n",
       "      <th>Data_nascita</th>\n",
       "      <th>Stipendio</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Laura Verdi</td>\n",
       "      <td>25.0</td>\n",
       "      <td>21/06/1999</td>\n",
       "      <td>3.000,00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>Extra colonna inutile</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                  Nome   Et√†  Data_nascita  Stipendio   Note\n",
       "0           8            Laura Verdi  25.0   21/06/1999    3.000,00   NaN\n",
       "1  Unnamed: 0  Extra colonna inutile   NaN           NaN        NaN   NaN"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "118730db-3a3f-4d62-8bf5-f9b1e5ab942f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T12:04:48.491769Z",
     "iopub.status.busy": "2025-10-21T12:04:48.491555Z",
     "iopub.status.idle": "2025-10-21T12:04:48.496177Z",
     "shell.execute_reply": "2025-10-21T12:04:48.495747Z",
     "shell.execute_reply.started": "2025-10-21T12:04:48.491754Z"
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3. PULIZIA NOMI COLONNE\n",
    "# =========================\n",
    "df.columns = df.columns.str.strip()                          # rimuove spazi\n",
    "df.columns = df.columns.str.replace(\"√É\", \"√†\", regex=False)   # corregge accenti errati\n",
    "df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\", case=False)]  # rimuove colonne Unnamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "745a6018-7cb7-4435-9db7-46705faef766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T12:04:56.004391Z",
     "iopub.status.busy": "2025-10-21T12:04:56.004064Z",
     "iopub.status.idle": "2025-10-21T12:04:56.011034Z",
     "shell.execute_reply": "2025-10-21T12:04:56.010686Z",
     "shell.execute_reply.started": "2025-10-21T12:04:56.004375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Nome</th>\n",
       "      <th>Et√†</th>\n",
       "      <th>Data_nascita</th>\n",
       "      <th>Stipendio</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Laura Verdi</td>\n",
       "      <td>25.0</td>\n",
       "      <td>21/06/1999</td>\n",
       "      <td>3.000,00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>Extra colonna inutile</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                   Nome   Et√† Data_nascita Stipendio  Note\n",
       "0           8            Laura Verdi  25.0  21/06/1999   3.000,00   NaN\n",
       "1  Unnamed: 0  Extra colonna inutile   NaN          NaN       NaN   NaN"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4789159d-f4a0-46c6-a439-ab4ce656d8c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T12:05:45.040706Z",
     "iopub.status.busy": "2025-10-21T12:05:45.040481Z",
     "iopub.status.idle": "2025-10-21T12:05:45.055201Z",
     "shell.execute_reply": "2025-10-21T12:05:45.054688Z",
     "shell.execute_reply.started": "2025-10-21T12:05:45.040693Z"
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4. TRASFORMAZIONI TIPICHE\n",
    "# =========================\n",
    "\n",
    "# -- Colonna Et√†\n",
    "if \"Et√†\" in df.columns:\n",
    "    df[\"Et√†\"] = pd.to_numeric(df[\"Et√†\"], errors=\"coerce\")\n",
    "\n",
    "# -- Colonna Stipendio\n",
    "if \"Stipendio\" in df.columns:\n",
    "    df[\"Stipendio\"] = (\n",
    "        df[\"Stipendio\"]\n",
    "        .astype(str)\n",
    "        .str.replace(\".\", \"\", regex=False)  # rimuove i punti (migliaia)\n",
    "        .str.replace(\",\", \".\", regex=False) # converte virgola in punto\n",
    "    )\n",
    "    df[\"Stipendio\"] = pd.to_numeric(df[\"Stipendio\"], errors=\"coerce\")\n",
    "\n",
    "# -- Colonna Data_nascita\n",
    "if \"Data_nascita\" in df.columns:\n",
    "    df[\"Data_nascita\"] = pd.to_datetime(df[\"Data_nascita\"], dayfirst=True, errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4892219e-17e2-4df9-b29c-3fde757bac0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T12:05:53.083670Z",
     "iopub.status.busy": "2025-10-21T12:05:53.083365Z",
     "iopub.status.idle": "2025-10-21T12:05:53.090487Z",
     "shell.execute_reply": "2025-10-21T12:05:53.090121Z",
     "shell.execute_reply.started": "2025-10-21T12:05:53.083652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Nome</th>\n",
       "      <th>Et√†</th>\n",
       "      <th>Data_nascita</th>\n",
       "      <th>Stipendio</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Laura Verdi</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1999-06-21</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>Extra colonna inutile</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                   Nome   Et√† Data_nascita  Stipendio  Note\n",
       "0           8            Laura Verdi  25.0   1999-06-21     3000.0   NaN\n",
       "1  Unnamed: 0  Extra colonna inutile   NaN          NaT        NaN   NaN"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bcf1ebe6-5367-40e4-8e63-bac6b1be4704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T12:06:12.546286Z",
     "iopub.status.busy": "2025-10-21T12:06:12.546096Z",
     "iopub.status.idle": "2025-10-21T12:06:12.551472Z",
     "shell.execute_reply": "2025-10-21T12:06:12.551153Z",
     "shell.execute_reply.started": "2025-10-21T12:06:12.546272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File caricato e pulito correttamente!\n",
      "\n",
      "           ID                   Nome   Et√† Data_nascita  Stipendio  Note\n",
      "0           8            Laura Verdi  25.0   1999-06-21     3000.0   NaN\n",
      "1  Unnamed: 0  Extra colonna inutile   NaN          NaT        NaN   NaN\n",
      "\n",
      "Tipi di dato:\n",
      " ID                      object\n",
      "Nome                    object\n",
      "Et√†                    float64\n",
      "Data_nascita    datetime64[ns]\n",
      "Stipendio              float64\n",
      "Note                   float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 5. RISULTATO FINALE\n",
    "# =========================\n",
    "print(\"‚úÖ File caricato e pulito correttamente!\\n\")\n",
    "print(df)\n",
    "print(\"\\nTipi di dato:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f95d4e1c-067e-4f6f-b5b2-9f1c59f6ef93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T11:57:10.910429Z",
     "iopub.status.busy": "2025-10-21T11:57:10.910197Z",
     "iopub.status.idle": "2025-10-21T11:57:10.915167Z",
     "shell.execute_reply": "2025-10-21T11:57:10.914753Z",
     "shell.execute_reply.started": "2025-10-21T11:57:10.910413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dati_sporchi.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile dati_sporchi.csv\n",
    "# Dati di esempio esportati da sistema legacy\n",
    "# Contengono errori di formato, encoding e separatori\n",
    "ID; Nome ; Et√† ; Data_nascita ; Stipendio ; Note\n",
    "0; \"Mario Rossi\"; 35 ; 12/05/1989 ; \"2.500,50\" ; \"Lavora a Roma, ottimo rendimento\"\n",
    "1; \"Anna Bianchi\"; 29 ; 01/09/1995 ; \"3.200,00\" ; \"Milano, nuovi progetti\"\n",
    "2; \"Jos√© √Ålvarez\"; 40 ; 15/02/1984 ; \"4.000,75\" ; \"Problemi di encoding √†√®√¨√≤√π\"\n",
    "3; \"Luigi Verdi\"; \"?\" ; 03/11/1990 ; \"2,800.00\" ; \"Errore nei separatori decimali\"\n",
    "4; \"Giulia Rossi\" ; 27 ; 31-08-1997 ; \"3.000,00\" ; \"Riga OK\"\n",
    "5; \"Paolo Bianchi\" ; 33 ; 02/04/1991 ; \"N/A\" ; \"Valore mancante stipendio\"\n",
    "6; \"Marco, Test\"; 38 ; 07/07/1986 ; \"2.900,00\" ; \"Virgola nel nome\"\n",
    "7 \"Sara Neri\" ; 31 ; 10/10/1993 ; \"3.200,00\" ; Riga con separatore mancante\n",
    "8; \"Laura Verdi\"; 25 ; 21/06/1999 ; \"3.000,00\"\n",
    "9; \"Andrea Neri\" ; ; ; ; \"Campi mancanti\"\n",
    "Unnamed: 0; \"Extra colonna inutile\"; ; ; ;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fa12a4-c3ff-4787-9dc7-efa5c5ec06b8",
   "metadata": {},
   "source": [
    "Il codice python per leggerlo correttamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4953e0b6-e131-4d3c-a82d-9530c77ef108",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T12:00:53.732406Z",
     "iopub.status.busy": "2025-10-21T12:00:53.732196Z",
     "iopub.status.idle": "2025-10-21T12:00:54.783581Z",
     "shell.execute_reply": "2025-10-21T12:00:54.782890Z",
     "shell.execute_reply.started": "2025-10-21T12:00:53.732392Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Et√†'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Et√†'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;241m~\u001b[39mdf\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^Unnamed\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Conversione dei tipi di dato\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEt√†\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEt√†\u001b[39m\u001b[38;5;124m\"\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStipendio\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     21\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStipendio\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# rimuove i punti come migliaia\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# converte la virgola in punto\u001b[39;00m\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     26\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStipendio\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStipendio\u001b[39m\u001b[38;5;124m\"\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3817\u001b[0m     ):\n\u001b[0;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Et√†'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lettura robusta del file CSV \"sporco\"\n",
    "df = pd.read_csv(\n",
    "    \"dati_sporchi.csv\",\n",
    "    sep=\";\",                     # separatore europeo\n",
    "    comment=\"#\",                 # ignora righe di commento iniziali\n",
    "    engine=\"python\",             # parser pi√π tollerante\n",
    "    encoding=\"latin1\",           # gestisce accenti e caratteri speciali\n",
    "    on_bad_lines=\"skip\",         # salta righe disallineate\n",
    "    skip_blank_lines=True,       # evita righe vuote\n",
    "    skipinitialspace=True        # rimuove spazi dopo il separatore\n",
    ")\n",
    "\n",
    "# Rimuove eventuale colonna \"Unnamed\"\n",
    "df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
    "\n",
    "# Conversione dei tipi di dato\n",
    "df[\"Et√†\"] = pd.to_numeric(df[\"Et√†\"], errors=\"coerce\")\n",
    "df[\"Stipendio\"] = (\n",
    "    df[\"Stipendio\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\".\", \"\", regex=False)  # rimuove i punti come migliaia\n",
    "    .str.replace(\",\", \".\", regex=False) # converte la virgola in punto\n",
    ")\n",
    "df[\"Stipendio\"] = pd.to_numeric(df[\"Stipendio\"], errors=\"coerce\")\n",
    "\n",
    "# Parsing delle date\n",
    "df[\"Data_nascita\"] = pd.to_datetime(df[\"Data_nascita\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "# Pulizia dei nomi colonna\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "print(\"\\n‚úÖ File caricato correttamente!\\n\")\n",
    "print(df)\n",
    "print(\"\\nTipi di dato:\\n\", df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a9a845",
   "metadata": {},
   "source": [
    "# Accelerare il caricamento in pandas di un file csv di grandi dimensioni\n",
    "\n",
    "Come accelerare il caricamento in pandas con read_csv di un file csv molto grande?<br>\n",
    "Ecco il **workflow consigliato**:\n",
    "```python\n",
    "    import pandas as pd\n",
    "\n",
    "    # Prima lettura, con ottimizzazioni di base\n",
    "    df_iter = pd.read_csv(\n",
    "        \"bigdata.csv\",\n",
    "        usecols=[\"A\", \"B\", \"C\"],\n",
    "        dtype={\"A\": \"int32\", \"B\": \"float32\"},\n",
    "        chunksize=1_000_000,\n",
    "        engine=\"pyarrow\"\n",
    "    )\n",
    "\n",
    "    # Elaborazione incrementale\n",
    "    df = pd.concat(df_iter)\n",
    "\n",
    "    # Salva in formato ottimizzato\n",
    "    df.to_parquet(\"bigdata.parquet\")\n",
    "```\n",
    "üëâ Le letture successive da Parquet o Feather saranno **fino a 50√ó pi√π rapide**.\n",
    "\n",
    "**Piccoli trucchi pratici**\n",
    "- pre-carica in RAM i file (es. `cat file.csv > /dev/null` su Linux) se il collo di bottiglia √® il disco.\n",
    "- se lavori spesso con gli stessi dati ‚Üí converti subito a Parquet.\n",
    "- se il file √® remoto ‚Üí usa `storage_options` (es. S3 o GDrive) per lettura diretta.\n",
    "- se non serve l‚Äôindice ‚Üí `index_col=False` o `index_col=None`.\n",
    "- per misurare l‚Äôeffetto: usa `%%time` in Jupyter o VSC oppure `timeit`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085db616",
   "metadata": {},
   "source": [
    "**Ecco una guida pratica per velocizzare la lettura** üëá\n",
    "\n",
    "Come velocizzare pandas.read_csv() su file grandi\n",
    "\n",
    "1Ô∏è‚É£ **Specifica i tipi di dato (dtype)**<br>\n",
    "Pandas, se non li dichiari, deve ‚Äúindovinare‚Äù i tipi scorrendo le righe ‚Üí lento e dispendioso in memoria.\n",
    "```python\n",
    "dtypes = {\n",
    "    \"id\": \"int32\",\n",
    "    \"categoria\": \"category\",\n",
    "    \"prezzo\": \"float32\",\n",
    "    \"quantita\": \"int16\"\n",
    "}\n",
    "df = pd.read_csv(\"file.csv\", dtype=dtypes)\n",
    "```\n",
    "\n",
    "‚úÖ Vantaggi: caricamento molto pi√π veloce e dataframe pi√π leggero.\n",
    "\n",
    "2Ô∏è‚É£ **Leggi solo alcune colonne**<br>\n",
    "Se non ti servono tutte, dichiara usecols:\n",
    "```python\n",
    "df = pd.read_csv(\"file.csv\", usecols=[\"id\", \"prezzo\", \"quantita\"])\n",
    "```\n",
    "\n",
    "‚úÖ Risparmi tempo e memoria.\n",
    "\n",
    "3Ô∏è‚É£ **Disattiva ci√≤ che non serve**\n",
    "\n",
    "Niente indice:\n",
    "```python\n",
    "index_col=False\n",
    "```\n",
    "\n",
    "Niente analisi di numeri mancanti complessa:\n",
    "```python\n",
    "keep_default_na=False\n",
    "na_values=[\"\"]\n",
    "```\n",
    "Niente conversione automatica di date:\n",
    "```python\n",
    "parse_dates=False\n",
    "```\n",
    "\n",
    "‚úÖ Tutto ci√≤ evita inferenze costose.\n",
    "\n",
    "4Ô∏è‚É£ **Usa un chunking (lettura a blocchi)**\n",
    "\n",
    "Se il file √® troppo grande per la RAM, leggilo a pezzi:\n",
    "```python\n",
    "chunks = pd.read_csv(\"file.csv\", chunksize=1_000_000)\n",
    "for chunk in chunks:\n",
    "    # elabora il chunk\n",
    "    process(chunk)\n",
    "```\n",
    "\n",
    "‚úÖ Mantieni basso il consumo di memoria e puoi elaborare in streaming.\n",
    "\n",
    "5Ô∏è‚É£ **Specifica l‚Äôengine**\n",
    "\n",
    "pandas pu√≤ usare due engine:\n",
    "- `engine='c'` (default, scritto in C) ‚Üí pi√π veloce\n",
    "- `engine='python'` ‚Üí pi√π flessibile ma pi√π lento\n",
    "\n",
    "Assicurati di usare:\n",
    "```python\n",
    "pd.read_csv(\"file.csv\", engine=\"c\")\n",
    "```\n",
    "\n",
    "6Ô∏è‚É£ **‚ÄúScalda‚Äù la cache del disco**\n",
    "\n",
    "Su Linux:\n",
    "\n",
    "cat file.csv > /dev/null\n",
    "\n",
    "‚Üí cos√¨ il file √® gi√† in cache RAM e il successivo read_csv sar√† pi√π rapido.\n",
    "(non migliora la prima lettura, ma le successive s√¨)\n",
    "\n",
    "7Ô∏è‚É£ **Converti in Parquet appena puoi**\n",
    "\n",
    "CSV ‚Üí Parquet una volta sola, poi lavora sempre in Parquet:\n",
    "```python\n",
    "df = pd.read_csv(\"file.csv\")\n",
    "df.to_parquet(\"file.parquet\")\n",
    "```\n",
    "\n",
    "e successivamente:\n",
    "```python\n",
    "df = pd.read_parquet(\"file.parquet\")\n",
    "```\n",
    "\n",
    "‚úÖ Spesso 5‚Äì10√ó pi√π veloce in lettura e 3‚Äì4√ó meno spazio su disco.\n",
    "\n",
    "8Ô∏è‚É£ Alternativa: usa Dask o Polars\n",
    "\n",
    "Se il file √® enorme (decine di GB):\n",
    "```python\n",
    "dask.dataframe.read_csv()\n",
    "```python\n",
    " ‚Üí lettura parallela su pi√π core;\n",
    "```python\n",
    "polars.read_csv()\n",
    "```\n",
    " ‚Üí motore Rust super veloce (anche 10√ó pi√π rapido di pandas).\n",
    "\n",
    "9Ô∏è‚É£ Misura sempre con %%time\n",
    "\n",
    "In Jupyter o VS Code:\n",
    "```python\n",
    "%%time\n",
    "df = pd.read_csv(\"file.csv\", dtype=dtypes, usecols=cols)\n",
    "```\n",
    "\n",
    "Confronta varie versioni e scegli la pi√π rapida nel tuo contesto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed47c7b-75b6-4196-9eee-6643b567c9f4",
   "metadata": {},
   "source": [
    "# Le prestazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a0074-88bb-4905-bd5d-e3cc60161f54",
   "metadata": {},
   "source": [
    "Per quanto riguarda le **prestazioni dei vari formati** (come occupazione in memoria, salvtaggio su disco e apertura /lettura) vedi il seguente utile studio.\n",
    "\n",
    "Il messaggio chiave dello studio √® che:\n",
    "- il formato CSV √® molto meglio di excel (neanche preso in considerazione nella comparazione), √® disponibile in tutti gli ambienti di *data management*\n",
    "- per big data (come vedremo) il formato migliore √® il parquet, soprattutto nella occupazione di memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d609bbdc-af9b-474c-81fb-ebaae9a984d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T10:05:59.935492Z",
     "iopub.status.busy": "2025-10-21T10:05:59.935129Z",
     "iopub.status.idle": "2025-10-21T10:05:59.939333Z",
     "shell.execute_reply": "2025-10-21T10:05:59.938890Z",
     "shell.execute_reply.started": "2025-10-21T10:05:59.935474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"600\"\n",
       "            src=\"I_O Optimization in Data Projects - by Avi Chawla.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x29290601310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Esempio d‚Äôuso:\n",
    "show_pdf(\"I_O Optimization in Data Projects - by Avi Chawla.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89bf0e3-903b-44ee-ab4a-9259cb996949",
   "metadata": {},
   "source": [
    "# Il formato dei dati per big data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d931f3-454c-48ce-8d18-e69d831b36a6",
   "metadata": {},
   "source": [
    "E' possibile caricare big data di 5M di righe in *pandas*? Dipende.\n",
    "\n",
    "La risposta breve √®: s√¨, pandas pu√≤ gestire anche 5 milioni di righe, ma dipende da cosa si intende per ‚Äúgestire‚Äù e da quanta RAM si ha a disposizione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b133258d-da10-4f09-b750-a16307861065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T10:07:32.323561Z",
     "iopub.status.busy": "2025-10-21T10:07:32.323226Z",
     "iopub.status.idle": "2025-10-21T10:07:32.326591Z",
     "shell.execute_reply": "2025-10-21T10:07:32.326201Z",
     "shell.execute_reply.started": "2025-10-21T10:07:32.323542Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e9d90ee-e5ff-40ff-bc57-722befe5677c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T10:17:05.312080Z",
     "iopub.status.busy": "2025-10-21T10:17:05.311883Z",
     "iopub.status.idle": "2025-10-21T10:17:05.316774Z",
     "shell.execute_reply": "2025-10-21T10:17:05.316174Z",
     "shell.execute_reply.started": "2025-10-21T10:17:05.312066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Il prefisso r dice a Python di non interpretare \\ come escape.\n",
    "path = r'C:\\Users\\Utente\\Desktop\\salvataggi\\SALVATAGGIO DATI\\Documents\\Seminari\\Data Science (corsi)\\Corso Python base\\linkage\\file_csv' \n",
    "\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b531d132-3f6b-412d-befc-378ef601c974",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T10:32:23.453540Z",
     "iopub.status.busy": "2025-10-21T10:32:23.453327Z",
     "iopub.status.idle": "2025-10-21T10:32:26.594957Z",
     "shell.execute_reply": "2025-10-21T10:32:26.594533Z",
     "shell.execute_reply.started": "2025-10-21T10:32:23.453525Z"
    }
   },
   "outputs": [],
   "source": [
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7eeada6-6166-4d45-a67a-cceb0c92f38d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T10:32:28.024779Z",
     "iopub.status.busy": "2025-10-21T10:32:28.024569Z",
     "iopub.status.idle": "2025-10-21T10:32:28.029549Z",
     "shell.execute_reply": "2025-10-21T10:32:28.028992Z",
     "shell.execute_reply.started": "2025-10-21T10:32:28.024767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5749132, 12)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "995c22ea-e5cf-49e8-a777-4b1d213c12b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T10:32:29.005691Z",
     "iopub.status.busy": "2025-10-21T10:32:29.005410Z",
     "iopub.status.idle": "2025-10-21T10:32:29.014130Z",
     "shell.execute_reply": "2025-10-21T10:32:29.013644Z",
     "shell.execute_reply.started": "2025-10-21T10:32:29.005676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>cmp_fname_c1</th>\n",
       "      <th>cmp_fname_c2</th>\n",
       "      <th>cmp_lname_c1</th>\n",
       "      <th>cmp_lname_c2</th>\n",
       "      <th>cmp_sex</th>\n",
       "      <th>cmp_bd</th>\n",
       "      <th>cmp_bm</th>\n",
       "      <th>cmp_by</th>\n",
       "      <th>cmp_plz</th>\n",
       "      <th>is_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37291</td>\n",
       "      <td>53113</td>\n",
       "      <td>0.833333333333333</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39086</td>\n",
       "      <td>47614</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70031</td>\n",
       "      <td>70237</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84795</td>\n",
       "      <td>97439</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36950</td>\n",
       "      <td>42116</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_1   id_2       cmp_fname_c1 cmp_fname_c2  cmp_lname_c1 cmp_lname_c2  \\\n",
       "0  37291  53113  0.833333333333333            ?           1.0            ?   \n",
       "1  39086  47614                  1            ?           1.0            ?   \n",
       "2  70031  70237                  1            ?           1.0            ?   \n",
       "3  84795  97439                  1            ?           1.0            ?   \n",
       "4  36950  42116                  1            ?           1.0            1   \n",
       "\n",
       "   cmp_sex cmp_bd cmp_bm cmp_by cmp_plz  is_match  \n",
       "0        1      1      1      1       0      True  \n",
       "1        1      1      1      1       1      True  \n",
       "2        1      1      1      1       1      True  \n",
       "3        1      1      1      1       1      True  \n",
       "4        1      1      1      1       1      True  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59cd49ea-916b-4b5c-a3cb-16cd72aebf25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T10:32:44.174708Z",
     "iopub.status.busy": "2025-10-21T10:32:44.174499Z",
     "iopub.status.idle": "2025-10-21T10:32:44.183125Z",
     "shell.execute_reply": "2025-10-21T10:32:44.182779Z",
     "shell.execute_reply.started": "2025-10-21T10:32:44.174693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>cmp_fname_c1</th>\n",
       "      <th>cmp_fname_c2</th>\n",
       "      <th>cmp_lname_c1</th>\n",
       "      <th>cmp_lname_c2</th>\n",
       "      <th>cmp_sex</th>\n",
       "      <th>cmp_bd</th>\n",
       "      <th>cmp_bm</th>\n",
       "      <th>cmp_by</th>\n",
       "      <th>cmp_plz</th>\n",
       "      <th>is_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5749127</th>\n",
       "      <td>47892</td>\n",
       "      <td>98941</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749128</th>\n",
       "      <td>53346</td>\n",
       "      <td>74894</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749129</th>\n",
       "      <td>18058</td>\n",
       "      <td>99971</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749130</th>\n",
       "      <td>84934</td>\n",
       "      <td>95688</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749131</th>\n",
       "      <td>20985</td>\n",
       "      <td>57829</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id_1   id_2 cmp_fname_c1 cmp_fname_c2  cmp_lname_c1 cmp_lname_c2  \\\n",
       "5749127  47892  98941            1            ?      0.166667            ?   \n",
       "5749128  53346  74894            1            ?      0.222222            ?   \n",
       "5749129  18058  99971            0            ?      1.000000            ?   \n",
       "5749130  84934  95688            1            ?      0.000000            ?   \n",
       "5749131  20985  57829            1            1      0.000000            ?   \n",
       "\n",
       "         cmp_sex cmp_bd cmp_bm cmp_by cmp_plz  is_match  \n",
       "5749127        1      0      0      1       0     False  \n",
       "5749128        1      0      0      1       0     False  \n",
       "5749129        1      0      0      0       0     False  \n",
       "5749130        1      0      1      0       0     False  \n",
       "5749131        1      0      1      1       0     False  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fdc05b-cd85-45cd-ba2e-2539460ca8ae",
   "metadata": {},
   "source": [
    "---\n",
    "‚öôÔ∏è **1. Dipende dalla dimensione totale in memoria**\n",
    "\n",
    "Pandas lavora tutto in RAM.\n",
    "\n",
    "Esempio:\n",
    "- 5 milioni di righe √ó 50 colonne\n",
    "- ogni cella occupa ~8 byte (`float64`)<br>\n",
    "üëâ $5.000.000 √ó 50 √ó 8 ‚âà 2 GB$\n",
    "\n",
    "Quindi un file CSV da 200 MB pu√≤ diventare **2‚Äì3 GB in RAM** una volta caricato, per via della conversione in tipi numerici, indici, metadati ecc.\n",
    "\n",
    "Se si ha un PC con **16 GB di RAM**, va bene; se si hanno 8 GB, pandas ci riesce ma sar√† lento e potremmo vedere ‚Äúswap‚Äù o crash per mancanza di memoria.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67f3b2d-f650-497d-a2d3-d473ba7ba953",
   "metadata": {},
   "source": [
    "üß† **2. Operazioni che pandas gestisce bene anche con 5M di righe**\n",
    "\n",
    "Con hardware \"decente\" (CPU moderna, 16 GB RAM) pandas gestisce tranquillamente:\n",
    "\n",
    "‚úÖ **Caricamento CSV**\n",
    "```python\n",
    "df = pd.read_csv(\"dati.csv\")\n",
    "```\n",
    "\n",
    "Si pu√≤ anche usare:\n",
    "- `dtype=` per tipizzare meglio le colonne (meno RAM);\n",
    "- `usecols=` per leggere solo alcune colonne;\n",
    "- `chunksize=` per leggere a blocchi.\n",
    "\n",
    "‚úÖ **Operazioni elementari e aggregazioni**\n",
    "- `df.describe()`, `df.mean()`, `df.groupby(\"col\").agg(...)`\n",
    "- `df.sort_values(\"col\")`\n",
    "- `df.query(\"x > 10 and y < 5\")`\n",
    "- `df.sample(100_000)`<br>\n",
    "tutte fattibili.\n",
    "\n",
    "‚úÖ **Join e merge moderati**<br>\n",
    "Fino a qualche milione di righe per tabella:\n",
    "```python\n",
    "pd.merge(df1, df2, on=\"id\", how=\"inner\")\n",
    "```\n",
    "funziona, ma attenzione ai picchi di memoria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d718ddf-6d75-48f4-86a2-8c2ed09d83d0",
   "metadata": {},
   "source": [
    "---\n",
    "üö´ **Operazioni che iniziano a diventare problematiche**\n",
    "\n",
    "Quando il dataset supera **i 5‚Äì10 milioni di righe o supera i 5 GB in RAM**, ecco cosa rallenta o esplode:\n",
    "\n",
    "‚ùå **ordinamenti multipli o sort complessi**\n",
    "```pythoon\n",
    "df.sort_values([\"col1\", \"col2\"])\n",
    "```\n",
    "Crea una copia in memoria grande quanto il DataFrame stesso.\n",
    "\n",
    "‚ùå **merge / join molto grandi**<br>\n",
    "se le due tabelle insieme superano la RAM disponibile.\n",
    "\n",
    "‚ùå **apply / lambda riga per riga**\n",
    "```python\n",
    "df.apply(lambda row: f(row.x), axis=1)\n",
    "```\n",
    "\n",
    "Molto lente: infatti sono eseguite in Python puro, non in C.<br>\n",
    "Meglio usare funzioni **vectorized** (`np.where`, `pd.Series.map`, ecc.).\n",
    "\n",
    "‚ùå **operazioni iterative**<br>\n",
    "Cicli `for row in df.itertuples()` su milioni di righe ‚Üí un disastro!\n",
    "\n",
    "‚ùå **Scrittura su CSV/parquet**\n",
    "```python\n",
    "df.to_csv(\"file.csv\")\n",
    "```\n",
    "\n",
    "Poco efficiente. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cbedac-bee5-4376-9f2d-50b5df007d8f",
   "metadata": {},
   "source": [
    "---\n",
    "‚ö° **Alternative e strategie**\n",
    "\n",
    "**1. Usare il *chunking***\n",
    "\n",
    "Il seguente codice √® **rischioso**\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dati.csv\")\n",
    "df[\"media\"] = df[\"valore\"].mean()\n",
    "\n",
    "```\n",
    "\n",
    "Meglio leggere a blocchi e processare iterativamente:\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "chunksize = 500.000   # legge 500 mila righe per volta\n",
    "risultati = []        # lista dove accumulare i risultati\n",
    "\n",
    "for chunk in pd.read_csv(\"dati.csv\", chunksize=chunksize):\n",
    "    media_chunk = chunk[\"valore\"].mean()       # calcolo sulla parte letta\n",
    "    risultati.append(media_chunk)              # salvo il risultato parziale\n",
    "\n",
    "# dopo il ciclo puoi combinare i risultati\n",
    "media_totale = sum(risultati) / len(risultati)\n",
    "print(\"Media complessiva:\", media_totale)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "**2. Usare il formato dati *parquet***<br>\n",
    "Vedi il prossimo capitolo.\n",
    "\n",
    "**3. Usare `cuDF`**<br>\n",
    "Utilizza la GPU senza modifiche al codice Pandas\n",
    "\n",
    "**4. Usare `Spark`**<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db88df-326e-44f1-b303-f6f1e7e64db3",
   "metadata": {},
   "source": [
    "# Il formato *parquet*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b851181-6e65-4abc-ae55-0defc23b36e8",
   "metadata": {},
   "source": [
    "Useremo laserie storica `usa_stocks_30m.parquet`: √® una serie OHLCV di 514 titoli del Nasdaq del NYSE (dal 1998 al 2024).\n",
    "\n",
    "Il dataset con cui lavoreremo √® un sottoinsieme del dataset [**USA 514 Stocks Prices NASDAQ NYSE**](https://www.kaggle.com/datasets/olegshpagin/usa-stocks-prices-ohlcv/data), anche disponibile su [Kaggle](https://www.kaggle.com/datasets), composto da circa **36 milioni** di elementi.\n",
    "\n",
    "Scarichiamo il dataset NON da Kaggle ma dal \"Public Google Cloud Storage bucket\" di NVIDIA, per garantire velocit√† di download maggiori.\n",
    "\n",
    "Il \"Public Google Cloud Storage bucket\" di NVIDIA √® uno spazio online dove NVIDIA mette a disposizione file pubblici (come dataset, modelli, esempi di codice) che chiunque pu√≤ scaricare.\n",
    "√à un po‚Äô come un grande armadio digitale aperto a tutti, ospitato su Google Cloud.\n",
    "\n",
    "Questo download richiede **circa 60 secondi**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42a21e07-e9a6-41ad-a212-58c4e9591723",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T11:07:44.454739Z",
     "iopub.status.busy": "2025-10-21T11:07:44.454485Z",
     "iopub.status.idle": "2025-10-21T11:08:48.677259Z",
     "shell.execute_reply": "2025-10-21T11:08:48.676694Z",
     "shell.execute_reply.started": "2025-10-21T11:07:44.454724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scarico il file usa_stocks_30m.parquet...\n",
      "Download completato.\n"
     ]
    }
   ],
   "source": [
    "# Download della big time series\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"usa_stocks_30m.parquet\"\n",
    "url = \"https://storage.googleapis.com/rapidsai/colab-data/usa_stocks_30m.parquet\"\n",
    "\n",
    "if not os.path.isfile(file_path):\n",
    "    print(f\"Scarico il file {file_path}...\")\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "    print(\"Download completato.\")\n",
    "else:\n",
    "    print(f\"{file_path} gi√† presente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980ec3c-2bb6-48ac-91f9-f3b51bfaf546",
   "metadata": {},
   "source": [
    "**Il file `usa_stocks_30m.parquet`**<br>\n",
    "\n",
    "Il file `usa_stocks_30m.parquet` √® un dataset messo a disposizione dal team RAPIDS (NVIDIA) per fare esempi di analisi su big time series finanziarie con librerie GPU-accelerate (tipo `cuDF`).\n",
    "\n",
    "üìå In pratica:\n",
    "- √à un file in **formato *Parquet*** (colonnare, compresso, molto efficiente per big data).\n",
    "- Contiene dati di **prezzi azionari USA** (titoli quotati) registrati con una **frequenza di 30 minuti**.\n",
    "- √à pensato per dimostrazioni: analisi di serie temporali, manipolazione con pandas/cuDF, benchmark CPU vs GPU.\n",
    "\n",
    "üìä Tipicamente include:\n",
    "- ticker ‚Üí il simbolo del titolo (es. AAPL, MSFT).\n",
    "- timestamp ‚Üí la data/ora della rilevazione (ogni 30 min).\n",
    "- open, high, low, close, volume (OHLCV) ‚Üí classici campi di trading.\n",
    "\n",
    "üìê Dimensioni indicative:\n",
    "- Circa **36 milioni di righe**,\n",
    "- Grandezza **~ 600‚Äì700 MB** in formato Parquet,\n",
    "- **Se convertito in CSV diventerebbe molto pi√π pesante (anche diversi GB)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2bcc0d",
   "metadata": {},
   "source": [
    "üëâ Il formato Parquet **√® molto pi√π efficiente di CSV**:\n",
    "- √® binario e compresso (occupa meno spazio);\n",
    "- √® colonnare ‚Üí pandas pu√≤ leggere solo le colonne necessarie;\n",
    "- conserva i tipi di dato (niente inferenza ogni volta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8f8500e-dc26-4e34-9d62-4c479d76d7fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T11:10:08.319709Z",
     "iopub.status.busy": "2025-10-21T11:10:08.319486Z",
     "iopub.status.idle": "2025-10-21T11:10:09.816654Z",
     "shell.execute_reply": "2025-10-21T11:10:09.816286Z",
     "shell.execute_reply.started": "2025-10-21T11:10:08.319695Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"usa_stocks_30m.parquet\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38c6387e-944b-4ac2-b893-ac306da0318b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T11:10:15.799215Z",
     "iopub.status.busy": "2025-10-21T11:10:15.799025Z",
     "iopub.status.idle": "2025-10-21T11:10:15.810892Z",
     "shell.execute_reply": "2025-10-21T11:10:15.810518Z",
     "shell.execute_reply.started": "2025-10-21T11:10:15.799201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999-11-18 17:00:00</td>\n",
       "      <td>45.56</td>\n",
       "      <td>50.00</td>\n",
       "      <td>45.50</td>\n",
       "      <td>46.00</td>\n",
       "      <td>9275000</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999-11-18 17:30:00</td>\n",
       "      <td>46.00</td>\n",
       "      <td>47.69</td>\n",
       "      <td>45.82</td>\n",
       "      <td>46.57</td>\n",
       "      <td>3200900</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999-11-18 18:00:00</td>\n",
       "      <td>46.56</td>\n",
       "      <td>46.63</td>\n",
       "      <td>41.00</td>\n",
       "      <td>41.00</td>\n",
       "      <td>3830500</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999-11-18 18:30:00</td>\n",
       "      <td>41.00</td>\n",
       "      <td>43.38</td>\n",
       "      <td>40.37</td>\n",
       "      <td>42.38</td>\n",
       "      <td>3688600</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999-11-18 19:00:00</td>\n",
       "      <td>42.31</td>\n",
       "      <td>42.44</td>\n",
       "      <td>41.56</td>\n",
       "      <td>41.69</td>\n",
       "      <td>1584300</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime   open   high    low  close   volume ticker\n",
       "0 1999-11-18 17:00:00  45.56  50.00  45.50  46.00  9275000      A\n",
       "1 1999-11-18 17:30:00  46.00  47.69  45.82  46.57  3200900      A\n",
       "2 1999-11-18 18:00:00  46.56  46.63  41.00  41.00  3830500      A\n",
       "3 1999-11-18 18:30:00  41.00  43.38  40.37  42.38  3688600      A\n",
       "4 1999-11-18 19:00:00  42.31  42.44  41.56  41.69  1584300      A"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8727b398-e653-4ea0-8325-b267c373595a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T11:10:21.336437Z",
     "iopub.status.busy": "2025-10-21T11:10:21.335997Z",
     "iopub.status.idle": "2025-10-21T11:10:21.340274Z",
     "shell.execute_reply": "2025-10-21T11:10:21.339849Z",
     "shell.execute_reply.started": "2025-10-21T11:10:21.336418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36087094, 7)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a888545a-9ee9-4253-be39-0ce7126d2cf3",
   "metadata": {},
   "source": [
    "# Nota tecnica sui PDF\n",
    "In VSC il rendering dei file PDF √® differente da quello di Jupyter Notebook/Lab e da quello di Google Colab.<br>\n",
    "La seguente funzione `show_pdf` rileva quale IDE √® attiva e \"rende\" il PDF in modo differente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ab319e3-b6e2-49d9-9221-66b650ba6f8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T09:10:50.296735Z",
     "iopub.status.busy": "2025-10-21T09:10:50.296470Z",
     "iopub.status.idle": "2025-10-21T09:10:50.301734Z",
     "shell.execute_reply": "2025-10-21T09:10:50.301396Z",
     "shell.execute_reply.started": "2025-10-21T09:10:50.296718Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_pdf(pdf_path, width=1000, height=600):\n",
    "    \"\"\"\n",
    "    Mostra un PDF nel modo pi√π appropriato per l'ambiente attuale:\n",
    "    - In Jupyter: visualizza inline con IFrame.\n",
    "    - In Colab: usa IFrame (gestisce bene i file caricati).\n",
    "    - In VS Code o altri ambienti: apre nel browser predefinito.\n",
    "    \"\"\"\n",
    "    import os, webbrowser, sys\n",
    "    from pathlib import Path\n",
    "\n",
    "    pdf_path = Path(pdf_path)\n",
    "    if not pdf_path.exists():\n",
    "        raise FileNotFoundError(f\"File non trovato: {pdf_path}\")\n",
    "\n",
    "    # Rileva ambiente\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "    except NameError:\n",
    "        shell = None\n",
    "\n",
    "    if shell == 'ZMQInteractiveShell':  # Jupyter o Colab\n",
    "        from IPython.display import IFrame, display\n",
    "        display(IFrame(str(pdf_path), width=width, height=height))\n",
    "    elif \"vscode\" in sys.executable.lower() or \"vscode\" in os.getcwd().lower():\n",
    "        # Ambiente VS Code ‚Üí apre nel browser\n",
    "        webbrowser.open(pdf_path.resolve().as_uri())\n",
    "        print(f\"üìÇ PDF aperto nel browser: {pdf_path}\")\n",
    "    else:\n",
    "        # Altri ambienti (terminali, script)\n",
    "        webbrowser.open(pdf_path.resolve().as_uri())\n",
    "        print(f\"üìÇ PDF aperto nel browser: {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e139d34b-1bdd-4a69-b5c0-cf5fd9cac163",
   "metadata": {},
   "source": [
    "# File JSON\n",
    "\n",
    "Un formato di file **non tabellare** ma di **frequente uso** in Python √® JSON.\n",
    "\n",
    "I file JSON (estensione *.json*) sono uno dei formati pi√π usati oggi per scambiare dati tra applicazioni, **specialmente sul web e in ambito API**.\n",
    "\n",
    "üí° **In breve**\n",
    "- JSON sta per **JavaScript Object Notation**<br>\n",
    "√à un formato **testuale**, <u>leggibile da umani e facilmente interpretabile dai programmi</u>, nato da JavaScript ma oggi usato in **praticamente tutti i linguaggi** (Python, Java, C#, PHP, ecc.).\n",
    "\n",
    "üì¶ **Struttura di un file JSON**\n",
    "\n",
    "Un file JSON contiene dati organizzati come **coppie chiave‚Äìvalore**, ad esempio:\n",
    "```json\n",
    "{\n",
    "  \"nome\": \"Antonio\",\n",
    "  \"eta\": 45,\n",
    "  \"iscritti\": [\"Mario\", \"Lucia\", \"Giorgio\"],\n",
    "  \"attivo\": true,\n",
    "  \"dettagli\": {\n",
    "    \"ruolo\": \"Analista\",\n",
    "    \"azienda\": \"ACI\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "üëÜ Questo √® un oggetto JSON, che contiene:\n",
    "- stringhe (\"`Antonio`\", \"`Analista`\")\n",
    "- numeri (`45`)\n",
    "- booleani (`true`)\n",
    "- liste (array) ([\"`Mario`\", \"`Lucia`\", \"`Giorgio`\"])\n",
    "- oggetti annidati (\"`dettagli\": {...}`)\n",
    "\n",
    "**In Python**<br>\n",
    "Puoi leggere o scrivere file JSON facilmente con il modulo json:\n",
    "```python\n",
    "    import json\n",
    "\n",
    "    # Lettura\n",
    "    with open(\"dati.json\", \"r\") as f:\n",
    "        dati = json.load(f)\n",
    "    print(dati[\"nome\"])  # -> Antonio\n",
    "\n",
    "    # Scrittura\n",
    "    nuovi_dati = {\"linguaggio\": \"Python\", \"versione\": 3.12}\n",
    "    with open(\"config.json\", \"w\") as f:\n",
    "        json.dump(nuovi_dati, f, indent=4)\n",
    "```\n",
    "\n",
    "**Dove si usa JSON?**\n",
    "- API REST (quasi tutte le API moderne usano JSON per scambiare dati)\n",
    "- Configurazioni (es. package.json in Node.js)\n",
    "- Database NoSQL come MongoDB (che usa BSON, una versione binaria di JSON)\n",
    "- Applicazioni web e mobile per passare dati tra frontend e backend\n",
    "\n",
    "üÜö Confronto rapido JSON vs CSV vs XML\n",
    "![](json_sintesi_2.png)\n",
    "\n",
    "**Come convertire un file CSV o un dizionario Python in JSON e viceversa (cio√® import/export completo)**\n",
    "\n",
    "1Ô∏è‚É£ **Dizionario Python ‚Üí JSON (scrittura)**\n",
    "\n",
    "```python\n",
    "    import json\n",
    "\n",
    "    dati = {\n",
    "        \"nome\": \"Antonio\",\n",
    "        \"eta\": 45,\n",
    "        \"linguaggi\": [\"Python\", \"SQL\", \"R\"],\n",
    "        \"attivo\": True\n",
    "    }\n",
    "\n",
    "    # Scrivi su file JSON\n",
    "    with open(\"dati.json\", \"w\") as f:\n",
    "        json.dump(dati, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(\"‚úÖ File JSON creato!\")\n",
    "```\n",
    "\n",
    "**Risultato** (`dati.json`):\n",
    "```json\n",
    "{\n",
    "    \"nome\": \"Antonio\",\n",
    "    \"eta\": 45,\n",
    "    \"linguaggi\": [\"Python\", \"SQL\", \"R\"],\n",
    "    \"attivo\": true\n",
    "}\n",
    "```\n",
    "\n",
    "üî∏ `indent=4` ‚Üí rende il file leggibile<br>\n",
    "üî∏ `ensure_ascii=False` ‚Üí mantiene i caratteri accentati\n",
    "\n",
    "2Ô∏è‚É£ **JSON ‚Üí Dizionario Python (lettura)**\n",
    "```python\n",
    "    import json\n",
    "\n",
    "    with open(\"dati.json\", \"r\") as f:\n",
    "        dati_letti = json.load(f)\n",
    "\n",
    "    print(dati_letti[\"nome\"])     # Antonio\n",
    "    print(type(dati_letti))       # dict\n",
    "```\n",
    "\n",
    "3Ô∏è‚É£ **CSV ‚Üí JSON**\n",
    "\n",
    "Immaginiamo un file `clienti.csv` cos√¨:\n",
    "\n",
    "*nome,citta,eta*<br>\n",
    "*Mario,Roma,30*<br>\n",
    "*Lucia,Milano,28*<br>\n",
    "*Giorgio,Napoli,35*<br>\n",
    "\n",
    "Convertiamolo in JSON:\n",
    "```python\n",
    "    import csv\n",
    "    import json\n",
    "\n",
    "    with open(\"clienti.csv\", \"r\") as f_csv:\n",
    "        reader = csv.DictReader(f_csv)\n",
    "        dati = list(reader)\n",
    "\n",
    "    with open(\"clienti.json\", \"w\") as f_json:\n",
    "        json.dump(dati, f_json, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(\"‚úÖ CSV convertito in JSON!\")\n",
    "```\n",
    "\n",
    "**Output** (`clienti.json`):\n",
    "\n",
    "[<br>\n",
    "    {\"nome\": \"Mario\", \"citta\": \"Roma\", \"eta\": \"30\"},<br>\n",
    "    {\"nome\": \"Lucia\", \"citta\": \"Milano\", \"eta\": \"28\"},<br>\n",
    "    {\"nome\": \"Giorgio\", \"citta\": \"Napoli\", \"eta\": \"35\"}<br>\n",
    "]\n",
    "\n",
    "4Ô∏è‚É£ **JSON ‚Üí CSV**\n",
    "\n",
    "Ora facciamo l‚Äôinverso:\n",
    "```python\n",
    "    import json\n",
    "    import csv\n",
    "\n",
    "    with open(\"clienti.json\", \"r\") as f_json:\n",
    "        dati = json.load(f_json)\n",
    "\n",
    "    with open(\"clienti_out.csv\", \"w\", newline=\"\") as f_csv:\n",
    "        writer = csv.DictWriter(f_csv, fieldnames=dati[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(dati)\n",
    "\n",
    "    print(\"‚úÖ JSON convertito in CSV!\")\n",
    "```\n",
    "\n",
    "\n",
    "![](json_sintesi.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7edba68",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
