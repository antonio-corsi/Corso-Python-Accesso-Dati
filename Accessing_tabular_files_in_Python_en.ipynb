{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d5a227-e219-418b-8d04-f62224e656c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:35:39.679553Z",
     "iopub.status.busy": "2025-10-21T08:35:39.679228Z",
     "iopub.status.idle": "2025-10-21T08:35:39.683523Z",
     "shell.execute_reply": "2025-10-21T08:35:39.683015Z",
     "shell.execute_reply.started": "2025-10-21T08:35:39.679537Z"
    },
    "id": "d9d5a227-e219-418b-8d04-f62224e656c5"
   },
   "source": [
    "<font size=\"6\">**Reading tabular files in Python**</font><br>\n",
    "\n",
    "> (c) 2025 Antonio Piemontese"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b6f42-42b8-4cc7-94ea-92ec013eb6a5",
   "metadata": {
    "id": "4b0b6f42-42b8-4cc7-94ea-92ec013eb6a5"
   },
   "source": [
    "# Data in Python (in Data Science)\n",
    "In Data Science we are often interested in:\n",
    "- analysing **past data**, not in real time\n",
    "- analysing a **single file**, not the DB\n",
    "\n",
    "These data are usually **tabular files**, so called because they are <u>made of rows and columns</u> (2 dimensions). They are usually **created by users**, received **from other companies**, or **simply exported from a DB** (as an export).\n",
    "\n",
    "One of the most widespread tabular formats for importing data into Python is the [**CSV**](https://en.wikipedia.org/wiki/Comma-separated_values) format. It is practically **ubiquitous in all tabular data‚Äëmanagement tools and environments**: Excel, Google Sheets, all relational DBs, etc.\n",
    "\n",
    "Even though in Python you can load **any kind of file** (xml, json, PDF, txt, etc.) and access **any kind of database** (Oracle, PostgreSQL, MySQL, etc.), the simplest and most efficient format to load in memory (into a `pandas` dataframe) is CSV.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data.csv')\n",
    "df = pd.read_excel('data.xlsx')\n",
    "```\n",
    "\n",
    "If you want **more performant** alternatives, nowadays `cuDF` (with GPU) or `Polars` are used a lot.\n",
    "\n",
    "<p style=\"color:red; font-size:18px; font-weight:bold;\">\n",
    "üö® Tabular files (and also the SQL tables we will see later) are usually loaded in a pandas dataframe üö®\n",
    "</p>\n",
    "Pandas is convenient but it is not the only way to import tabular files in Python.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae679e73-95ba-4316-836a-6186867428c8",
   "metadata": {
    "id": "ae679e73-95ba-4316-836a-6186867428c8"
   },
   "source": [
    "> ‚ÄúA CSV or Excel file (**a *tabular* file**) may look like a table as in the database, but it is only a container of raw data.\n",
    "An SQL table instead is a controlled structure, with rules, types and relations, which the database manages in a consistent, safe and transactional way.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e0dab-91fb-4180-90ef-6a9ad3acbba1",
   "metadata": {
    "id": "0f7e0dab-91fb-4180-90ef-6a9ad3acbba1"
   },
   "source": [
    "# Reading tabular files through specific libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fabb53-a6c8-4127-902f-e4e40e57bdaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T14:10:19.887968Z",
     "iopub.status.busy": "2025-10-21T14:10:19.887715Z",
     "iopub.status.idle": "2025-10-21T14:10:19.891475Z",
     "shell.execute_reply": "2025-10-21T14:10:19.890966Z",
     "shell.execute_reply.started": "2025-10-21T14:10:19.887952Z"
    },
    "id": "a1fabb53-a6c8-4127-902f-e4e40e57bdaf"
   },
   "source": [
    "To read <u>csv</u> tabular files a **first option** is the **`csv` library**, i.e. Python‚Äôs **built‚Äëin CSV**.<br>\n",
    "The reason for this choice, which however has many limits, is to avoid loading the *pandas* package (heavy) and to avoid dependency problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09fef0bb-3656-4cab-b542-baeef33d366c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:15:17.495587Z",
     "iopub.status.busy": "2025-10-24T18:15:17.495287Z",
     "iopub.status.idle": "2025-10-24T18:15:17.500288Z",
     "shell.execute_reply": "2025-10-24T18:15:17.499950Z",
     "shell.execute_reply.started": "2025-10-24T18:15:17.495572Z"
    },
    "id": "09fef0bb-3656-4cab-b542-baeef33d366c",
    "outputId": "705fc084-5d20-4861-e33b-6633a9eb95e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'ID', 'Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education', 'Gender', 'Student', 'Married', 'Ethnicity', 'Balance']\n",
      "['1', '1', '14.891', '3606', '283', '2', '34', '11', ' Male', 'No', 'Yes', 'Caucasian', '333']\n",
      "['2', '2', '106.025', '6645', '483', '3', '82', '15', 'Female', 'Yes', 'Yes', 'Asian', '903']\n",
      "['3', '3', '104.593', '7075', '514', '4', '71', '11', ' Male', 'No', 'No', 'Asian', '580']\n",
      "['4', '4', '148.924', '9504', '681', '3', '36', '11', 'Female', 'No', 'No', 'Asian', '964']\n",
      "['5', '5', '55.882', '4897', '357', '2', '68', '16', ' Male', 'No', 'Yes', 'Caucasian', '331']\n",
      "['6', '6', '80.18', '8047', '569', '4', '77', '10', ' Male', 'No', 'No', 'Caucasian', '1151']\n",
      "['7', '7', '20.996', '3388', '259', '2', '37', '12', 'Female', 'No', 'No', 'African American', '203']\n",
      "['8', '8', '71.408', '7114', '512', '2', '87', '9', ' Male', 'No', 'No', 'Asian', '872']\n",
      "['9', '9', '15.125', '3300', '266', '5', '66', '13', 'Female', 'No', 'No', 'Caucasian', '279']\n",
      "['10', '10', '71.061', '6819', '491', '3', '41', '19', 'Female', 'Yes', 'Yes', 'African American', '1350']\n",
      "['11', '11', '63.095', '8117', '589', '4', '30', '14', ' Male', 'No', 'Yes', 'Caucasian', '1407']\n",
      "['12', '12', '15.045', '1311', '138', '3', '64', '16', ' Male', 'No', 'No', 'Caucasian', '0']\n",
      "['13', '13', '80.616', '5308', '394', '1', '57', '7', 'Female', 'No', 'Yes', 'Asian', '204']\n",
      "['14', '14', '43.682', '6922', '511', '1', '49', '9', ' Male', 'No', 'Yes', 'Caucasian', '1081']\n",
      "['15', '15', '19.144', '3291', '269', '2', '75', '13', 'Female', 'No', 'No', 'African American', '148']\n",
      "['16', '16', '20.089', '2525', '200', '3', '57', '15', 'Female', 'No', 'Yes', 'African American', '0']\n",
      "['17', '17', '53.598', '3714', '286', '3', '73', '17', 'Female', 'No', 'Yes', 'African American', '0']\n",
      "['18', '18', '36.496', '4378', '339', '3', '69', '15', 'Female', 'No', 'Yes', 'Asian', '368']\n",
      "['19', '19', '49.57', '6384', '448', '1', '28', '9', 'Female', 'No', 'Yes', 'Asian', '891']\n",
      "['20', '20', '42.079', '6626', '479', '2', '44', '9', ' Male', 'No', 'No', 'Asian', '1048']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"Credit_ISLR.csv\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        print(row)\n",
    "        if i >= 20:   # print only the first 20 rows (0‚Äì19)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf0b1aa-8afa-40e2-8549-a4a916c5d04d",
   "metadata": {
    "id": "1cf0b1aa-8afa-40e2-8549-a4a916c5d04d"
   },
   "source": [
    "It is **very lightweight**, but you have to handle **everything manually** (types, header, encoding...):\n",
    "- type handling (everything is a string) ‚Äì the `csv` module does not automatically convert types: everything it reads is a string.\n",
    "- header handled manually ‚Äì the `csv` module does not know by itself whether the first line is a header or data.\n",
    "- encoding issues ‚Äì if the file is not UTF‚Äë8 (e.g. it is `latin-1` or `windows-1252`), `open()` will raise an error or show strange characters.\n",
    "- different separators (`,` or `;` or `\\t`) ‚Äì CSV files are not always comma‚Äëseparated ‚Äî in Italy often `;` or tabs.\n",
    "- quotes and special characters ‚Äì if a field contains a comma or a newline, parsing can break if you don‚Äôt use the right parameters.\n",
    "- missing data (empty cells) ‚Äì there is no concept of `NaN`.\n",
    "- with millions of rows, `csv.reader` is faster than pandas at reading raw rows, but then you cannot easily filter, join, or operate on data.\n",
    "\n",
    "**In short**:<br>\n",
    "Using plain `csv` is like reading the file ‚Äúby hand‚Äù: we have full control but all the work is on us. *pandas* (or *Polars*) instead understand header, types, separators, encoding, missing, etc. automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c553916-4ccd-46c2-b099-4dd448006f6a",
   "metadata": {
    "id": "3c553916-4ccd-46c2-b099-4dd448006f6a"
   },
   "source": [
    "A **second option** is the **`openpyxl`** module (for Excel **.xlsx** files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "447e03bf-b3ed-4816-bfd1-830e2031f9b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:15:26.579532Z",
     "iopub.status.busy": "2025-10-24T18:15:26.579333Z",
     "iopub.status.idle": "2025-10-24T18:15:27.063881Z",
     "shell.execute_reply": "2025-10-24T18:15:27.063340Z",
     "shell.execute_reply.started": "2025-10-24T18:15:26.579518Z"
    },
    "id": "447e03bf-b3ed-4816-bfd1-830e2031f9b3",
    "outputId": "2e7ecdf9-e515-413e-9b0f-1cf2df2ceb30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Column1', 'ID', 'Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education', 'Gender', 'Student', 'Married', 'Ethnicity', 'Balance')\n",
      "(1, 1, 14891, 3606, 283, 2, 34, 11, ' Male', 'No', 'Yes', 'Caucasian', 333)\n",
      "(2, 2, 106025, 6645, 483, 3, 82, 15, 'Female', 'Yes', 'Yes', 'Asian', 903)\n",
      "(3, 3, 104593, 7075, 514, 4, 71, 11, ' Male', 'No', 'No', 'Asian', 580)\n",
      "(4, 4, 148924, 9504, 681, 3, 36, 11, 'Female', 'No', 'No', 'Asian', 964)\n",
      "(5, 5, 55882, 4897, 357, 2, 68, 16, ' Male', 'No', 'Yes', 'Caucasian', 331)\n",
      "(6, 6, 8018, 8047, 569, 4, 77, 10, ' Male', 'No', 'No', 'Caucasian', 1151)\n",
      "(7, 7, 20996, 3388, 259, 2, 37, 12, 'Female', 'No', 'No', 'African American', 203)\n",
      "(8, 8, 71408, 7114, 512, 2, 87, 9, ' Male', 'No', 'No', 'Asian', 872)\n",
      "(9, 9, 15125, 3300, 266, 5, 66, 13, 'Female', 'No', 'No', 'Caucasian', 279)\n",
      "(10, 10, 71061, 6819, 491, 3, 41, 19, 'Female', 'Yes', 'Yes', 'African American', 1350)\n",
      "(11, 11, 63095, 8117, 589, 4, 30, 14, ' Male', 'No', 'Yes', 'Caucasian', 1407)\n",
      "(12, 12, 15045, 1311, 138, 3, 64, 16, ' Male', 'No', 'No', 'Caucasian', 0)\n",
      "(13, 13, 80616, 5308, 394, 1, 57, 7, 'Female', 'No', 'Yes', 'Asian', 204)\n",
      "(14, 14, 43682, 6922, 511, 1, 49, 9, ' Male', 'No', 'Yes', 'Caucasian', 1081)\n",
      "(15, 15, 19144, 3291, 269, 2, 75, 13, 'Female', 'No', 'No', 'African American', 148)\n",
      "(16, 16, 20089, 2525, 200, 3, 57, 15, 'Female', 'No', 'Yes', 'African American', 0)\n",
      "(17, 17, 53598, 3714, 286, 3, 73, 17, 'Female', 'No', 'Yes', 'African American', 0)\n",
      "(18, 18, 36496, 4378, 339, 3, 69, 15, 'Female', 'No', 'Yes', 'Asian', 368)\n",
      "(19, 19, 4957, 6384, 448, 1, 28, 9, 'Female', 'No', 'Yes', 'Asian', 891)\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "wb = load_workbook(\"Credit_ISLR.xlsx\")\n",
    "ws = wb.active\n",
    "\n",
    "for i, row in enumerate(ws.iter_rows(values_only=True)):\n",
    "    print(row)\n",
    "    if i >= 19:    # index starts from 0 ‚Üí 0‚Äì19 = 20 rows\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e637d41-8fbc-4d8e-a75d-5525141c1167",
   "metadata": {
    "id": "0e637d41-8fbc-4d8e-a75d-5525141c1167"
   },
   "source": [
    "A **third** option is the `xlrd` or `xlwt` module ‚Äì still for Excel ‚Äì for reading and writing `.xls`.<br>\n",
    "‚ö†Ô∏è Deprecated for `.xlsx`, so today they are less recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e830e7-b719-4de9-a97a-833f78a85cd2",
   "metadata": {
    "id": "83e830e7-b719-4de9-a97a-833f78a85cd2"
   },
   "source": [
    "So, in a nutshell:<br>\n",
    "![](sintesi_formati_tabellari_en.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4687b0-f8d9-414a-a71e-05b9fc4226a6",
   "metadata": {
    "id": "2e4687b0-f8d9-414a-a71e-05b9fc4226a6"
   },
   "source": [
    "# Loading into pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d825a892-c123-438e-83f0-ef3b519f894c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T14:46:39.234929Z",
     "iopub.status.busy": "2025-10-21T14:46:39.234603Z",
     "iopub.status.idle": "2025-10-21T14:46:39.238351Z",
     "shell.execute_reply": "2025-10-21T14:46:39.237877Z",
     "shell.execute_reply.started": "2025-10-21T14:46:39.234914Z"
    },
    "id": "d825a892-c123-438e-83f0-ef3b519f894c"
   },
   "source": [
    "If you only want to read/write (tabular) files without installing heavy external libraries, you can use `csv` or `openpyxl`, as shown before.\n",
    "\n",
    "Otherwise, you use the [**dataframes**](https://en.wikipedia.org/wiki/Pandas_(software)#DataFrames), which are the **most used data structure in Data Science**:\n",
    "- they live in memory\n",
    "- they can be loaded from disk or saved to disk with the `pd.read_***` functions or with the `df.to_XXX` methods for the following formats:<br>\n",
    "*clipboard, csv, excel, html, json, parquet, pickle, sas, sql, spss, stata, xml*.\n",
    "\n",
    "To load a *csv* or *xlsx* tabular file into a dataframe you use these two *pandas* functions:\n",
    "```python\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('data.csv')\n",
    "    df = pd.read_excel('data.xlsx')\n",
    "```\n",
    "\n",
    "If you want **more performant** alternatives, nowadays `cuDF` (with GPU) or `Polars` are used a lot.\n",
    "\n",
    "<p style=\"color:red; font-size:18px; font-weight:bold;\">\n",
    "üö® Tabular files (and also the SQL tables we will see later) are usually loaded in a pandas dataframe üö®\n",
    "</p>\n",
    "Pandas is convenient but it is not the only way to import tabular files in Python.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad194c6-cfc5-4629-acff-81f2ed49d549",
   "metadata": {
    "id": "aad194c6-cfc5-4629-acff-81f2ed49d549"
   },
   "source": [
    "# Excel or csv?\n",
    "What is the best format to import tabular files? Excel or csv?<br>\n",
    "It depends on the goal and on the context, but **in most cases CSV is more efficient, transparent and robust, whereas Excel is more convenient for the human user**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d64c30a-84f9-4f1d-a18f-842a6a4b31d9",
   "metadata": {
    "id": "9d64c30a-84f9-4f1d-a18f-842a6a4b31d9"
   },
   "source": [
    "---\n",
    "\n",
    "Let‚Äôs compare **CSV vs Excel** from the <u>technical</u> point of view and from the <u>human</u> point of view.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5d500-d1ee-4b7d-bb33-87f8d95e15d5",
   "metadata": {
    "id": "95b5d500-d1ee-4b7d-bb33-87f8d95e15d5"
   },
   "source": [
    "---\n",
    "**\"External libraries\"**: what does it mean?<br>\n",
    "The function `pd.read_excel()` is built into pandas, but it does not do *everything* by itself: for some Excel formats it relies on **external libraries** that actually handle the Excel format.<br>\n",
    "That is, how does `pd.read_excel()` really work?<br>\n",
    "When you call:\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_excel(\"data.xlsx\")\n",
    "```\n",
    "pandas:\n",
    "- recognises the file format (e.g. `.xls`, `.xlsx`, `.xlsb`)\n",
    "- uses an external ‚Äúengine‚Äù to read the data\n",
    "- turns what it reads into a `DataFrame`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693a008-8fd0-47b2-ba08-c65f68a81034",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T15:06:22.273393Z",
     "iopub.status.busy": "2025-10-21T15:06:22.273077Z",
     "iopub.status.idle": "2025-10-21T15:06:22.291626Z",
     "shell.execute_reply": "2025-10-21T15:06:22.291290Z",
     "shell.execute_reply.started": "2025-10-21T15:06:22.273373Z"
    },
    "id": "7693a008-8fd0-47b2-ba08-c65f68a81034"
   },
   "source": [
    "**3. Performance: indicative comparison**:<br>\n",
    "![](performance_csv_excel_en.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e55ed59-708f-4da5-913c-3c84dd371dde",
   "metadata": {
    "id": "6e55ed59-708f-4da5-913c-3c84dd371dde"
   },
   "source": [
    "**4. In practice**:<br>\n",
    "üëâ If the file **comes from a system or an application** (ERP, CRM, management, accounting, ‚Ä¶) it is almost always better to receive it as **CSV**.<br>\n",
    "üëâ If the file **is made by a person** and must be read by a person, Excel is nicer, but as soon as you need to automate the processing, CSV (or another machine‚Äëfriendly format) becomes preferable.<br>\n",
    "\n",
    "üëâ In any case you can always convert Excel ‚Üí csv with `pandas.to_csv()` or `to_parquet()` for internal / efficient storage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cfda21-e530-405e-8f22-4d3672241961",
   "metadata": {
    "id": "33cfda21-e530-405e-8f22-4d3672241961"
   },
   "source": [
    "# The importance and spread of the *csv* format\n",
    "Let‚Äôs see in more detail **why the CSV format is so ubiquitous in the data world**: practically one cannot imagine a tool for managing tabular data (Excel, Google Sheets, LibreOffice Calc, most reporting, BI and relational DB tools, etc.) that does not allow csv import/export."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e00a387",
   "metadata": {
    "id": "9e00a387"
   },
   "source": [
    "# The importance and diffusion of the *csv* format\n",
    "Let‚Äôs see in more detail **why the CSV format is so ubiquitous in the data world** (it is found everywhere, always).\n",
    "It is a format supported by almost all tools (Excel, DBs, BI, reporting, data‚Äëscience tools...).\n",
    "![](importanza_csv_en.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4810542f",
   "metadata": {
    "id": "4810542f"
   },
   "source": [
    "# Technical reasons for the diffusion of csv\n",
    "There are several **very concrete technical reasons** that explain why the CSV format is so omnipresent in the data world.<br>\n",
    "Here is a clear and technical summary table:\n",
    "\n",
    "![](diffusione_csv_en.png)\n",
    "\n",
    "üí° In short:<br>\n",
    "CSV is the **‚Äúlowest common denominator‚Äù of tabular data**: simple, textual, line‚Äëbased, without dependencies and compatible with everything ‚Äî from Excel to Spark.<br>\n",
    "It is not perfect (no types, schema, compression or metadata), but exactly **its structural poverty is its strength**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d45255-be27-4a5d-9932-790901b179e2",
   "metadata": {
    "id": "99d45255-be27-4a5d-9932-790901b179e2"
   },
   "source": [
    "# Reading CSV files in pandas\n",
    "\n",
    "As said, in Data Science we often are NOT interested in the online DB but in **a local file**, which can be of various formats (csv, json, parquet, etc.).\n",
    "\n",
    "## 3 technical notes on the CSV format\n",
    "\n",
    "* the two main arguments of the pandas `read_csv` method are the **column separator** (default `,`) and the **presence** (and possible number) of heading rows (header).\n",
    "* there are different csv formats available from Excel; you must choose the correct one (see the YouTube video of *Excel Tutorials by EasyClick Academy*).\n",
    "  \n",
    "  ![](tipi_csv_en.png)\n",
    "* [pros and cons](https://towardsdatascience.com/why-i-stopped-dumping-dataframes-to-a-csv-and-why-you-should-too-c0954c410f8f) of the csv format\n",
    "\n",
    "A csv file is textual and therefore can also be read in Notepad.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae4ec6c-ed02-4ecc-b78c-b8feb1efda58",
   "metadata": {
    "id": "cae4ec6c-ed02-4ecc-b78c-b8feb1efda58"
   },
   "source": [
    "Let‚Äôs load in *pandas* the well‚Äëknown banking file `Credit_ISLR`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64f8f076-dc8b-488e-8659-e63b4e9f43d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:43:33.738874Z",
     "iopub.status.busy": "2025-10-21T08:43:33.738657Z",
     "iopub.status.idle": "2025-10-21T08:43:33.761516Z",
     "shell.execute_reply": "2025-10-21T08:43:33.761072Z",
     "shell.execute_reply.started": "2025-10-21T08:43:33.738860Z"
    },
    "id": "64f8f076-dc8b-488e-8659-e63b4e9f43d8",
    "outputId": "b5f5374a-7219-4142-9354-234ed26c8527"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.891</td>\n",
       "      <td>3606</td>\n",
       "      <td>283</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>106.025</td>\n",
       "      <td>6645</td>\n",
       "      <td>483</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Asian</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>104.593</td>\n",
       "      <td>7075</td>\n",
       "      <td>514</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>148.924</td>\n",
       "      <td>9504</td>\n",
       "      <td>681</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>55.882</td>\n",
       "      <td>4897</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>396</td>\n",
       "      <td>396</td>\n",
       "      <td>12.096</td>\n",
       "      <td>4100</td>\n",
       "      <td>307</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>397</td>\n",
       "      <td>397</td>\n",
       "      <td>13.364</td>\n",
       "      <td>3838</td>\n",
       "      <td>296</td>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "      <td>17</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>African American</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>398</td>\n",
       "      <td>398</td>\n",
       "      <td>57.872</td>\n",
       "      <td>4171</td>\n",
       "      <td>321</td>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>399</td>\n",
       "      <td>399</td>\n",
       "      <td>37.728</td>\n",
       "      <td>2525</td>\n",
       "      <td>192</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>13</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>18.701</td>\n",
       "      <td>5524</td>\n",
       "      <td>415</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows √ó 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0   ID   Income  Limit  Rating  Cards  Age  Education  Gender  \\\n",
       "0             1    1   14.891   3606     283      2   34         11    Male   \n",
       "1             2    2  106.025   6645     483      3   82         15  Female   \n",
       "2             3    3  104.593   7075     514      4   71         11    Male   \n",
       "3             4    4  148.924   9504     681      3   36         11  Female   \n",
       "4             5    5   55.882   4897     357      2   68         16    Male   \n",
       "..          ...  ...      ...    ...     ...    ...  ...        ...     ...   \n",
       "395         396  396   12.096   4100     307      3   32         13    Male   \n",
       "396         397  397   13.364   3838     296      5   65         17    Male   \n",
       "397         398  398   57.872   4171     321      5   67         12  Female   \n",
       "398         399  399   37.728   2525     192      1   44         13    Male   \n",
       "399         400  400   18.701   5524     415      5   64          7  Female   \n",
       "\n",
       "    Student Married         Ethnicity  Balance  \n",
       "0        No     Yes         Caucasian      333  \n",
       "1       Yes     Yes             Asian      903  \n",
       "2        No      No             Asian      580  \n",
       "3        No      No             Asian      964  \n",
       "4        No     Yes         Caucasian      331  \n",
       "..      ...     ...               ...      ...  \n",
       "395      No     Yes         Caucasian      560  \n",
       "396      No      No  African American      480  \n",
       "397      No     Yes         Caucasian      138  \n",
       "398      No     Yes         Caucasian        0  \n",
       "399      No      No             Asian      966  \n",
       "\n",
       "[400 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_credit = pd.read_csv(\"Credit_ISLR.csv\",header=0)\n",
    "df_credit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f89a5-9591-40a8-8bfb-75d1643f41ae",
   "metadata": {
    "id": "973f89a5-9591-40a8-8bfb-75d1643f41ae"
   },
   "source": [
    "As you can see, the *pandas* `read_csv` function automatically created the column with index, so the original index `ID` is now <u>redundant</u>, and it added a column `Unnamed: 0` (we will see later why). It is good to drop both because they are useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adedb20e-72fa-4c93-95ed-4a48b2853d83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:51:08.184431Z",
     "iopub.status.busy": "2025-10-21T08:51:08.183921Z",
     "iopub.status.idle": "2025-10-21T08:51:08.191669Z",
     "shell.execute_reply": "2025-10-21T08:51:08.191323Z",
     "shell.execute_reply.started": "2025-10-21T08:51:08.184414Z"
    },
    "id": "adedb20e-72fa-4c93-95ed-4a48b2853d83"
   },
   "outputs": [],
   "source": [
    "df_credit.drop(columns=['Unnamed: 0', 'ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64d44fbb-2b10-4079-9d36-05411f3ce003",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:51:24.310829Z",
     "iopub.status.busy": "2025-10-21T08:51:24.310609Z",
     "iopub.status.idle": "2025-10-21T08:51:24.318242Z",
     "shell.execute_reply": "2025-10-21T08:51:24.317874Z",
     "shell.execute_reply.started": "2025-10-21T08:51:24.310813Z"
    },
    "id": "64d44fbb-2b10-4079-9d36-05411f3ce003",
    "outputId": "d4613669-fb74-4ea0-940d-e6dcd22ae105"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.891</td>\n",
       "      <td>3606</td>\n",
       "      <td>283</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106.025</td>\n",
       "      <td>6645</td>\n",
       "      <td>483</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Asian</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104.593</td>\n",
       "      <td>7075</td>\n",
       "      <td>514</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148.924</td>\n",
       "      <td>9504</td>\n",
       "      <td>681</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.882</td>\n",
       "      <td>4897</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Income  Limit  Rating  Cards  Age  Education  Gender Student Married  \\\n",
       "0   14.891   3606     283      2   34         11    Male      No     Yes   \n",
       "1  106.025   6645     483      3   82         15  Female     Yes     Yes   \n",
       "2  104.593   7075     514      4   71         11    Male      No      No   \n",
       "3  148.924   9504     681      3   36         11  Female      No      No   \n",
       "4   55.882   4897     357      2   68         16    Male      No     Yes   \n",
       "\n",
       "   Ethnicity  Balance  \n",
       "0  Caucasian      333  \n",
       "1      Asian      903  \n",
       "2      Asian      580  \n",
       "3      Asian      964  \n",
       "4  Caucasian      331  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5011c2",
   "metadata": {},
   "source": [
    "## Is pandas ‚Äúheavy‚Äù?\n",
    "\n",
    "‚öôÔ∏è What does it mean that ‚Äúpandas is heavy‚Äù? It is, in several ways:\n",
    "\n",
    "1Ô∏è‚É£ **Size and complexity**\n",
    "\n",
    "* Pandas is not a tiny library:<br>\n",
    "\n",
    "  * installing it brings in a lot of dependencies:\n",
    "  * `numpy`, `dateutil`, `pytz`, `tzdata`, `matplotlib`, `openpyxl`, `xlrd`, etc.\n",
    "* The package size is **dozens of MB**.\n",
    "* Loading it into memory at startup is **slower** than a simple `import csv`.\n",
    "\n",
    "üëâ So for a script that just has to read a CSV file and print 10 rows, importing the whole of pandas is like using a truck to deliver a letter.\n",
    "\n",
    "2Ô∏è‚É£ **External dependencies**<br>\n",
    "\n",
    "To work well with many formats, pandas uses external libraries (as mentioned above):\n",
    "\n",
    "* `openpyxl` for *.xlsx* files\n",
    "* `xlrd` for *.xls* files\n",
    "* `pyarrow` for *.parquet* files\n",
    "* `numexpr` for numeric operations\n",
    "* `matplotlib` for `.plot()`<br>\n",
    "\n",
    "üëâ These dependencies are convenient in a data-science environment,\n",
    "but excessive in a system script or a microservice.\n",
    "\n",
    "3Ô∏è‚É£ **Impact on small environments**<br>\n",
    "\n",
    "In contexts such as:\n",
    "\n",
    "* Docker microservices\n",
    "* lightweight CLI scripts\n",
    "* serverless functions (AWS Lambda, GCP Functions)\n",
    "* systems with memory constraints\n",
    "\n",
    "importing pandas can:\n",
    "\n",
    "* slow down the script startup,\n",
    "* increase the Docker image by tens or even hundreds of MB,\n",
    "* lead to incompatibilities or long cold-start times.\n",
    "\n",
    "**That‚Äôs why sometimes you want to ‚Äúavoid pandas‚Äù:**\n",
    "\n",
    "* You don‚Äôt need its advanced analytics features.\n",
    "* You just want to read a file and iterate over its rows.\n",
    "* You want to reduce dependencies and startup time.\n",
    "\n",
    "In that case it makes more sense to use:\n",
    "\n",
    "```python\n",
    "  import csv      # for CSV files\n",
    "  import openpyxl # for modern Excel files\n",
    "```\n",
    "\n",
    "which are much lighter modules.\n",
    "\n",
    "üß† **Metaphor**<br>\n",
    "Pandas is like Excel or a full ERP system.<br>\n",
    "If you just need to open a text file and print two columns, Notepad is enough.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf41df-1eaf-473d-849a-d7eddca92f06",
   "metadata": {
    "id": "f2bf41df-1eaf-473d-849a-d7eddca92f06"
   },
   "source": [
    "## Converting Excel files to CSV format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d281f-0e94-42e6-956f-122e767c9aaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:43:41.545329Z",
     "iopub.status.busy": "2025-10-21T08:43:41.545074Z",
     "iopub.status.idle": "2025-10-21T08:43:41.549360Z",
     "shell.execute_reply": "2025-10-21T08:43:41.548809Z",
     "shell.execute_reply.started": "2025-10-21T08:43:41.545312Z"
    },
    "id": "e06d281f-0e94-42e6-956f-122e767c9aaa"
   },
   "source": [
    "Conversely, to view a CSV file **in the standard Excel format** you can do this (there are other ways too):\n",
    "* open a **new file**\n",
    "* `Data` tab\n",
    "* button at the top left `Get Data`\n",
    "* `From file` --> `From text/CSV`\n",
    "* in the preview make any changes (on loading) ‚Äì maybe it‚Äôs enough ‚Äì and then press the \"Load\" button at the bottom right.\n",
    "\n",
    "This process is well described in the video [How to Convert CSV to Excel (Simple and Quick)](https://www.youtube.com/watch?v=jw1DSuqr3ew) from *Excel Tutorials by EasyClick Academy*. Subtitles are available. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433355c2",
   "metadata": {},
   "source": [
    "If a file is created in Excel and then converted to CSV, saved, and later reopened in Excel, Excel shows it ‚Äúas Excel‚Äù (i.e. with the grid). Why?<br>\n",
    "\n",
    "Excel doesn‚Äôt open the CSV as a real Excel file ‚Äî it **parses it as a text table** and displays it in the same UI (cells, rows, columns).<br>\n",
    "That makes it *look* like Excel, but the file actually doesn‚Äôt contain any of the typical `.xlsx` features:\n",
    "\n",
    "* no formatting,\n",
    "* no formulas,\n",
    "* no multiple sheets,\n",
    "* no complex data types.\n",
    "\n",
    "Excel is simply showing **a grid on top of a text file**.<br>\n",
    "It‚Äôs a bit like opening a `.txt` file in Word: the content is just text, but the environment is Word, with its ‚Äúrich‚Äù look.\n",
    "\n",
    "üí° One-sentence summary:<br>\n",
    "\n",
    "> Excel detects the `.csv` extension, interprets the data as tabular, and shows it in its interface ‚Äî but the file remains plain structured text, not a real Excel workbook.\n",
    "\n",
    "**Objection**: if I open in Excel a CSV that was generated independently of Excel, the grid is not applied.<br>\n",
    "True, Excel doesn‚Äôt parse CSV ‚Äúsmartly‚Äù based on the content ‚Äî it uses the system‚Äôs **regional settings** (the Windows ‚Äúlocale‚Äù or ‚Äúlist separator‚Äù).<br>\n",
    "For example:\n",
    "\n",
    "* in Europe, the default list separator is `;` (semicolon);\n",
    "* in the US/UK, it‚Äôs `,` (comma).\n",
    "\n",
    "So:\n",
    "\n",
    "* if the CSV comes from Excel, it uses the same separator as your regional settings ‚Üí Excel ‚Äúrecognizes‚Äù it and shows the table.\n",
    "* if the CSV comes from another program (e.g. Python `to_csv()`, MySQL, or international systems) that uses the comma, Excel doesn‚Äôt know that‚Äôs the separator and puts everything in a single cell.\n",
    "\n",
    "The user can fix it manually:\n",
    "\n",
    "```text\n",
    "Data ‚Üí From Text/CSV ‚Üí choose the correct delimiter (comma, semicolon, tab)\n",
    "```\n",
    "\n",
    "or change the system‚Äôs regional setting.\n",
    "\n",
    "üí° In short:\n",
    "\n",
    "> Excel ‚Äútreats as a table‚Äù only those CSV files that follow its regional conventions (separator and encoding).<br>\n",
    "> If the file is generated elsewhere with other standards, Excel shows it as text in one column.\n",
    "\n",
    "---\n",
    "## Summary table ‚Äî Ways to open a CSV with the correct grid\n",
    "\n",
    "| # | Method           | Path in Excel | Advantage | When to use it |\n",
    "|---|------------------|---------------|-----------|----------------|\n",
    "| 1 | **Guided import (recommended)** | **Data ‚Üí From Text/CSV ‚Üí** select the file ‚Üí choose **Delimiter** (comma, semicolon, tab) ‚Üí **Load** | Detects separator, encoding and shows a preview | Always, if the CSV was **not** generated by Excel |\n",
    "| 2 | **Set system default separator (Windows)** | **Control Panel ‚Üí Region ‚Üí Additional settings ‚Üí ‚ÄúList separator‚Äù ‚Üí** set `,` or `;` | Excel opens CSV correctly with double-click | Useful if you often open CSVs with the **same** delimiter |\n",
    "| 3 | **Rename file and change extension (trick)** | Rename `.csv` to `.txt`, then **Data ‚Üí From Text ‚Üí** follow the wizard | Forces you to choose the delimiter | Useful if Excel keeps putting everything into **one column** |\n",
    "| 4 | **Open Excel first, then ‚ÄúOpen ‚Üí File ‚Üí CSV‚Äù** | **File ‚Üí Open ‚Üí Browse ‚Üí** file type: **All files** ‚Üí pick the CSV ‚Üí the import window appears | Lets you choose encoding and separator | Alternative to importing from **Data** |\n",
    "| 5 | **Change Excel language/locale (optional)** | **File ‚Üí Options ‚Üí Advanced ‚Üí List separator** | Aligns Excel to the file format (e.g. US = `,`, Italy = `;`) | For mixed international / cloud usage |\n",
    "\n",
    "---\n",
    "üí¨ **Short explanation**\n",
    "\n",
    "When Excel shows everything in one column, it‚Äôs because:\n",
    "\n",
    "* the file delimiter (e.g. `,`)\n",
    "  ‚â†\n",
    "* the system list separator (e.g. `;` in Italy).\n",
    "\n",
    "üëâ Fix: use the import wizard, which lets you choose the delimiter and encoding (UTF-8 recommended).<br>\n",
    "After that choice, Excel immediately shows the correct grid and you can save as `.xlsx` if you want to keep it stable.\n",
    "\n",
    "üí° Practical tip for those who often work with Python or external CSVs:\n",
    "\n",
    "* use\n",
    "\n",
    "  ```python\n",
    "    df.to_csv(\"file.csv\", sep=\";\", encoding=\"utf-8-sig\")\n",
    "  ```\n",
    "\n",
    "  ‚Üí this way Excel (Italian version) opens it already ‚Äúas a grid‚Äù with no manual steps.\n",
    "* `utf-8-sig` is a variant of UTF-8 encoding commonly used so that Excel correctly recognizes CSV files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3651627f",
   "metadata": {},
   "source": [
    "# Input parameters of `pd.read_csv` function\n",
    "\n",
    "We have already mentioned the two **fundamental input arguments** of the pandas `read_csv` function: `sep` and `header`. They are **critical**:\n",
    "\n",
    "- `header=1` tells pandas to skip the first row of the file and use the second row as the header.<br>\n",
    "  If our CSVs do **not** have two header rows, or if the first file has a slightly different format from the others (spaces, separator, BOM, etc.), pandas will **misinterpret the columns**.\n",
    "- `sep=';'` can break all the data (if the file is actually a real CSV with commas!).<br>\n",
    "  Be careful: many ‚ÄúCSV files‚Äù actually use `sep=';'`!\n",
    "\n",
    "In reality, the `read_csv` function has **many other input arguments**, as you can see from the help in the next cell ‚Äî we will later go deeper into the **main** ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d3b132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_csv in module pandas.io.parsers.readers:\n",
      "\n",
      "read_csv(\n",
      "    filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]',\n",
      "    *,\n",
      "    sep: 'str | None | lib.NoDefault' = <no_default>,\n",
      "    delimiter: 'str | None | lib.NoDefault' = None,\n",
      "    header: \"int | Sequence[int] | None | Literal['infer']\" = 'infer',\n",
      "    names: 'Sequence[Hashable] | None | lib.NoDefault' = <no_default>,\n",
      "    index_col: 'IndexLabel | Literal[False] | None' = None,\n",
      "    usecols: 'UsecolsArgType' = None,\n",
      "    dtype: 'DtypeArg | None' = None,\n",
      "    engine: 'CSVEngine | None' = None,\n",
      "    converters: 'Mapping[Hashable, Callable] | None' = None,\n",
      "    true_values: 'list | None' = None,\n",
      "    false_values: 'list | None' = None,\n",
      "    skipinitialspace: 'bool' = False,\n",
      "    skiprows: 'list[int] | int | Callable[[Hashable], bool] | None' = None,\n",
      "    skipfooter: 'int' = 0,\n",
      "    nrows: 'int | None' = None,\n",
      "    na_values: 'Hashable | Iterable[Hashable] | Mapping[Hashable, Iterable[Hashable]] | None' = None,\n",
      "    keep_default_na: 'bool' = True,\n",
      "    na_filter: 'bool' = True,\n",
      "    verbose: 'bool | lib.NoDefault' = <no_default>,\n",
      "    skip_blank_lines: 'bool' = True,\n",
      "    parse_dates: 'bool | Sequence[Hashable] | None' = None,\n",
      "    infer_datetime_format: 'bool | lib.NoDefault' = <no_default>,\n",
      "    keep_date_col: 'bool | lib.NoDefault' = <no_default>,\n",
      "    date_parser: 'Callable | lib.NoDefault' = <no_default>,\n",
      "    date_format: 'str | dict[Hashable, str] | None' = None,\n",
      "    dayfirst: 'bool' = False,\n",
      "    cache_dates: 'bool' = True,\n",
      "    iterator: 'bool' = False,\n",
      "    chunksize: 'int | None' = None,\n",
      "    compression: 'CompressionOptions' = 'infer',\n",
      "    thousands: 'str | None' = None,\n",
      "    decimal: 'str' = '.',\n",
      "    lineterminator: 'str | None' = None,\n",
      "    quotechar: 'str' = '\"',\n",
      "    quoting: 'int' = 0,\n",
      "    doublequote: 'bool' = True,\n",
      "    escapechar: 'str | None' = None,\n",
      "    comment: 'str | None' = None,\n",
      "    encoding: 'str | None' = None,\n",
      "    encoding_errors: 'str | None' = 'strict',\n",
      "    dialect: 'str | csv.Dialect | None' = None,\n",
      "    on_bad_lines: 'str' = 'error',\n",
      "    delim_whitespace: 'bool | lib.NoDefault' = <no_default>,\n",
      "    low_memory: 'bool' = True,\n",
      "    memory_map: 'bool' = False,\n",
      "    float_precision: \"Literal['high', 'legacy'] | None\" = None,\n",
      "    storage_options: 'StorageOptions | None' = None,\n",
      "    dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>\n",
      ") -> 'DataFrame | TextFileReader'\n",
      "    Read a comma-separated values (csv) file into DataFrame.\n",
      "\n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "\n",
      "    Additional help can be found in the online docs for\n",
      "    `IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : str, path object or file-like object\n",
      "        Any valid string path is acceptable. The string could be a URL. Valid\n",
      "        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
      "        expected. A local file could be: file://localhost/path/to/table.csv.\n",
      "\n",
      "        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "\n",
      "        By file-like object, we refer to objects with a ``read()`` method, such as\n",
      "        a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
      "    sep : str, default ','\n",
      "        Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n",
      "        C engine cannot automatically detect\n",
      "        the separator, but the Python parsing engine can, meaning the latter will\n",
      "        be used and automatically detect the separator from only the first valid\n",
      "        row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n",
      "        In addition, separators longer than 1 character and different from\n",
      "        ``'\\s+'`` will be interpreted as regular expressions and will also force\n",
      "        the use of the Python parsing engine. Note that regex delimiters are prone\n",
      "        to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
      "    delimiter : str, optional\n",
      "        Alias for ``sep``.\n",
      "    header : int, Sequence of int, 'infer' or None, default 'infer'\n",
      "        Row number(s) containing column labels and marking the start of the\n",
      "        data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n",
      "        are passed the behavior is identical to ``header=0`` and column\n",
      "        names are inferred from the first line of the file, if column\n",
      "        names are passed explicitly to ``names`` then the behavior is identical to\n",
      "        ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "        replace existing names. The header can be a list of integers that\n",
      "        specify row locations for a :class:`~pandas.MultiIndex` on the columns\n",
      "        e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n",
      "        skipped (e.g. 2 in this example is skipped). Note that this\n",
      "        parameter ignores commented lines and empty lines if\n",
      "        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
      "        data rather than the first line of the file.\n",
      "    names : Sequence of Hashable, optional\n",
      "        Sequence of column labels to apply. If the file contains a header row,\n",
      "        then you should explicitly pass ``header=0`` to override the column names.\n",
      "        Duplicates in this list are not allowed.\n",
      "    index_col : Hashable, Sequence of Hashable or False, optional\n",
      "      Column(s) to use as row label(s), denoted either by column labels or column\n",
      "      indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n",
      "      will be formed for the row labels.\n",
      "\n",
      "      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
      "      column as the index, e.g., when you have a malformed file with delimiters at\n",
      "      the end of each line.\n",
      "    usecols : Sequence of Hashable or Callable, optional\n",
      "        Subset of columns to select, denoted either by column labels or column indices.\n",
      "        If list-like, all elements must either\n",
      "        be positional (i.e. integer indices into the document columns) or strings\n",
      "        that correspond to column names provided either by the user in ``names`` or\n",
      "        inferred from the document header row(s). If ``names`` are given, the document\n",
      "        header row(s) are not taken into account. For example, a valid list-like\n",
      "        ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
      "        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "        To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n",
      "        preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n",
      "        for columns in ``['foo', 'bar']`` order or\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "        for ``['bar', 'foo']`` order.\n",
      "\n",
      "        If callable, the callable function will be evaluated against the column\n",
      "        names, returning names where the callable function evaluates to ``True``. An\n",
      "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "        parsing time and lower memory usage.\n",
      "    dtype : dtype or dict of {Hashable : dtype}, optional\n",
      "        Data type(s) to apply to either the whole dataset or individual columns.\n",
      "        E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n",
      "        Use ``str`` or ``object`` together with suitable ``na_values`` settings\n",
      "        to preserve and not interpret ``dtype``.\n",
      "        If ``converters`` are specified, they will be applied INSTEAD\n",
      "        of ``dtype`` conversion.\n",
      "\n",
      "        .. versionadded:: 1.5.0\n",
      "\n",
      "            Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n",
      "            the default determines the ``dtype`` of the columns which are not explicitly\n",
      "            listed.\n",
      "    engine : {'c', 'python', 'pyarrow'}, optional\n",
      "        Parser engine to use. The C and pyarrow engines are faster, while the python engine\n",
      "        is currently more feature-complete. Multithreading is currently only supported by\n",
      "        the pyarrow engine.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "            The 'pyarrow' engine was added as an *experimental* engine, and some features\n",
      "            are unsupported, or may not work correctly, with this engine.\n",
      "    converters : dict of {Hashable : Callable}, optional\n",
      "        Functions for converting values in specified columns. Keys can either\n",
      "        be column labels or column indices.\n",
      "    true_values : list, optional\n",
      "        Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\n",
      "    false_values : list, optional\n",
      "        Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\n",
      "    skipinitialspace : bool, default False\n",
      "        Skip spaces after delimiter.\n",
      "    skiprows : int, list of int or Callable, optional\n",
      "        Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n",
      "        at the start of the file.\n",
      "\n",
      "        If callable, the callable function will be evaluated against the row\n",
      "        indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n",
      "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "    skipfooter : int, default 0\n",
      "        Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\n",
      "    nrows : int, optional\n",
      "        Number of rows of file to read. Useful for reading pieces of large files.\n",
      "    na_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n",
      "        Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n",
      "        per-column ``NA`` values.  By default the following values are interpreted as\n",
      "        ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n",
      "        \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n",
      "        \"n/a\", \"nan\", \"null \".\n",
      "\n",
      "    keep_default_na : bool, default True\n",
      "        Whether or not to include the default ``NaN`` values when parsing the data.\n",
      "        Depending on whether ``na_values`` is passed in, the behavior is as follows:\n",
      "\n",
      "        * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n",
      "          is appended to the default ``NaN`` values used for parsing.\n",
      "        * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n",
      "          the default ``NaN`` values are used for parsing.\n",
      "        * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n",
      "          the ``NaN`` values specified ``na_values`` are used for parsing.\n",
      "        * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n",
      "          strings will be parsed as ``NaN``.\n",
      "\n",
      "        Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n",
      "        ``na_values`` parameters will be ignored.\n",
      "    na_filter : bool, default True\n",
      "        Detect missing value markers (empty strings and the value of ``na_values``). In\n",
      "        data without any ``NA`` values, passing ``na_filter=False`` can improve the\n",
      "        performance of reading a large file.\n",
      "    verbose : bool, default False\n",
      "        Indicate number of ``NA`` values placed in non-numeric columns.\n",
      "\n",
      "        .. deprecated:: 2.2.0\n",
      "    skip_blank_lines : bool, default True\n",
      "        If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\n",
      "    parse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n",
      "        The behavior is as follows:\n",
      "\n",
      "        * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n",
      "          ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n",
      "        * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n",
      "          each as a separate date column.\n",
      "        * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n",
      "          as a single date column. Values are joined with a space before parsing.\n",
      "        * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n",
      "          result 'foo'. Values are joined with a space before parsing.\n",
      "\n",
      "        If a column or index cannot be represented as an array of ``datetime``,\n",
      "        say because of an unparsable value or a mixture of timezones, the column\n",
      "        or index will be returned unaltered as an ``object`` data type. For\n",
      "        non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n",
      "        :func:`~pandas.read_csv`.\n",
      "\n",
      "        Note: A fast-path exists for iso8601-formatted dates.\n",
      "    infer_datetime_format : bool, default False\n",
      "        If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n",
      "        format of the ``datetime`` strings in the columns, and if it can be inferred,\n",
      "        switch to a faster method of parsing them. In some cases this can increase\n",
      "        the parsing speed by 5-10x.\n",
      "\n",
      "        .. deprecated:: 2.0.0\n",
      "            A strict version of this argument is now the default, passing it has no effect.\n",
      "\n",
      "    keep_date_col : bool, default False\n",
      "        If ``True`` and ``parse_dates`` specifies combining multiple columns then\n",
      "        keep the original columns.\n",
      "    date_parser : Callable, optional\n",
      "        Function to use for converting a sequence of string columns to an array of\n",
      "        ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "        conversion. pandas will try to call ``date_parser`` in three different ways,\n",
      "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "        (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n",
      "        string values from the columns defined by ``parse_dates`` into a single array\n",
      "        and pass that; and 3) call ``date_parser`` once for each row using one or\n",
      "        more strings (corresponding to the columns defined by ``parse_dates``) as\n",
      "        arguments.\n",
      "\n",
      "        .. deprecated:: 2.0.0\n",
      "           Use ``date_format`` instead, or read in as ``object`` and then apply\n",
      "           :func:`~pandas.to_datetime` as-needed.\n",
      "    date_format : str or dict of column -> format, optional\n",
      "        Format to use for parsing dates when used in conjunction with ``parse_dates``.\n",
      "        The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n",
      "        `strftime documentation\n",
      "        <https://docs.python.org/3/library/datetime.html\n",
      "        #strftime-and-strptime-behavior>`_ for more information on choices, though\n",
      "        note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n",
      "        You can also pass:\n",
      "\n",
      "        - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n",
      "            time string (not necessarily in exactly the same format);\n",
      "        - \"mixed\", to infer the format for each element individually. This is risky,\n",
      "            and you should probably use it along with `dayfirst`.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "    dayfirst : bool, default False\n",
      "        DD/MM format dates, international and European format.\n",
      "    cache_dates : bool, default True\n",
      "        If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n",
      "        conversion. May produce significant speed-up when parsing duplicate\n",
      "        date strings, especially ones with timezone offsets.\n",
      "\n",
      "    iterator : bool, default False\n",
      "        Return ``TextFileReader`` object for iteration or getting chunks with\n",
      "        ``get_chunk()``.\n",
      "    chunksize : int, optional\n",
      "        Number of lines to read from the file per chunk. Passing a value will cause the\n",
      "        function to return a ``TextFileReader`` object for iteration.\n",
      "        See the `IO Tools docs\n",
      "        <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "        for more information on ``iterator`` and ``chunksize``.\n",
      "\n",
      "    compression : str or dict, default 'infer'\n",
      "        For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n",
      "        path-like, then detect compression from the following extensions: '.gz',\n",
      "        '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      "        (otherwise no compression).\n",
      "        If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n",
      "        Set to ``None`` for no decompression.\n",
      "        Can also be a dict with key ``'method'`` set\n",
      "        to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
      "        other key-value pairs are forwarded to\n",
      "        ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      "        ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n",
      "        ``tarfile.TarFile``, respectively.\n",
      "        As an example, the following could be passed for Zstandard decompression using a\n",
      "        custom compression dictionary:\n",
      "        ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n",
      "\n",
      "        .. versionadded:: 1.5.0\n",
      "            Added support for `.tar` files.\n",
      "\n",
      "        .. versionchanged:: 1.4.0 Zstandard support.\n",
      "\n",
      "    thousands : str (length 1), optional\n",
      "        Character acting as the thousands separator in numerical values.\n",
      "    decimal : str (length 1), default '.'\n",
      "        Character to recognize as decimal point (e.g., use ',' for European data).\n",
      "    lineterminator : str (length 1), optional\n",
      "        Character used to denote a line break. Only valid with C parser.\n",
      "    quotechar : str (length 1), optional\n",
      "        Character used to denote the start and end of a quoted item. Quoted\n",
      "        items can include the ``delimiter`` and it will be ignored.\n",
      "    quoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n",
      "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n",
      "        ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n",
      "        characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n",
      "        or ``lineterminator``.\n",
      "    doublequote : bool, default True\n",
      "       When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n",
      "       whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n",
      "       field as a single ``quotechar`` element.\n",
      "    escapechar : str (length 1), optional\n",
      "        Character used to escape other characters.\n",
      "    comment : str (length 1), optional\n",
      "        Character indicating that the remainder of line should not be parsed.\n",
      "        If found at the beginning\n",
      "        of a line, the line will be ignored altogether. This parameter must be a\n",
      "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "        fully commented lines are ignored by the parameter ``header`` but not by\n",
      "        ``skiprows``. For example, if ``comment='#'``, parsing\n",
      "        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n",
      "        treated as the header.\n",
      "    encoding : str, optional, default 'utf-8'\n",
      "        Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n",
      "        standard encodings\n",
      "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
      "\n",
      "    encoding_errors : str, optional, default 'strict'\n",
      "        How encoding errors are treated. `List of possible values\n",
      "        <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "    dialect : str or csv.Dialect, optional\n",
      "        If provided, this parameter will override values (default or not) for the\n",
      "        following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n",
      "        ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n",
      "        override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n",
      "        documentation for more details.\n",
      "    on_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n",
      "        Specifies what to do upon encountering a bad line (a line with too many fields).\n",
      "        Allowed values are :\n",
      "\n",
      "        - ``'error'``, raise an Exception when a bad line is encountered.\n",
      "        - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n",
      "        - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "            - Callable, function with signature\n",
      "              ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n",
      "              bad line. ``bad_line`` is a list of strings split by the ``sep``.\n",
      "              If the function returns ``None``, the bad line will be ignored.\n",
      "              If the function returns a new ``list`` of strings with more elements than\n",
      "              expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n",
      "              Only supported when ``engine='python'``\n",
      "\n",
      "        .. versionchanged:: 2.2.0\n",
      "\n",
      "            - Callable, function with signature\n",
      "              as described in `pyarrow documentation\n",
      "              <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n",
      "              #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n",
      "\n",
      "    delim_whitespace : bool, default False\n",
      "        Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n",
      "        used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n",
      "        is set to ``True``, nothing should be passed in for the ``delimiter``\n",
      "        parameter.\n",
      "\n",
      "        .. deprecated:: 2.2.0\n",
      "            Use ``sep=\"\\s+\"`` instead.\n",
      "    low_memory : bool, default True\n",
      "        Internally process the file in chunks, resulting in lower memory use\n",
      "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "        types either set ``False``, or specify the type with the ``dtype`` parameter.\n",
      "        Note that the entire file is read into a single :class:`~pandas.DataFrame`\n",
      "        regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n",
      "        chunks. (Only valid with C parser).\n",
      "    memory_map : bool, default False\n",
      "        If a filepath is provided for ``filepath_or_buffer``, map the file object\n",
      "        directly onto memory and access the data directly from there. Using this\n",
      "        option can improve performance because there is no longer any I/O overhead.\n",
      "    float_precision : {'high', 'legacy', 'round_trip'}, optional\n",
      "        Specifies which converter the C engine should use for floating-point\n",
      "        values. The options are ``None`` or ``'high'`` for the ordinary converter,\n",
      "        ``'legacy'`` for the original lower precision pandas converter, and\n",
      "        ``'round_trip'`` for the round-trip converter.\n",
      "\n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "        are forwarded to ``urllib.request.Request`` as header options. For other\n",
      "        URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      "        forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      "        details, and for more examples on storage options refer `here\n",
      "        <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      "        highlight=storage_options#reading-writing-remote-files>`_.\n",
      "\n",
      "    dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n",
      "        Back-end data type applied to the resultant :class:`DataFrame`\n",
      "        (still experimental). Behaviour is as follows:\n",
      "\n",
      "        * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n",
      "          (default).\n",
      "        * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n",
      "          DataFrame.\n",
      "\n",
      "        .. versionadded:: 2.0\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or TextFileReader\n",
      "        A comma-separated values (csv) file is returned as two-dimensional\n",
      "        data structure with labeled axes.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "    read_table : Read general delimited file into DataFrame.\n",
      "    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b128b8-189d-4145-a0f1-9bf4d943e1a9",
   "metadata": {
    "id": "02b128b8-189d-4145-a0f1-9bf4d943e1a9"
   },
   "source": [
    "> Reading a *csv* file is often one of the **first real difficulties** that those who start using pandas run into.\n",
    "> The reason is that csv reading is simple in theory but in practice often the data is dirty, mixed, encoded differently, irregular.\n",
    "> This makes the initial approach a bit **tricky**, a source of **not a few frustrations**, for two reasons:\n",
    "> - the many and non‚Äëtrivial arguments of the `read_csv` function\n",
    "> - the **irregularities** in the data in the files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5394f7a9-8061-400e-a1af-3a9a87099765",
   "metadata": {
    "id": "5394f7a9-8061-400e-a1af-3a9a87099765"
   },
   "source": [
    "## The mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c16f43-e516-48ac-95b9-1534a96430a2",
   "metadata": {
    "id": "f1c16f43-e516-48ac-95b9-1534a96430a2"
   },
   "source": [
    "The function `pd.read_csv` does an **automatic mapping** of CSV data into pandas, as described here:\n",
    "\n",
    "![](how_pandas_infers_CSV_datatypes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e93810-ba9e-41ed-be16-49f71508751d",
   "metadata": {
    "id": "a4e93810-ba9e-41ed-be16-49f71508751d"
   },
   "source": [
    "There is a problem that is not mentioned in the slide: the `read_csv` function **often fails to infer categorical variables** (when they are present in the CSV file as strings), so they are imported as `object`, the generic pandas string data type. As you can see:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9292db96-52aa-4873-ae2b-79451efae594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T09:32:27.750380Z",
     "iopub.status.busy": "2025-10-21T09:32:27.750139Z",
     "iopub.status.idle": "2025-10-21T09:32:27.754749Z",
     "shell.execute_reply": "2025-10-21T09:32:27.754389Z",
     "shell.execute_reply.started": "2025-10-21T09:32:27.750364Z"
    },
    "id": "9292db96-52aa-4873-ae2b-79451efae594",
    "outputId": "0dc7062c-bade-4afc-9dfe-2b8d559a6321"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Income       float64\n",
       "Limit          int64\n",
       "Rating         int64\n",
       "Cards          int64\n",
       "Age            int64\n",
       "Education      int64\n",
       "Gender        object\n",
       "Student       object\n",
       "Married       object\n",
       "Ethnicity     object\n",
       "Balance        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_credit.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99baa579-39ee-4f63-9bfc-4e8c39b72841",
   "metadata": {
    "id": "99baa579-39ee-4f63-9bfc-4e8c39b72841"
   },
   "source": [
    "The variables `Gender`, `Student`, `Married` and `Ethnicity` are **categorical** in nature: that is, they can only take on a small, fixed number of values.\n",
    "Other variables such as `Education`, if imported as text, could potentially take on an infinite number of values.\n",
    "\n",
    "Each cell of the variable, if imported as `object`, **points to a string in memory, often duplicated several times**.\n",
    "\n",
    "It is therefore necessary to **convert** these variables to the `category` format available from *pandas* (it does not exist in base Python), as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65df6bda-6ddc-4f71-a4f3-c5374269103f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T09:40:41.996556Z",
     "iopub.status.busy": "2025-10-21T09:40:41.996253Z",
     "iopub.status.idle": "2025-10-21T09:40:42.001137Z",
     "shell.execute_reply": "2025-10-21T09:40:42.000829Z",
     "shell.execute_reply.started": "2025-10-21T09:40:41.996542Z"
    },
    "id": "65df6bda-6ddc-4f71-a4f3-c5374269103f",
    "outputId": "9bfc1afa-b4bc-4f74-cc46-88085e8ba564"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Income        float64\n",
       "Limit           int64\n",
       "Rating          int64\n",
       "Cards           int64\n",
       "Age             int64\n",
       "Education       int64\n",
       "Gender       category\n",
       "Student        object\n",
       "Married        object\n",
       "Ethnicity      object\n",
       "Balance         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_credit['Gender'] = df_credit['Gender'].astype('category')\n",
    "df_credit.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b894adb0-45e6-47ef-b6d2-993f4163a6de",
   "metadata": {
    "id": "b894adb0-45e6-47ef-b6d2-993f4163a6de"
   },
   "source": [
    "The *pandas* method `astype('category')`:\n",
    "\n",
    "* creates an **internal encoding table** (the ‚Äúlevels‚Äù or ‚Äúcategories‚Äù),\n",
    "* represents the column as **internal integers** (0, 1, 2, ‚Ä¶) instead of repeated strings.\n",
    "\n",
    "The behavior of `category` is similar to the **factor** in R.\n",
    "\n",
    "üöÄ <font size=\"4\">**Main advantages** (of `category`)</font><br>\n",
    "\n",
    "üîπ **Memory efficiency**<br>\n",
    "Each value becomes **an integer**, and the string is **stored only once in the category table**.<br>\n",
    "üëâ On large datasets, the saving can reach 70‚Äì90% of RAM.\n",
    "Example:\n",
    "\n",
    "```python\n",
    "    df['citt√†'].memory_usage(deep=True)\n",
    "    df['citt√†'].astype('category').memory_usage(deep=True)\n",
    "```\n",
    "\n",
    "The second one takes much less space.\n",
    "\n",
    "üîπ **Processing speed**<br>\n",
    "Many pandas operations (`groupby`, `sort`, `value_counts`, `merges`) become **much faster**; in fact:\n",
    "\n",
    "* comparing integers is faster than comparing strings,\n",
    "* grouping and join algorithms work on numeric codes.\n",
    "  üí° Typical case: `df.groupby('categoria').agg(...)` is much faster if `categoria` is `category`.\n",
    "\n",
    "üîπ **Semantic meaning**<br>\n",
    "A categorical variable has **a finite and known number of levels**.<br>\n",
    "This is useful to:\n",
    "\n",
    "* ensure that ‚Äúout-of-list‚Äù values don‚Äôt appear (e.g. ‚ÄòFemmina‚Äô vs ‚ÄòF‚Äô),\n",
    "* keep the logical or hierarchical order (e.g. Low < Medium < High).<br>\n",
    "\n",
    "You can also explicitly define the order like this:\n",
    "\n",
    "```python\n",
    "    df['livello'] = pd.Categorical(df['livello'], categories=['basso','medio','alto'], ordered=True)\n",
    "```\n",
    "\n",
    "‚Üí useful for comparisons, sorting, or encoding in machine learning.\n",
    "\n",
    "üîπ **ML and preprocessing compatibility**<br>\n",
    "Many machine-learning algorithms or encoders (e.g. `sklearn.preprocessing.OrdinalEncoder`, `OneHotEncoder`) detect `category` and immediately treat it as a discrete variable, **without first having to convert it from object**.\n",
    "\n",
    "---\n",
    "\n",
    "‚ö†Ô∏è **When is `category` not convenient?**\n",
    "\n",
    "* if the column has **many unique values** (e.g. a unique code or a customer ID), the conversion brings no benefit: the category table would be as large as the column itself.\n",
    "* if values are frequently changed (adding new categories), the `category` type is less flexible.\n",
    "\n",
    "üîç **Practical example**:\n",
    "\n",
    "```python\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({\n",
    "        'sesso': ['M','F','M','F','F']*100000\n",
    "    })\n",
    "    print(df['sesso'].memory_usage(deep=True))   # object\n",
    "    df['sesso'] = df['sesso'].astype('category')\n",
    "    print(df['sesso'].memory_usage(deep=True))   # category (much less!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "> üí¨ In brief\n",
    "\n",
    "| Aspect           | `object`             | `category`                      |\n",
    "|------------------|----------------------|----------------------------------|\n",
    "| Base type        | Python strings       | integer codes + category list   |\n",
    "| Memory           | High                 | Very low                        |\n",
    "| Speed            | Slower               | Faster                          |\n",
    "| Semantics        | Free text            | Finite discrete values          |\n",
    "| Machine Learning | Must be encoded first| Already ready / suitable        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefdf89b-2214-4ed7-8b59-49939285ecfe",
   "metadata": {
    "id": "aefdf89b-2214-4ed7-8b59-49939285ecfe"
   },
   "source": [
    "## The arguments of the `pd.read_csv` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1fd02f-b8fe-4d70-9bbe-235863bef8d5",
   "metadata": {
    "id": "9b1fd02f-b8fe-4d70-9bbe-235863bef8d5"
   },
   "source": [
    "[Here](https://github.com/nikitaprasad21/ML-Cheat-Codes/blob/main/Data-Gathering/CSV-(Comma-Separated-Values)-Files/csv_file_cheatcodes.ipynb) is an excellent notebook that **illustrates the various arguments** of `pd.read_csv` ‚Äì **downloaded** in the directory of this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0786d-2382-40f2-8247-5b828130b23d",
   "metadata": {
    "id": "20a0786d-2382-40f2-8247-5b828130b23d"
   },
   "source": [
    "## The `Unnamed: 0` column\n",
    "See [this chat](https://chatgpt.com/share/68f74bca-554c-8012-a844-7260ce18391d) of ChatGPT. Ask for translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36889a3b-f5e6-406e-8905-83d23e15a66b",
   "metadata": {
    "id": "36889a3b-f5e6-406e-8905-83d23e15a66b"
   },
   "source": [
    "# Frequent problems when loading CSV files in pandas.\n",
    "\n",
    "Here is a list of the **most common problems** you encounter when loading a csv with `pandas.read_csv()`, together with **typical causes** and **solutions**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea061b7-be45-4711-af0a-04cbac19db08",
   "metadata": {
    "id": "bea061b7-be45-4711-af0a-04cbac19db08"
   },
   "source": [
    "üß© **1. Columns ‚ÄúUnnamed: 0‚Äù or ‚ÄúUnnamed: n‚Äù** ‚Äì already seen before\n",
    "\n",
    "<u>Problem</u>: an unwanted column called `Unnamed: 0` appears.<br>\n",
    "<u>Cause</u>: often the CSV includes an index saved from a previous `DataFrame.to_csv()` (i.e. `index=True` by default).<br>\n",
    "<u>Solution</u>:\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", index_col=0)\n",
    "    # oppure\n",
    "    pd.read_csv(\"file.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ca7d0b-38e8-4a23-99b8-5a8ff7749428",
   "metadata": {
    "id": "11ca7d0b-38e8-4a23-99b8-5a8ff7749428"
   },
   "source": [
    "‚öôÔ∏è **2. Wrong delimiters** ‚Äì already seen before\n",
    "\n",
    "<u>Problem</u>: the file is not split correctly (all columns end up in a single one).<br> <u>Cause</u>: the separator is not a comma, but a semicolon `;`, a tab `\\t`, or something else.<br> <u>Solution</u>:\n",
    "\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", sep=\";\")      # for European-style CSV\n",
    "    pd.read_csv(\"file.csv\", sep=\"\\t\")     # for TSV files\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A TSV (*Tab-Separated Values*) is basically a CSV, but instead of `,` or `;` it uses the tab character `\\t` as the field separator.\n",
    "You can also detect it automatically:\n",
    "\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", sep=None, engine=\"python\")\n",
    "```\n",
    "\n",
    "üß© When do you use a TSV?\n",
    "\n",
    "* when the data contains lots of commas or semicolons (e.g. text descriptions).\n",
    "* when the file is exported by Unix systems or databases (e.g. PostgreSQL COPY TO, Excel ‚Üí ‚ÄúText (tab delimited)‚Äù).\n",
    "* when you want to avoid ambiguity between decimal separators and field separators.\n",
    "\n",
    "---\n",
    "\n",
    "The `engine` parameter in `pandas.read_csv()` is used to tell pandas which **parsing engine** to use to read and interpret the CSV file.<br>\n",
    "In practice, pandas has **two different parser engines** that do the same job (read the file and turn it into a DataFrame), but **with different features and performance**.\n",
    "\n",
    "1Ô∏è‚É£ **`engine=\"c\"`** ‚Üí the ‚Äúfast‚Äù parser (default)\n",
    "\n",
    "* written in C ‚Üí very fast\n",
    "* it is the default in almost all cases\n",
    "* great for clean, regular files\n",
    "* but‚Ä¶ it is less flexible: it doesn‚Äôt support every option and may fail on ‚Äúmessy‚Äù or complex CSVs\n",
    "\n",
    "Usage example:\n",
    "\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", engine=\"c\")\n",
    "```\n",
    "\n",
    "2Ô∏è‚É£ **`engine=\"python\"`** ‚Üí the ‚Äúrobust‚Äù parser\n",
    "\n",
    "* written in pure Python ‚Üí slower, but more tolerant\n",
    "* supports options that the C parser doesn‚Äôt handle well, such as:\n",
    "\n",
    "  * `sep=None` (i.e. **automatic separator detection**),\n",
    "  * multiple or irregular delimiters,\n",
    "  * malformed lines (`on_bad_lines`),\n",
    "  * complex quotes and special characters.\n",
    "\n",
    "Usage example:\n",
    "\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", sep=None, engine=\"python\")\n",
    "```\n",
    "\n",
    "üëâ Here pandas tries to guess the separator automatically (`,`, `;`, `\\t`, etc.) by looking at the first rows.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a39c444-17bf-457e-aed9-9ba843517807",
   "metadata": {
    "id": "4a39c444-17bf-457e-aed9-9ba843517807"
   },
   "source": [
    "Let‚Äôs go back to the list of problems and solutions of `read_csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee6a66-6045-429c-abfc-2e04eae898e9",
   "metadata": {
    "id": "f0ee6a66-6045-429c-abfc-2e04eae898e9"
   },
   "source": [
    "üî§ **3. Wrong encoding**\n",
    "\n",
    "<u>Problem</u>: accented characters or special symbols appear as ÔøΩ or raise `UnicodeDecodeError`.<br>\n",
    "<u>Cause</u>: the file is not in `UTF‚Äë8` but in `latin1`, `cp1252`, etc.<br>\n",
    "<u>Solution</u>:\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", encoding=\"latin1\")\n",
    "```\n",
    "\n",
    "> What is `latin1`?\n",
    "> - `latin1` (or `ISO‚Äë8859‚Äë1`) is a 1‚Äëbyte (8‚Äëbit) encoding **widely used in Western Europe** before UTF‚Äë8 became standard.\n",
    "> - It supports many Western European characters:\n",
    ">   - Italian accented letters: `√†`, `√®`, `√©`, `√¨`, `√≤`, `√π`, ...\n",
    ">   - Spanish/French/Portuguese letters: `√±`, `√ß`, `√°`, `√©`, `√µ`, ...\n",
    ">   - German umlaut vowels: `√§`, `√∂`, `√º`, ...\n",
    ">   - Nordic `√∏`, `√•`\n",
    ">\n",
    "> **BUT** latin1 does NOT support:\n",
    "> - emoji\n",
    "> - euro symbol `‚Ç¨`\n",
    "> - Greek, Cyrillic, Arabic, Chinese, etc.\n",
    ">\n",
    "> **So `latin1` is basically ‚ÄúWestern Europe in the 90s‚Äù.** If we encounter characters outside that set, pandas either shows the replacement char or throws `UnicodeDecodeError`.\n",
    "\n",
    "<img src=\"ascii_latin1_utf_8.png\" alt=\"image\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b34055",
   "metadata": {
    "id": "f6b34055"
   },
   "source": [
    "Test of **some errors**, in various steps:<br>\n",
    "1. we create a `DataFrame` with typical `latin1` characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25fe2780",
   "metadata": {
    "id": "25fe2780"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"ID\": [1, 2, 3],\n",
    "    \"Nome\": [\"Andr√©\", \"Jos√©\", \"Ana Mar√≠a\"],\n",
    "    \"Citt√†\": [\"Torino\", \"M√°laga\", \"Z√ºrich\"],\n",
    "    \"Note\": [\n",
    "        \"pagato 50$ gi√† fatturato\",              # 'latin1' estende ASCII, che conteneva il carattere '$', dunque anche 'latin1' lo accetta\n",
    "        \"a√±o siguiente -> revisi√≥n t√©cnica\",\n",
    "        \"pi√π vecchio -> gi√† sostituito\"\n",
    "    ]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1558aca4",
   "metadata": {
    "id": "1558aca4"
   },
   "source": [
    "2. we save the `DataFrame` to a `latin1` (ISO‚Äë8859‚Äë1) CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1330ba6",
   "metadata": {
    "id": "b1330ba6",
    "outputId": "0282d025-b24a-41e8-c90d-36d2d411e565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creato file clienti_latin1.csv in encoding latin1\n"
     ]
    }
   ],
   "source": [
    "file_name = \"clienti_latin1.csv\"\n",
    "df.to_csv(\n",
    "    file_name,\n",
    "    index=False,\n",
    "    sep=\";\",             # we also set the ';' separator so it is even more realistic \"European style\"\n",
    "    encoding=\"latin1\"    # <-- key point\n",
    ")\n",
    "\n",
    "print(f\"Creato file {file_name} in encoding latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970238e5",
   "metadata": {
    "id": "970238e5"
   },
   "source": [
    "3. we try to read it again WITHOUT specifying the encoding.<br>\n",
    "This is what a distracted user usually does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1726235f",
   "metadata": {
    "id": "1726235f",
    "outputId": "3ed59a07-93ef-482c-acbd-f950644644f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Errore di decodifica previsto leggendo senza encoding esplicito:\n",
      "'utf-8' codec can't decode byte 0xe0 in position 12: invalid continuation byte\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_fail = pd.read_csv(file_name, sep=\";\")  # no encoding passed, i.e. default = UTF-8\n",
    "    print(\"Letto senza errori?! Ecco le prime righe:\")\n",
    "    print(df_fail.head())\n",
    "except UnicodeDecodeError as e:\n",
    "    print(\"‚ö†Ô∏è Errore di decodifica previsto leggendo senza encoding esplicito:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0545da",
   "metadata": {
    "id": "ae0545da"
   },
   "source": [
    "4. it raises an error: we want to read a `latin1` file as `utf‚Äë8`.<br>\n",
    "correct solution: we read specifying `encoding=latin1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d90885c5",
   "metadata": {
    "id": "d90885c5",
    "outputId": "2e42539d-4742-4268-f81b-da3c9d1d3edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Lettura corretta con encoding='latin1':\n",
      "   ID       Nome   Citt√†                               Note\n",
      "0   1      Andr√©  Torino           pagato 50$ gi√† fatturato\n",
      "1   2       Jos√©  M√°laga  a√±o siguiente -> revisi√≥n t√©cnica\n",
      "2   3  Ana Mar√≠a  Z√ºrich      pi√π vecchio -> gi√† sostituito\n"
     ]
    }
   ],
   "source": [
    "df_ok = pd.read_csv(file_name, sep=\";\", encoding=\"latin1\")\n",
    "print(\"\\nüí° Lettura corretta con encoding='latin1':\")\n",
    "print(df_ok.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02c27f7",
   "metadata": {
    "id": "e02c27f7"
   },
   "source": [
    "5. and what happens with the following dataframe, which contains the `‚Ç¨` character (instead of `$`), which is part of neither `Ascii` nor `latin1`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e670c2b",
   "metadata": {
    "id": "4e670c2b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"ID\": [1, 2, 3],\n",
    "    \"Nome\": [\"Andr√©\", \"Jos√©\", \"Ana Mar√≠a\"],\n",
    "    \"Citt√†\": [\"Torino\", \"M√°laga\", \"Z√ºrich\"],\n",
    "    \"Note\": [\n",
    "        \"pagato 50‚Ç¨ gi√† fatturato\",\n",
    "        \"a√±o siguiente -> revisi√≥n t√©cnica\",\n",
    "        \"pi√π vecchio -> gi√† sostituito\"\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d1936",
   "metadata": {},
   "source": [
    "This code goes wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1adf1fb",
   "metadata": {
    "id": "e1adf1fb",
    "outputId": "b38a7ab8-e8a3-4b43-908c-e10cfa350af2"
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'latin-1' codec can't encode character '\\u20ac' in position 24: ordinal not in range(256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclienti_latin1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m      3\u001b[0m     file_name,\n\u001b[0;32m      4\u001b[0m     index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m     sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m,             \u001b[38;5;66;03m# we also set the ';' separator so it is even more realistic \"European style\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m\"\u001b[39m    \u001b[38;5;66;03m# <-- key point\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreato file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in encoding latin1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3986\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3975\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3977\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3978\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3979\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3983\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3984\u001b[0m )\n\u001b[1;32m-> 3986\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3987\u001b[0m     path_or_buf,\n\u001b[0;32m   3988\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3989\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3990\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3991\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3992\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3993\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3994\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3995\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3996\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3997\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3998\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3999\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   4000\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   4001\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   4002\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   4003\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:270\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:275\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_to_save_header:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_header()\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_body()\n",
      "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:313\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end_i:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_chunk(start_i, end_i)\n",
      "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:324\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    321\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(res\u001b[38;5;241m.\u001b[39m_iter_column_arrays())\n\u001b[0;32m    323\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_index[slicer]\u001b[38;5;241m.\u001b[39m_get_values_for_csv(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n\u001b[1;32m--> 324\u001b[0m libwriters\u001b[38;5;241m.\u001b[39mwrite_csv_rows(\n\u001b[0;32m    325\u001b[0m     data,\n\u001b[0;32m    326\u001b[0m     ix,\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlevels,\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcols,\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter,\n\u001b[0;32m    330\u001b[0m )\n",
      "File \u001b[1;32mpandas/_libs/writers.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'latin-1' codec can't encode character '\\u20ac' in position 24: ordinal not in range(256)"
     ]
    }
   ],
   "source": [
    "file_name = \"clienti_latin1.csv\"\n",
    "df.to_csv(\n",
    "    file_name,\n",
    "    index=False,\n",
    "    sep=\";\",             # we also set the ';' separator so it is even more realistic \"European style\"\n",
    "    encoding=\"latin1\"    # <-- key point\n",
    ")\n",
    "\n",
    "print(f\"Creato file {file_name} in encoding latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23514c60",
   "metadata": {
    "id": "23514c60"
   },
   "source": [
    "6. it raises an error, because `latin1` does not contain the `‚Ç¨` character.<br>\n",
    "We must write in `utf‚Äë8`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb81607",
   "metadata": {
    "id": "7bb81607",
    "outputId": "33c0e3e5-7a32-4cb8-ff8f-3fd2a96f0b74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creato file clienti_latin1.csv in encoding latin1\n"
     ]
    }
   ],
   "source": [
    "file_name = \"clienti_latin1.csv\"\n",
    "df.to_csv(\n",
    "    file_name,\n",
    "    index=False,\n",
    "    sep=\";\",             # we also set the ';' separator so it is even more realistic \"European style\"\n",
    "    encoding=\"utf-8\"    # <-- key point\n",
    ")\n",
    "\n",
    "print(f\"Creato file {file_name} in encoding latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e36a52b-1484-4355-9b4d-c85f6a1d80b6",
   "metadata": {
    "id": "7e36a52b-1484-4355-9b4d-c85f6a1d80b6"
   },
   "source": [
    "üìâ **4. Wrong data type**\n",
    "\n",
    "<u>Problem</u>: numeric columns imported as strings (`object`).<br>\n",
    "<u>Cause</u>: presence of thousand separators, symbols, or empty cells.<br>\n",
    "<u>Solution</u>:\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", thousands=\".\", decimal=\",\")\n",
    "```\n",
    "or after the read:\n",
    "```python\n",
    "    df[\"col\"] = pd.to_numeric(df[\"col\"], errors=\"coerce\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e15596c",
   "metadata": {
    "id": "0e15596c",
    "outputId": "a85ec4d1-90a5-41e3-f779-013590f79954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Creato file CSV 'prezzi_legacy.csv' con separatori migliaia '.' e decimali ','.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ###############################################\n",
    "# 1. CSV CREATION with EUROPEAN FORMATTING      #\n",
    "# ###############################################\n",
    "\n",
    "# Notes:\n",
    "# - \"1.234,50\" = one thousand two hundred thirtyfour comma fifty\n",
    "# - \"2.000\"    = two thousands (integer)\n",
    "# - \"\"         = empty cell (missing value)\n",
    "\n",
    "df_orig = pd.DataFrame({\n",
    "    \"Prodotto\": [\"A123\", \"B777\", \"C900\", \"D010\"],\n",
    "    \"PrezzoUnitario\": [\"1.234,50\", \"99,99\", \"\", \"2.000,00\"],\n",
    "    \"Quantit√†\": [\"1.000\", \"250\", \"\", \"1.500\"]\n",
    "})\n",
    "\n",
    "csv_name = \"prezzi_legacy.csv\"\n",
    "\n",
    "# We save as CSV with ';' because it is very common in Italian administrative exports\n",
    "df_orig.to_csv(\n",
    "    csv_name,\n",
    "    index=False,\n",
    "    sep=\";\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(f\"[OK] Creato file CSV '{csv_name}' con separatori migliaia '.' e decimali ','.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bb500b0",
   "metadata": {
    "id": "5bb500b0",
    "outputId": "b2cfea58-9393-4ad8-9e77-f3ca316c23fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LETTURA SBAGLIATA (senza thousands/decimal) ===\n",
      "\n",
      "DataFrame letto (sbagliato):\n",
      "  Prodotto PrezzoUnitario  Quantit√†\n",
      "0     A123       1.234,50       1.0\n",
      "1     B777          99,99     250.0\n",
      "2     C900            NaN       NaN\n",
      "3     D010       2.000,00       1.5\n",
      "\n",
      "Tipi di dato dopo lettura sbagliata:\n",
      "Prodotto           object\n",
      "PrezzoUnitario     object\n",
      "Quantit√†          float64\n",
      "dtype: object\n",
      "\n",
      "Provo a sommare la colonna Quantit√† (che √® testo):\n",
      "252.5\n"
     ]
    }
   ],
   "source": [
    "# ##################################\n",
    "# 2. WRONG READ (DEFAULT)          #\n",
    "# ##################################\n",
    "\n",
    "print(\"=== LETTURA SBAGLIATA (senza thousands/decimal) ===\")\n",
    "\n",
    "df_bad = pd.read_csv(\n",
    "    csv_name,\n",
    "    sep=\";\"          # we correctly read the column separator\n",
    "                     # but we do NOT tell pandas how to interpret numbers (thousands and decimals)\n",
    ")\n",
    "\n",
    "print(\"\\nDataFrame letto (sbagliato):\")\n",
    "print(df_bad)\n",
    "\n",
    "print(\"\\nTipi di dato dopo lettura sbagliata:\")\n",
    "print(df_bad.dtypes)\n",
    "\n",
    "# Test of numerical operations: here 'Quantit√†' e 'PrezzoUnitario' are still strings (object)\n",
    "print(\"\\nProvo a sommare la colonna Quantit√† (che √® testo):\")\n",
    "try:\n",
    "    print(df_bad[\"Quantit√†\"].sum())\n",
    "except Exception as e:\n",
    "    print(\"Errore durante la somma:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e73f674",
   "metadata": {
    "id": "3e73f674"
   },
   "source": [
    "**Why is `PrezzoUnitario` `object` in `df_bad.dtypes` (instead of `float`)?**\n",
    "\n",
    "For three reasons together:<br>\n",
    "\n",
    "**1. The decimal separator is a comma, not a dot**<br>\n",
    "*Example*: *1.234,50*<br>\n",
    "for pandas (with no extra instructions), *1.234,50* is not a valid number, **it‚Äôs a string**.<br>\n",
    "pandas actually expects *1234.50* (dot for decimals, no thousands separator).<br>\n",
    "So it leaves it as text (`object`).\n",
    "\n",
    "**2. There is a thousands separator `.`**<br>\n",
    "Look at *2.000,00*:<br>\n",
    "the ideal for pandas would be *2000.00*<br>\n",
    "instead it finds *2.000,00*, which looks like ‚Äú2 dot 000 comma 00‚Äù.<br>\n",
    "For the standard parser this is not a valid float ‚Üí it stays a string (`object`).\n",
    "\n",
    "Same thing for *1.000* in the `Quantit√†` column: pandas doesn‚Äôt know whether it‚Äôs ‚Äúone thousand‚Äù or ‚Äúone point zero zero zero‚Äù.<br>\n",
    "So it prefers **not** to guess and keeps it as text (`object`).\n",
    "\n",
    "**3. There are empty cells**<br>\n",
    "In the column you have values like \"\" (empty string).<br>\n",
    "So in the same column you have:\n",
    "\n",
    "* *1.234,50* (text)\n",
    "* *99,99* (text)\n",
    "* \"\" (empty text)\n",
    "* *2.000,00* (text)\n",
    "\n",
    "Heterogeneous column ‚Üí pandas says: ‚Äúok, everything is `object` (strings) and let‚Äôs move on‚Äù.\n",
    "\n",
    "If all the values were clear, English-style numbers (1234.50, 99.99, 2000.00, etc.) then pandas would have inferred `float64` by itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2560874",
   "metadata": {
    "id": "b2560874",
    "outputId": "8280c62e-f112-4ec7-d19e-2ff5567cddab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== LETTURA CORRETTA (thousands='.', decimal=',') ===\n",
      "\n",
      "DataFrame letto (corretto):\n",
      "  Prodotto  PrezzoUnitario  Quantit√†\n",
      "0     A123         1234.50    1000.0\n",
      "1     B777           99.99     250.0\n",
      "2     C900             NaN       NaN\n",
      "3     D010         2000.00    1500.0\n",
      "\n",
      "Tipi di dato dopo lettura corretta:\n",
      "Prodotto           object\n",
      "PrezzoUnitario    float64\n",
      "Quantit√†          float64\n",
      "dtype: object\n",
      "\n",
      "Somma Quantit√† (ora numerica):\n",
      "2750.0\n",
      "\n",
      "Somma PrezzoUnitario (notare i NaN dove c'erano celle vuote):\n",
      "3334.49\n"
     ]
    }
   ],
   "source": [
    "# ############################################\n",
    "# 3. RIGHT READ (input parsing)              #\n",
    "# ############################################\n",
    "\n",
    "print(\"\\n\\n=== LETTURA CORRETTA (thousands='.', decimal=',') ===\")\n",
    "\n",
    "df_good = pd.read_csv(\n",
    "    csv_name,\n",
    "    sep=\";\",\n",
    "    thousands=\".\",  # removes thousands separator\n",
    "    decimal=\",\"     # interprets comma as decimal separator\n",
    ")\n",
    "\n",
    "print(\"\\nDataFrame letto (corretto):\")\n",
    "print(df_good)\n",
    "\n",
    "print(\"\\nTipi di dato dopo lettura corretta:\")\n",
    "print(df_good.dtypes)\n",
    "\n",
    "print(\"\\nSomma Quantit√† (ora numerica):\")\n",
    "print(df_good[\"Quantit√†\"].sum())\n",
    "\n",
    "print(\"\\nSomma PrezzoUnitario (notare i NaN dove c'erano celle vuote):\")\n",
    "print(df_good[\"PrezzoUnitario\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84574a93-10bb-434d-8295-e9164b1e51a5",
   "metadata": {
    "id": "84574a93-10bb-434d-8295-e9164b1e51a5"
   },
   "source": [
    "üßæ **5. Header not on the first line** ‚Äì already seen before\n",
    "\n",
    "<u>Problem</u>: column names are not read correctly.<br>\n",
    "<u>Cause</u>: the file has descriptive lines or metadata at the beginning.<br>\n",
    "<u>Solution</u>:\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", header=2)   # if the header is on the third line\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63024a49",
   "metadata": {
    "id": "63024a49",
    "outputId": "04f41998-1b44-471d-e5ab-da20931dd9b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creato file dati_commentati.csv\n"
     ]
    }
   ],
   "source": [
    "# creates CSV file\n",
    "import csv\n",
    "\n",
    "file_name = \"dati_commentati.csv\"\n",
    "\n",
    "with open(file_name, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    # writes manually two commented lines\n",
    "    f.write(\"# Questo file contiene dati di esempio\\n\")\n",
    "    f.write(\"# Formato: ID,Nome,Valore\\n\")\n",
    "\n",
    "    writer = csv.writer(f, delimiter=\";\")\n",
    "\n",
    "    # Actual header\n",
    "    writer.writerow([\"ID\", \"Nome\", \"Valore\"])\n",
    "\n",
    "    # 3 data rows\n",
    "    writer.writerow([1, \"Alpha\", 10.5])\n",
    "    writer.writerow([2, \"Beta\", 20.0])\n",
    "    writer.writerow([3, \"Gamma\", 7.25])\n",
    "\n",
    "print(f\"Creato file {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1f8539e",
   "metadata": {
    "id": "f1f8539e",
    "outputId": "03939264-2b8b-48ea-b5e8-00899482436d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th># Questo file contiene dati di esempio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th># Formato: ID</th>\n",
       "      <th>Nome</th>\n",
       "      <td>Valore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID;Nome;Valore</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1;Alpha;10.5</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2;Beta;20.0</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3;Gamma;7.25</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    # Questo file contiene dati di esempio\n",
       "# Formato: ID  Nome                                 Valore\n",
       "ID;Nome;Valore NaN                                     NaN\n",
       "1;Alpha;10.5   NaN                                     NaN\n",
       "2;Beta;20.0    NaN                                     NaN\n",
       "3;Gamma;7.25   NaN                                     NaN"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wrong read\n",
    "pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19dcfd25",
   "metadata": {
    "id": "19dcfd25",
    "outputId": "2f5f4568-dac4-4b87-a455-32c4b78db711"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Nome</th>\n",
       "      <th>Valore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alpha</td>\n",
       "      <td>10.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Beta</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Gamma</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID   Nome  Valore\n",
       "0   1  Alpha   10.50\n",
       "1   2   Beta   20.00\n",
       "2   3  Gamma    7.25"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# right read\n",
    "pd.read_csv(\n",
    "    file_name,\n",
    "    sep=\";\",        # column separator\n",
    "    header=2)       # header is in third row (Python counts from 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "977f70f4",
   "metadata": {
    "id": "977f70f4",
    "outputId": "8b6c9fec-b7f0-4de1-de1d-a45da1e93ce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID   Nome  Valore\n",
      "0   1  Alpha   10.50\n",
      "1   2   Beta   20.00\n",
      "2   3  Gamma    7.25\n",
      "ID          int64\n",
      "Nome       object\n",
      "Valore    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# smart read\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"dati_commentati.csv\",\n",
    "    sep=\";\",         # field separator\n",
    "    comment=\"#\"      # ignores all lines beginning with '#'\n",
    ")\n",
    "\n",
    "print(df)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a34b8c9-0a5b-4333-82b7-1d87f085d1a5",
   "metadata": {
    "id": "3a34b8c9-0a5b-4333-82b7-1d87f085d1a5"
   },
   "source": [
    "ü™ì **6. File too large**\n",
    "\n",
    "<u>Problem</u>: `MemoryError` or very slow loading.<br>\n",
    "<u>Cause</u>: CSV much bigger than available RAM.<br>\n",
    "<u>Solution</u>:<br>\n",
    "\n",
    "**Chunk** loading:\n",
    "```python\n",
    "    for chunk in pd.read_csv(\"file.csv\", chunksize=100000):\n",
    "        process(chunk)                                        # 'process' is a user-defined function\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd48b972",
   "metadata": {
    "id": "dd48b972"
   },
   "source": [
    "Let‚Äôs see **how *chunks* work** in <u>two parts</u>:\n",
    "\n",
    "**1. the internal read**:\n",
    "\n",
    "```python\n",
    "pd. (..., chunksize=100000)\n",
    "```\n",
    "\n",
    "Normally, the function `pd.read_csv(\"file.csv\")` **reads the entire file into RAM** and returns **a single `DataFrame`**.<br>\n",
    "With `chunksize=100000`, instead, pandas does NOT load everything.<br>\n",
    "It returns an **`iterator`** (a generator) that yields one `DataFrame` at a time, each with **at most 100,000 rows**.\n",
    "\n",
    "So:\n",
    "\n",
    "* first loop ‚Üí rows 0‚Äì99,999\n",
    "* second loop ‚Üí rows 100,000‚Äì199,999\n",
    "* third loop ‚Üí etc.\n",
    "\n",
    "‚Ä¶until the end of the file.\n",
    "\n",
    "‚ö†Ô∏è This means that **in memory, at any given time, there are only 100k rows**, not millions/billions. This is perfect if **the file is too large to fit in RAM**.\n",
    "\n",
    "**2. the outer loop**:\n",
    "\n",
    "```python\n",
    "    for chunk in ... :\n",
    "```\n",
    "\n",
    "`chunk` is a ‚Äúpartial‚Äù pandas `DataFrame`, i.e. a slice of the CSV.<br>\n",
    "The `for` loops over all slices of the file, one after the other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773d63e",
   "metadata": {
    "id": "8773d63e"
   },
   "source": [
    "üß™ Let‚Äôs look at **a concrete example**, in two steps:\n",
    "\n",
    "* we define a **function to create the CSV file**, in <u>two variants</u>:\n",
    "\n",
    "  * the <u>small</u> version of the function (10 rows) ‚Äî useful to inspect by eye\n",
    "  * the <u>large</u> version of the function (1_000_000 rows) ‚Äî useful for real tests on `chunk`\n",
    "  * you can choose which version to use by changing only the `n_righe` argument in the call.\n",
    "* we process it in chunks to **compute the global sum of the `Importo` column**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e27792",
   "metadata": {
    "id": "28e27792",
    "outputId": "0433c720-6aad-4cbb-f10f-b79360153a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creato file CSV 'transazioni_demo.csv' con 10 righe.\n",
      "Creato file CSV 'transazioni_grandi.csv' con 1000000 righe.\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: definition of a function that creates a big size CSV to test chunk loading\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "def crea_csv_grande(\n",
    "    file_name=\"transazioni_grandi.csv\",      # default\n",
    "    n_righe=1_000_000,                       # default  (1_000_000: sugar syntax to make the number more readable)\n",
    "    seed=42                                  # default\n",
    "):\n",
    "    \"\"\"\n",
    "    Crea un CSV con molte righe, con le colonne:\n",
    "    ID, DataOperazione, Categoria, Importo\n",
    "\n",
    "    - ID: intero progressivo\n",
    "    - DataOperazione: data fittizia\n",
    "    - Categoria: tipo transazione (es. Vendita / Rimborso / Spesa)\n",
    "    - Importo: float positivo o negativo\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    categorie = [\n",
    "        \"Vendita\",\n",
    "        \"Rimborso\",\n",
    "        \"Spesa Marketing\",\n",
    "        \"Spesa Fornitore\",\n",
    "        \"Abbonamento\",\n",
    "        \"Servizio\"\n",
    "    ]\n",
    "\n",
    "    start_date = datetime.date(2024, 1, 1)\n",
    "\n",
    "    # Creates the CSV file\n",
    "    with open(file_name, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f, delimiter=\",\")\n",
    "\n",
    "        # Header\n",
    "        writer.writerow([\"ID\", \"DataOperazione\", \"Categoria\", \"Importo\"])\n",
    "\n",
    "        for i in range(1, n_righe + 1):\n",
    "            # data = start_date + offset giorni\n",
    "            data_operazione = start_date + datetime.timedelta(days=i % 365)\n",
    "\n",
    "            categoria = random.choice(categorie)\n",
    "\n",
    "            # Importo (Amount):\n",
    "            # - positive sales between 10 and 500\n",
    "            # - negative returns between -200 and -5\n",
    "            # - negatives expenses between -1000 and -20\n",
    "            if categoria == \"Vendita\" or categoria == \"Abbonamento\" or categoria == \"Servizio\":\n",
    "                importo = round(random.uniform(10, 500), 2)\n",
    "            elif categoria == \"Rimborso\":\n",
    "                importo = round(random.uniform(-200, -5), 2)\n",
    "            else:\n",
    "                # Spesa Marketing / Spesa Fornitore\n",
    "                importo = round(random.uniform(-1000, -20), 2)\n",
    "\n",
    "            writer.writerow([\n",
    "                i,\n",
    "                data_operazione.isoformat(),  # tipo 2024-03-15\n",
    "                categoria,\n",
    "                importo\n",
    "            ])\n",
    "\n",
    "    print(f\"Creato file CSV '{file_name}' con {n_righe} righe.\")\n",
    "\n",
    "# Example of use of the function (the MAIN)\n",
    "# if __name__ == \"__main__\": tells:\n",
    "# - \"runs this code block just if I'm running it directly within this file, and NOT if I'm importing it from within another file\n",
    "# \n",
    "if __name__ == \"__main__\":\n",
    "    # small demo version (to look at it manually)\n",
    "    crea_csv_grande(\"transazioni_demo.csv\", n_righe=10)\n",
    "\n",
    "    # large demo version (to test chunksize ecc.)\n",
    "    # ATTENTION: this code creates ~1 million rows. Change this number as you prefer.\n",
    "    crea_csv_grande(\"transazioni_grandi.csv\", n_righe=1_000_000)   # 1_000_000: sugar synthax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1e97687",
   "metadata": {
    "id": "a1e97687",
    "outputId": "1f7bdbda-f5d3-4093-97c7-3cfcb7838110"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totale Importo: -59772293.91\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: process in chunks:\n",
    "\n",
    "totale = 0.0\n",
    "\n",
    "for chunk in pd.read_csv(\"transazioni_grandi.csv\", chunksize=100_000):   # numeric sugar syntax\n",
    "    totale += chunk[\"Importo\"].sum()\n",
    "\n",
    "print(\"Totale Importo:\", totale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ce36e",
   "metadata": {
    "id": "ab4ce36e"
   },
   "source": [
    "The code in the previous cell gives **the global sum of the `Importo` column** without ever loading the whole million rows into RAM at once ‚úÖ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3cb71a",
   "metadata": {
    "id": "8b3cb71a"
   },
   "source": [
    "---\n",
    "\n",
    "**Comment on the NUMERICAL result obtained**<br>\n",
    "\n",
    "The dataset we created earlier has both inflows and outflows.<br>\n",
    "In the CSV generator we had this logic:\n",
    "\n",
    "* Categories like `Vendita`, `Abbonamento`, `Servizio` ‚Üí **positive** amounts (revenues, +10 to +500)\n",
    "* Categories like `\"Rimborso\"` ‚Üí **negative** amounts (refunds to the customer, -5 to -200)\n",
    "* `Spesa Marketing` and `Spesa Fornitore` ‚Üí **large negative** amounts (costs, from -1000 to -20)\n",
    "\n",
    "So:\n",
    "\n",
    "* sales bring money in,\n",
    "* expenses and suppliers pull money out,\n",
    "* and often costs are, in absolute value, larger than sales.\n",
    "\n",
    "If in the sample there are many cost rows compared to sales, the final balance drops hard ‚Üí hence the very negative total like `-59,772,293.91`.\n",
    "\n",
    "In business terms: **we‚Äôre spending more than we‚Äôre earning** üòÖ.\n",
    "\n",
    "Is that a ‚Äúnormal‚Äù result?<br>\n",
    "**Yes, it‚Äôs consistent with the random generation**:\n",
    "\n",
    "* negative amounts can go down to -1000\n",
    "* positive amounts go only up to +500\n",
    "\n",
    "So even if half the rows were sales and half expenses, the expense side would still win in absolute value.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac0d14-6fb4-4909-9c4d-1935bc98df71",
   "metadata": {
    "id": "86ac0d14-6fb4-4909-9c4d-1935bc98df71"
   },
   "source": [
    "üßÆ **7. Columns with missing or misaligned values**\n",
    "\n",
    "<u>Problem</u>: rows with different number of columns, error like `ParserError: Error tokenizing data`.<br>\n",
    "<u>Cause</u>: unclosed quotes or separators inside fields.<br>\n",
    "<u>Solution</u>:\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", on_bad_lines=\"skip\", quoting=csv.QUOTE_NONE)\n",
    "```\n",
    "\n",
    "Or check the delimiters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc8974",
   "metadata": {
    "id": "20fc8974"
   },
   "source": [
    "Below is the code in 3 steps (for each of the two errors):\n",
    "- creation of the csv file\n",
    "- wrong (intentional) reading\n",
    "- robust reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f99a3",
   "metadata": {
    "id": "705f99a3",
    "outputId": "c6a78c3c-3fa8-4f8a-a09f-d86ea9a08794"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ creati i due file CSV di test\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 1) CSV con VIRGOLETTE NON CHIUSE  -\n",
    "# -----------------------------------\n",
    "content_unclosed = \"\"\"id,name,amount,notes\n",
    "1,Mario Rossi,1200,OK\n",
    "2,Luigi Bianchi,950,pagato\n",
    "3,Carla Verdi,800,\"nota con virgolette non chiuse\n",
    "4,Paolo Neri,700,ok\n",
    "\"\"\"\n",
    "\n",
    "file_unclosed = \"csv_virgolette_non_chiuse.csv\"\n",
    "with open(file_unclosed, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    f.write(content_unclosed)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2) CSV con SEPARATORI DENTRO I CAMPI (non quotati) -\n",
    "# ----------------------------------------------------\n",
    "# header: 4 colonne\n",
    "content_separators = \"\"\"id,name,city,amount\n",
    "1,Mario Rossi,Milano,1200\n",
    "2,Luigi Bianchi,Roma,900\n",
    "3,Carla Verdi,Milano, Italia,800\n",
    "4,Paolo Neri,Torino,700\n",
    "\"\"\"\n",
    "\n",
    "file_separators = \"csv_separatori_dentro_campi.csv\"\n",
    "with open(file_separators, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    f.write(content_separators)\n",
    "\n",
    "print(\"‚úÖ creati i due file CSV di test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19bd7b",
   "metadata": {
    "id": "8a19bd7b",
    "outputId": "6058669c-063d-4a13-b2e3-10d7adba7550"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 1) TEST: virgolette non chiuse ===\n",
      "‚ùå errore atteso (virgolette non chiuse):\n",
      "Error tokenizing data. C error: EOF inside string starting at row 3\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# TEST di LETTURA file 1   =\n",
    "# ==========================\n",
    "\n",
    "print(\"\\n=== 1) TEST: virgolette non chiuse ===\")\n",
    "try:\n",
    "    df1 = pd.read_csv(file_unclosed)\n",
    "    print(df1)\n",
    "except Exception as e:\n",
    "    print(\"‚ùå errore atteso (virgolette non chiuse):\")\n",
    "    print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30c4ef6",
   "metadata": {
    "id": "d30c4ef6",
    "outputId": "b4e9d0d8-fe58-4965-8482-18e6a0abc34a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ lettura robusta (file 1):\n",
      "   id           name  amount                            notes\n",
      "0   1    Mario Rossi    1200                               OK\n",
      "1   2  Luigi Bianchi     950                           pagato\n",
      "2   3    Carla Verdi     800  \"nota con virgolette non chiuse\n",
      "3   4     Paolo Neri     700                               ok\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# lettura 'robusta' del file 1  =\n",
    "# ===============================\n",
    "df1_ok = pd.read_csv(\n",
    "    file_unclosed,\n",
    "    on_bad_lines=\"skip\",\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    engine=\"python\",\n",
    ")\n",
    "print(\"\\n‚úÖ lettura robusta (file 1):\")\n",
    "print(df1_ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23db78eb",
   "metadata": {
    "id": "23db78eb"
   },
   "source": [
    "`quoting = csv.QUOTE_NONE`:<br>\n",
    "this tells it: ‚Äúdo not treat quotes (\") as something special, consider them normal text‚Äù.\n",
    "\n",
    "Why did we also put `engine=\"python\"`?<br>\n",
    "...\n",
    "**Practical rule:**<br>\n",
    "1 format ‚Üí `parse_dates`<br>\n",
    "few formats ‚Üí `to_datetime`<br>\n",
    "mixed/dirty formats ‚Üí **custom function** ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797e8be",
   "metadata": {
    "id": "e797e8be",
    "outputId": "aecbf25c-1cbc-441f-c1ac-066d15b1dea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 2) TEST: separatori dentro i campi ===\n",
      "‚ùå errore atteso (troppi separatori):\n",
      "Error tokenizing data. C error: Expected 4 fields in line 4, saw 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# TEST di LETTURA file 2   =\n",
    "# ==========================\n",
    "\n",
    "print(\"\\n=== 2) TEST: separatori dentro i campi ===\")\n",
    "try:\n",
    "    df2 = pd.read_csv(file_separators)\n",
    "    print(df2)\n",
    "except Exception as e:\n",
    "    print(\"‚ùå errore atteso (troppi separatori):\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d57bb",
   "metadata": {
    "id": "dc2d57bb",
    "outputId": "3676d4e9-d253-416f-cd8e-ff2abb6a94f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ lettura robusta (file 2):\n",
      "   id           name    city  amount\n",
      "0   1    Mario Rossi  Milano    1200\n",
      "1   2  Luigi Bianchi    Roma     900\n",
      "2   4     Paolo Neri  Torino     700\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# lettura 'robusta' del file 2  =\n",
    "# ===============================\n",
    "\n",
    "df2_ok = pd.read_csv(\n",
    "    file_separators,\n",
    "    on_bad_lines=\"skip\",\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    engine=\"python\",\n",
    ")\n",
    "print(\"\\n‚úÖ lettura robusta (file 2):\")\n",
    "print(df2_ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab5064-1469-451a-8732-786123690fa4",
   "metadata": {
    "id": "caab5064-1469-451a-8732-786123690fa4"
   },
   "source": [
    "üß† **8. Date non interpretate correttamente**\n",
    "\n",
    "<u>Problema</u>: le date restano stringhe o sono nel formato errato.<br>\n",
    "<u>Soluzione</u>:\n",
    "```python\n",
    "pd.read_csv(\"file.csv\", parse_dates=[\"data\"])\n",
    "```\n",
    "\n",
    "oppure\n",
    "```python\n",
    "df[\"data\"] = pd.to_datetime(df[\"data\"], dayfirst=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c384078",
   "metadata": {
    "id": "2c384078"
   },
   "source": [
    "Creiamo un CSV con deliberatamente dentro date mischiate (italiane, americane, con testo, con ore) cos√¨ `read_csv` non le riconosce e le lascia come `object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a34ec",
   "metadata": {
    "id": "ac1a34ec",
    "outputId": "5fa1ec7e-7713-46d7-a41a-3087316baf8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ creato date_mischiate.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Creazione CSV con date \"brutte\"\n",
    "# CSV \"cattivo\": formati data diversi ‚Üí pandas non riesce a unificarli\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "csv_text = \"\"\"data;descrizione;importo\n",
    "01/02/2025;Fattura cliente A;120.50\n",
    "2025-02-01;Fattura cliente B;85.00\n",
    "12/31/2024;Formato USA;15.75\n",
    "31/12/2024;Chiusura anno;999.99\n",
    "2025/02/01 14:30;Con orario;50.00\n",
    ";Data mancante;0.00\n",
    "non-data;Valore sporco;5.25\n",
    "\"\"\"\n",
    "\n",
    "with open(\"date_mischiate.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    f.write(csv_text)\n",
    "\n",
    "print(\"‚úÖ creato date_mischiate.csv\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf127f",
   "metadata": {
    "id": "7adf127f"
   },
   "source": [
    "E' un file CSV con date miste.\n",
    "\n",
    "Ora facciamo la lettura ‚Äúingenua‚Äù (pandas non capisce le date e le lascia `object`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e92d4f6",
   "metadata": {
    "id": "2e92d4f6",
    "outputId": "3f316ca0-a12d-49e6-c1cf-876d248629c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¥ LETTURA INGENUA\n",
      "data            object\n",
      "descrizione     object\n",
      "importo        float64\n",
      "dtype: object\n",
      "               data        descrizione  importo\n",
      "0        01/02/2025  Fattura cliente A   120.50\n",
      "1        2025-02-01  Fattura cliente B    85.00\n",
      "2        12/31/2024        Formato USA    15.75\n",
      "3        31/12/2024      Chiusura anno   999.99\n",
      "4  2025/02/01 14:30         Con orario    50.00\n",
      "5               NaN      Data mancante     0.00\n",
      "6          non-data      Valore sporco     5.25 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Lettura \"sbagliata\" / ingenua\n",
    "df_raw = pd.read_csv(\"date_mischiate.csv\", sep=\";\")\n",
    "print(\"üî¥ LETTURA INGENUA\")\n",
    "print(df_raw.dtypes)\n",
    "print(df_raw, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4da2ad3",
   "metadata": {
    "id": "f4da2ad3"
   },
   "source": [
    "`data` √® `object` ‚Üí cio√® stringa.<br>\n",
    "Adesso le due soluzioni:\n",
    "\n",
    "Soluzione 1 ‚Äì direttamente in `read_csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47731e86",
   "metadata": {
    "id": "47731e86",
    "outputId": "c577319e-3795-4600-fcaf-d05e8d66a613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü† LETTURA CON parse_dates (pandas non ce la fa)\n",
      "data            object\n",
      "descrizione     object\n",
      "importo        float64\n",
      "dtype: object\n",
      "               data        descrizione  importo\n",
      "0        01/02/2025  Fattura cliente A   120.50\n",
      "1        2025-02-01  Fattura cliente B    85.00\n",
      "2        12/31/2024        Formato USA    15.75\n",
      "3        31/12/2024      Chiusura anno   999.99\n",
      "4  2025/02/01 14:30         Con orario    50.00\n",
      "5               NaN      Data mancante     0.00\n",
      "6          non-data      Valore sporco     5.25 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 3) LETTURA con parse_dates\n",
    "# ‚Üí con queste date miste, pandas si arrende\n",
    "# ‚Üí data RESTA object\n",
    "# =========================================\n",
    "df_auto = pd.read_csv(\n",
    "    \"date_mischiate.csv\",\n",
    "    sep=\";\",\n",
    "    parse_dates=[\"data\"],\n",
    "    dayfirst=True\n",
    ")\n",
    "print(\"üü† LETTURA CON parse_dates (pandas non ce la fa)\")\n",
    "print(df_auto.dtypes)\n",
    "print(df_auto, \"\\n\")\n",
    "# üëâ data = object\n",
    "# perch√© nel file ci sono: dd/mm/yyyy, ISO, mm/dd/yyyy, con orario, vuote, testo...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cff7fc",
   "metadata": {
    "id": "97cff7fc",
    "outputId": "de08422a-dffb-4447-986e-4c2d6965281a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ LETTURA ROBUSTA (dopo apply)\n",
      "data           datetime64[ns]\n",
      "descrizione            object\n",
      "importo               float64\n",
      "dtype: object\n",
      "                 data        descrizione  importo\n",
      "0 2025-02-01 00:00:00  Fattura cliente A   120.50\n",
      "1 2025-02-01 00:00:00  Fattura cliente B    85.00\n",
      "2 2024-12-31 00:00:00        Formato USA    15.75\n",
      "3 2024-12-31 00:00:00      Chiusura anno   999.99\n",
      "4 2025-02-01 14:30:00         Con orario    50.00\n",
      "5                 NaT      Data mancante     0.00\n",
      "6                 NaT      Valore sporco     5.25\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 4) LETTURA ROBUSTA (quella che DEVE riuscire)\n",
    "# ‚Üí leggiamo come stringa\n",
    "# ‚Üí convertiamo noi una per una\n",
    "# =========================================\n",
    "\n",
    "# definizione di una funzione di parsing \"flessibile\"\n",
    "def parse_flessibile(x: str):\n",
    "    if pd.isna(x) or x == \"\":\n",
    "        return pd.NaT\n",
    "    # proviamo pi√π formati noti\n",
    "    for fmt in (\"%d/%m/%Y\", \"%Y-%m-%d\", \"%m/%d/%Y\", \"%Y/%m/%d %H:%M\", \"%d/%m/%Y %H:%M\"):\n",
    "        try:\n",
    "            return datetime.strptime(x, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return pd.NaT   # quello che proprio non √® data\n",
    "\n",
    "# la apply sulla colonna \"data\"\n",
    "df_ok = pd.read_csv(\"date_mischiate.csv\", sep=\";\")\n",
    "df_ok[\"data\"] = df_ok[\"data\"].apply(parse_flessibile)\n",
    "\n",
    "print(\"üü¢ LETTURA ROBUSTA (dopo apply)\")\n",
    "print(df_ok.dtypes)\n",
    "print(df_ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c825068",
   "metadata": {
    "id": "6c825068"
   },
   "source": [
    "Vediamo **riga per riga** cosa fa il codice della cella precedente:\n",
    "\n",
    "1. `def parse_flessibile(x: str):`<br>\n",
    "definisce una funzione che riceve **una sola cella** (una stringa) e restituisce una **data** oppure `NaT`.\n",
    "\n",
    "2. `if pd.isna(x) or x == \"\":`<br>\n",
    "se la cella √® vuota (`\"\"`) oppure √® un NA (`NaN` letto da pandas) ‚Üí non prova nemmeno ‚Üí **restituisce** `pd.NaT`.<br>\n",
    "(`pd.NaT` = Not A Time, l‚Äôequivalente di `NaN` ma per le date.)\n",
    "\n",
    "3. `for fmt in (...):`<br>\n",
    "qui c‚Äô√® la lista dei **formati che vuole provare**, in ordine:\n",
    "    - `\"%d/%m/%Y\"` ‚Üí 31/12/2024 (italiano)\n",
    "    - `\"%Y-%m-%d\"` ‚Üí 2025-02-01 (ISO)\n",
    "    - `\"%m/%d/%Y\"` ‚Üí 12/31/2024 (americano)\n",
    "    - `\"%Y/%m/%d %H:%M\"` ‚Üí 2025/02/01 14:30\n",
    "    - `\"%d/%m/%Y %H:%M\"` ‚Üí 31/12/2024 09:15\n",
    "\n",
    "    L‚Äôidea √®: ‚Äúnon sa il formato ‚Üí li prova tutti‚Äù.\n",
    "\n",
    "4. `try: return datetime.strptime(x, fmt)`<br>\n",
    "prova a convertire *quella singola stringa* col formato corrente.\n",
    "- se **funziona** ‚Üí esce subito dalla funzione (`return`) ed abbiamo la `datetime`\n",
    "- se **non funziona** ‚Üí scatta il `ValueError` ‚Üí va nell‚Äô`except`\n",
    "\n",
    "5. `except ValueError: continue`<br>\n",
    "cio√®: ‚Äúok, con questo formato non andava ‚Üí prova il prossimo‚Äù.\n",
    "\n",
    "6. `return pd.NaT` (**alla fine**)<br>\n",
    "se provati **tutti** i formati e nessuno ha funzionato ‚Üí quella riga non √® una data ‚Üí la si marca come vuota (`NaT`).\n",
    "\n",
    "---\n",
    "\n",
    "**Perch√© c‚Äô√® l‚Äôapply**??\n",
    "```python\n",
    "df_ok = pd.read_csv(\"date_mischiate.csv\", sep=\";\")\n",
    "df_ok[\"data\"] = df_ok[\"data\"].apply(parse_flessibile)\n",
    "```\n",
    "\n",
    "- `read_csv(...)` legge tutta la colonna come stringhe (perch√© erano tutte diverse).\n",
    "- `df_ok[\"data\"].apply(parse_flessibile)` vuol dire:<br>\n",
    "    ‚Äúper **ogni riga** della colonna `data` esegue `parse_flessibile(...)`‚Äù.\n",
    "- il risultato √® **una nuova Series di tipo `datetime`** (con dentro anche dei `NaT`).\n",
    "- la riassegna a `df_ok[\"data\"]` ‚Üí la colonna diventa davvero `datetime64[ns]`.\n",
    "\n",
    "√à il trucco classico: **quando `parse_dates` non basta** ‚Üí si fa la `apply`.\n",
    "\n",
    "---\n",
    "\n",
    "**Perch√© non abbiamo usato solo `pd.to_datetime(...)`?**\n",
    "\n",
    "Potevamo fare:\n",
    "```python\n",
    "df_ok[\"data\"] = pd.to_datetime(df_ok[\"data\"], dayfirst=True, errors=\"coerce\")\n",
    "```\n",
    "\n",
    "e in molti casi va bene.<br>\n",
    "Qui per√≤ avevamo formati sia italiani sia americani, sia con orario che no. `to_datetime` da solo a volte indovina, a volte no.<br>\n",
    "Con questa funzione di parsing, invece, decidiamo noi l‚Äôordine dei formati.<br>\n",
    "Esempio: prima prova italiano, poi americano ‚Üí cos√¨ non sbaglia 03/04/2025.\n",
    "\n",
    "---\n",
    "\n",
    "**Cosa succede alle righe ‚Äúsporche‚Äù?**\n",
    "- `\"\"` ‚Üí `NaT`\n",
    "- `\"non-data\"` ‚Üí nessun formato lo capisce ‚Üí `NaT`\n",
    "- `\"2025/02/01 14:30\"` ‚Üí lo becca al 4¬∞ formato ‚Üí diventa una vera `datetime`\n",
    "- `\"12/31/2024\"` ‚Üí lo becca al 3¬∞ formato ‚Üí ok\n",
    "- `\"31/12/2024\"` ‚Üí lo becca al 1¬∞ formato ‚Üí ok\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "print(df_ok.dtypes)\n",
    "```\n",
    "\n",
    "data           datetime64[ns]<br>\n",
    "descrizione            object<br>\n",
    "importo               float64<br>\n",
    "dtype: object<br>\n",
    "\n",
    "E' questo il risultato che volevamo fin dall‚Äôinizio: la colonna non √® pi√π `object`, √® una colonna di date üí™\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a007798",
   "metadata": {
    "id": "5a007798"
   },
   "source": [
    "**RIASSUNTO FINALE del punto 8**\n",
    "\n",
    "Se le date sono **tutte nello stesso formato** ‚Üí<br>\n",
    "`pd.read_csv(..., parse_dates=[\"data\"])` va benissimo.\n",
    "\n",
    "Se le date hanno **formati diversi ma ‚Äúsimili‚Äù** (tutte europee, o tutte ISO) ‚Üí<br>\n",
    "si pu√≤ leggere normalmente e poi fare:\n",
    "    ```python\n",
    "    df[\"data\"] = pd.to_datetime(df[\"data\"], dayfirst=True, errors=\"coerce\")\n",
    "    ```\n",
    "\n",
    "spesso basta.\n",
    "\n",
    "Se le date sono proprio **eterogenee** (eu, usa, con ora, vuote, testo) ‚Üí<br>\n",
    "`parse_dates` da solo non basta, e a volte nemmeno `to_datetime(...)` indovinato;<br>\n",
    "allora conviene una **funzione di parsing flessibile** che provi pi√π formati.\n",
    "\n",
    "**Regola pratica:**<br>\n",
    "1 formato ‚Üí `parse_dates`<br>\n",
    "pochi formati ‚Üí `to_datetime`<br>\n",
    "formati misti/sporchi ‚Üí **funzione custom** ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6f29c-cd33-4d11-b3e1-0b6c97d9b118",
   "metadata": {
    "id": "f2c6f29c-cd33-4d11-b3e1-0b6c97d9b118"
   },
   "source": [
    "‚¨ú **9. Duplicates or whitespace in column names**\n",
    "\n",
    "<u>Problem</u>: names with spaces or duplicates (`'Name '` ‚â† `'Name'`).<br>\n",
    "<u>Solution</u>:\n",
    "```python\n",
    "df.columns = df.columns.str.strip()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329743cc",
   "metadata": {
    "id": "329743cc",
    "outputId": "8d8b1f71-50f2-4df7-87cc-2a8bf1557613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ creato con_spazi.csv\n"
     ]
    }
   ],
   "source": [
    "# 1) creo un CSV con nomi colonna con spazi\n",
    "csv_text = \"\"\"  data  ;  nome cliente ; importo ;  note\n",
    "01/02/2025;Mario Rossi;120.50;pagato\n",
    "02/02/2025;  Anna Bianchi ;89.00;ritardo\n",
    "03/02/2025;ACME S.p.A.;250.00;\n",
    "\"\"\"\n",
    "\n",
    "file_name = \"con_spazi.csv\"\n",
    "with open(file_name, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    f.write(csv_text)\n",
    "\n",
    "print(f\"‚úÖ creato {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d66f6fd",
   "metadata": {
    "id": "6d66f6fd",
    "outputId": "e95a748b-15c5-460f-f5e6-152afa9ecf47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé colonne lette (sporche):\n",
      "['  data  ', '  nome cliente ', ' importo ', '  note ']\n"
     ]
    }
   ],
   "source": [
    "# 2) lettura \"normale\" ‚Üí i nomi sono sporchi\n",
    "df = pd.read_csv(file_name, sep=\";\")\n",
    "print(\"üîé colonne lette (sporche):\")\n",
    "print(repr(df.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d47684",
   "metadata": {
    "id": "23d47684",
    "outputId": "40b44fd3-68db-4f07-d848-600cbc825c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ colonne dopo strip():\n",
      "['data', 'nome cliente', 'importo', 'note']\n",
      "\n",
      "üìÑ dataframe finale:\n",
      "         data     nome cliente  importo     note\n",
      "0  01/02/2025      Mario Rossi    120.5   pagato\n",
      "1  02/02/2025    Anna Bianchi      89.0  ritardo\n",
      "2  03/02/2025      ACME S.p.A.    250.0      NaN\n"
     ]
    }
   ],
   "source": [
    "# 3) pulizia nomi colonna\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "print(\"\\n‚úÖ colonne dopo strip():\")\n",
    "print(repr(df.columns.tolist()))\n",
    "\n",
    "print(\"\\nüìÑ dataframe finale:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d89d151",
   "metadata": {
    "id": "9d89d151"
   },
   "source": [
    "**What happened?**\n",
    "- before `strip()` the columns were like:<br>\n",
    "[*'  date  ', '  customer name ', ' amount ', '  notes ']*<br>\n",
    "- after:<br>\n",
    "*['date', 'customer name', 'amount', 'notes']*<br>\n",
    "\n",
    "So if you want to do:\n",
    "```python\n",
    "df[\"date\"]\n",
    "```\n",
    "now it works, while before you would have had to write `df[\" date \"]` and it‚Äôs not nice üòÖ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dfd8c9",
   "metadata": {
    "id": "42dfd8c9"
   },
   "source": [
    "üß± **10. Quotes and special characters**\n",
    "\n",
    "<u>Problem</u>: CSV with inner quotes, double quotes, etc.\n",
    "<u>Solution</u>:\n",
    "```python\n",
    "pd.read_csv(\"file.csv\", quotechar='\"', escapechar='\\\\')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa07a51",
   "metadata": {
    "id": "3fa07a51"
   },
   "source": [
    "Let‚Äôs see the usual flow (as for the previous errors), in this case with 4 steps:\n",
    "- we create a CSV **deliberately dirty, but not enough ‚Üí it still manages to read it** (with quotes inside, doubled quotes, backslash‚Ä¶)\n",
    "- we try the **naive reading** ‚Üí it breaks / errors / splits columns\n",
    "- we do the **robust reading** ‚Üí `quotechar='\"'`, `escapechar='\\\\'`, and if we want also `engine=\"python\"` to be safe.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc81c93a",
   "metadata": {
    "id": "cc81c93a",
    "outputId": "134acd68-fe56-4566-b867-1f09435be9d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scritto csv_sporco_ok.csv\n",
      "id,descrizione,note\n",
      "1,\"Martello, 500g\",\"tutto ok\"\n",
      "2,\"Cacciavite \\\"piatto\\\"\",\"virgolette con backslash (\\\")\"\n",
      "3,\"Set \"\"professionale\"\" 24 pz\",\"virgolette raddoppiate nel campo descrizione\"\n",
      "4,\"C:\\\\attrezzi\\\\nuovo\",\"percorso Windows con backslash\"\n",
      "\n",
      "\n",
      "=== CASO A - LETTURA INGENUA (funziona) ===\n",
      "   id                descrizione                                          note\n",
      "0   1             Martello, 500g                                      tutto ok\n",
      "1   2      Cacciavite \\piatto\\\"\"                 virgolette con backslash (\\)\"\n",
      "2   3  Set \"professionale\" 24 pz  virgolette raddoppiate nel campo descrizione\n",
      "3   4        C:\\\\attrezzi\\\\nuovo                percorso Windows con backslash\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# =========================================================\n",
    "# CASO A - CSV \"sporco ma leggibile\"\n",
    "# ---------------------------------------------------------\n",
    "# Qui vogliamo mostrare che: anche se ci sono virgolette interne\n",
    "# e backslash, la lettura ingenua *potrebbe* funzionare comunque.\n",
    "# =========================================================\n",
    "\n",
    "file_ok = \"csv_sporco_ok.csv\"\n",
    "\n",
    "csv_ok = (\n",
    "    # riga 1\n",
    "    'id,descrizione,note\\n'\n",
    "    # riga 2\n",
    "    '1,\"Martello, 500g\",\"tutto ok\"\\n'\n",
    "    # riga 3 - qui c'√® il backslash + virgolette: \\\"piatto\\\"\n",
    "    '2,\"Cacciavite \\\\\"piatto\\\\\"\",\"virgolette con backslash (\\\\\")\"\\n'\n",
    "    # riga 4 - virgolette raddoppiate stile CSV\n",
    "    '3,\"Set \"\"professionale\"\" 24 pz\",\"virgolette raddoppiate nel campo descrizione\"\\n'\n",
    "    # riga 5 - percorso Windows\n",
    "    '4,\"C:\\\\\\\\attrezzi\\\\\\\\nuovo\",\"percorso Windows con backslash\"\\n'\n",
    ")\n",
    "\n",
    "Path(file_ok).write_text(csv_ok, encoding=\"utf-8\")\n",
    "print(f\"‚úÖ Scritto {file_ok}\")\n",
    "print(csv_ok)\n",
    "\n",
    "print(\"\\n=== CASO A - LETTURA INGENUA (funziona) ===\")\n",
    "# üëâ QUI *NON* mettiamo n√© quotechar n√© escapechar\n",
    "df_ok_naive = pd.read_csv(file_ok)\n",
    "print(df_ok_naive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f513c7b",
   "metadata": {
    "id": "0f513c7b"
   },
   "source": [
    "We don‚Äôt want the warning anymore ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d002b",
   "metadata": {
    "id": "514d002b",
    "outputId": "001bd2ca-5664-4ce4-8e90-5cf975e021e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CASO A - LETTURA ROBUSTA ===\n",
      "   id                descrizione                                          note\n",
      "0   1             Martello, 500g                                      tutto ok\n",
      "1   2        Cacciavite \"piatto\"                  virgolette con backslash (\")\n",
      "2   3  Set \"professionale\" 24 pz  virgolette raddoppiate nel campo descrizione\n",
      "3   4          C:\\attrezzi\\nuovo                percorso Windows con backslash\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== CASO A - LETTURA ROBUSTA ===\")\n",
    "df_ok_safe = pd.read_csv(\n",
    "    file_ok,\n",
    "    quotechar='\"',\n",
    "    escapechar='\\\\',\n",
    "    engine=\"python\",\n",
    ")\n",
    "print(df_ok_safe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eee3d8",
   "metadata": {
    "id": "d1eee3d8"
   },
   "source": [
    "We build the **correct** csv with the previous fixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c72305",
   "metadata": {
    "id": "89c72305",
    "outputId": "08d5d9b7-ce82-40ae-dff9-0f11c028333f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Scritto csv_sporco_rotto.csv\n",
      "id,descrizione,prezzo\n",
      "1,\"Martello\",12.5\n",
      "2,\"Cacciavite \\\"piatto\\\"\",8.9\n",
      "3,\"Set \"\"professionale\"\" 24 pz\",49.0\n",
      "4,\"Pinza con \"virgolette\" dentro, con virgola\",15.0\n",
      "\n",
      "\n",
      "=== CASO B - LETTURA INGENUA (DEVE FALLIRE) ===\n",
      "‚ùå Lettura ingenua fallita: Error tokenizing data. C error: Expected 3 fields in line 5, saw 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# CASO B - CSV ROTTO APPOSTA (virgolette non bilanciate)\n",
    "# ---------------------------------------------------------\n",
    "# Qui vogliamo mostrare il caso nel quale la lettura\n",
    "# ingenua NON funziona.\n",
    "# L'idea √® mettere una riga con: \"Pinza con \"virgolette\" dentro\"\n",
    "# ma SENZA escape e con una virgola dentro ‚Üí il parser C salta.\n",
    "# =========================================================\n",
    "\n",
    "file_bad = \"csv_sporco_rotto.csv\"\n",
    "\n",
    "csv_bad = (\n",
    "    'id,descrizione,prezzo\\n'                    # riga 1 (header)\n",
    "    '1,\"Martello\",12.5\\n'                        # riga 2 ok\n",
    "    '2,\"Cacciavite \\\\\"piatto\\\\\"\",8.9\\n'          # riga 3 ok (ha \\\")\n",
    "    '3,\"Set \"\"professionale\"\" 24 pz\",49.0\\n'     # riga 4 ok (ha \"\")\n",
    "    # riga 5 - QUESTA ROMPE:\n",
    "    # - campo quotato che contiene altre virgolette NON escape\n",
    "    # - e contiene anche una virgola ‚Üí il parser pensa che inizi/finisca un altro campo\n",
    "    '4,\"Pinza con \"virgolette\" dentro, con virgola\",15.0\\n'\n",
    ")\n",
    "\n",
    "Path(file_bad).write_text(csv_bad, encoding=\"utf-8\")\n",
    "print(f\"\\n‚úÖ Scritto {file_bad}\")\n",
    "print(csv_bad)\n",
    "\n",
    "\n",
    "print(\"\\n=== CASO B - LETTURA INGENUA (DEVE FALLIRE) ===\")\n",
    "try:\n",
    "    df_bad_naive = pd.read_csv(file_bad)\n",
    "    print(df_bad_naive)\n",
    "except Exception as e:\n",
    "    # qui ti aspetti qualcosa tipo:\n",
    "    # ParserError: Error tokenizing data. C error: Expected 3 fields in line 5, saw 4\n",
    "    print(\"‚ùå Lettura ingenua fallita:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d74c684",
   "metadata": {
    "id": "1d74c684"
   },
   "source": [
    "Now let‚Äôs read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43170b",
   "metadata": {
    "id": "3a43170b",
    "outputId": "c157ab6e-6e67-4867-dfdc-d00580de075c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CASO B - LETTURA ROBUSTA  ===\n",
      "   id                descrizione  prezzo\n",
      "0   1                   Martello    12.5\n",
      "1   2        Cacciavite \"piatto\"     8.9\n",
      "2   3  Set \"professionale\" 24 pz    49.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_17000\\3188488824.py:2: ParserWarning: Skipping line 5: ',' expected after '\"'\n",
      "\n",
      "  df_bad_safe = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== CASO B - LETTURA ROBUSTA  ===\")\n",
    "df_bad_safe = pd.read_csv(\n",
    "    file_bad,\n",
    "    quotechar='\"',\n",
    "    escapechar='\\\\',\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"warn\",   # oppure \"skip\" per saltare via le righe rotte\n",
    ")\n",
    "print(df_bad_safe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c73e41b",
   "metadata": {
    "id": "5c73e41b"
   },
   "source": [
    "As can be seen: no more warning ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807e8d3",
   "metadata": {
    "id": "f807e8d3"
   },
   "source": [
    "<u>First way</u>: **standard CSV** style ‚Üí **we double the quotes**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79cb36",
   "metadata": {
    "id": "ac79cb36",
    "outputId": "a1dae2af-00b8-4841-d4e6-7280f712d684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LETTURA ROBUSTA (CSV VALIDO, virgolette raddoppiate) ===\n",
      "   id                                 descrizione  prezzo\n",
      "0   1                                    Martello    12.5\n",
      "1   2                         Cacciavite \"piatto\"     8.9\n",
      "2   3                   Set \"professionale\" 24 pz    49.0\n",
      "3   4  Pinza con \"virgolette\" dentro, con virgola    15.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "file_good = \"csv_sporco_riparato_doppie.csv\"\n",
    "\n",
    "csv_good = (\n",
    "    'id,descrizione,prezzo\\n'\n",
    "    '1,\"Martello\",12.5\\n'\n",
    "    '2,\"Cacciavite \\\\\"piatto\\\\\"\",8.9\\n'\n",
    "    '3,\"Set \"\"professionale\"\" 24 pz\",49.0\\n'\n",
    "    # üëá qui √® corretto: le virgolette interne sono raddoppiate\n",
    "    '4,\"Pinza con \"\"virgolette\"\" dentro, con virgola\",15.0\\n'\n",
    ")\n",
    "\n",
    "Path(file_good).write_text(csv_good, encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n=== LETTURA ROBUSTA (CSV VALIDO, virgolette raddoppiate) ===\")\n",
    "df_ok = pd.read_csv(\n",
    "    file_good,\n",
    "    quotechar='\"',\n",
    "    escapechar='\\\\',\n",
    "    engine=\"python\",\n",
    ")\n",
    "print(df_ok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b1eedd",
   "metadata": {
    "id": "f7b1eedd"
   },
   "source": [
    "There is no longer any warning! Why?<br> Because:\n",
    "- the field is quoted `\"...\"`,\n",
    "- inside there are quotes ‚Üí we rewrote them as `\"\"`,\n",
    "- inside there is also the comma ‚Üí but since the field is quoted, the comma is ok."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b68cf89",
   "metadata": {
    "id": "8b68cf89"
   },
   "source": [
    "<u>Second way</u>: **‚Äú`escapechar`‚Äù** style ‚Üí we use the backslash `\\` before the inner quotes (if we really want to show the use of `escapechar='\\\\'`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17297188",
   "metadata": {
    "id": "17297188",
    "outputId": "d5c09e41-1f45-4564-b711-b7cd675d93ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LETTURA ROBUSTA (CSV VALIDO, escape con backslash) ===\n",
      "   id                                 descrizione  prezzo\n",
      "0   1                                    Martello    12.5\n",
      "1   2                         Cacciavite \"piatto\"     8.9\n",
      "2   3                   Set \"professionale\" 24 pz    49.0\n",
      "3   4  Pinza con \"virgolette\" dentro, con virgola    15.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "file_good2 = \"csv_sporco_riparato_escape.csv\"\n",
    "\n",
    "csv_good2 = (\n",
    "    'id,descrizione,prezzo\\n'\n",
    "    '1,\"Martello\",12.5\\n'\n",
    "    '2,\"Cacciavite \\\\\"piatto\\\\\"\",8.9\\n'\n",
    "    '3,\"Set \"\"professionale\"\" 24 pz\",49.0\\n'\n",
    "    # üëá qui ESCAPE TUTTE le virgolette interne\n",
    "    '4,\"Pinza con \\\\\"virgolette\\\\\" dentro, con virgola\",15.0\\n'\n",
    ")\n",
    "\n",
    "Path(file_good2).write_text(csv_good2, encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n=== LETTURA ROBUSTA (CSV VALIDO, escape con backslash) ===\")\n",
    "df_ok2 = pd.read_csv(\n",
    "    file_good2,\n",
    "    quotechar='\"',\n",
    "    escapechar='\\\\',   # üëà adesso serve davvero\n",
    "    engine=\"python\",\n",
    ")\n",
    "print(df_ok2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249bd460",
   "metadata": {
    "id": "249bd460"
   },
   "source": [
    "As can be seen again: no warning ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ea55dd",
   "metadata": {
    "id": "55ea55dd"
   },
   "source": [
    "**SUMMARY of point 10**:\n",
    "- ‚Äúnaive reading‚Äù: `pd.read_csv(\"file.csv\")` ‚Üí if the CSV is well‚Äëformed, it works; if it is slightly dirty, sometimes it reads; if it is really broken, it raises `ParserError`.\n",
    "- ‚Äúrobust reading‚Äù: `pd.read_csv(\"file.csv\", quotechar='\"', escapechar='\\\\', engine=\"python\")` ‚Üí it tries to read even when quotes are bad, but **warns** that **there were problems** ‚Üí and therefore gives the **warning** seen before.\n",
    "- if we do NOT want the warning ‚Üí the two reading modes seen above, plus **writing the CSV in a valid way** (double `\"\"` or escape `\\\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d418a88b",
   "metadata": {
    "id": "d418a88b"
   },
   "source": [
    "**SUMMARY of the 10 cases**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a303a02e-92b9-43e7-8b9f-39df6858cc3f",
   "metadata": {
    "id": "a303a02e-92b9-43e7-8b9f-39df6858cc3f"
   },
   "source": [
    "![](problemi_tipici_read_csv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a31fa67-aee3-4f8d-b74c-fdcd27295ea4",
   "metadata": {
    "id": "1a31fa67-aee3-4f8d-b74c-fdcd27295ea4"
   },
   "source": [
    "**Now follows a second set of examples** with **data correction**:<br>\n",
    "1Ô∏è‚É£ creation of a **‚Äúdirty‚Äù** CSV file with various **real errors and inconsistencies**;<br>\n",
    "2Ô∏è‚É£ the full Python code to read it correctly with `pandas.read_csv()`;<br>\n",
    "3Ô∏è‚É£ **<u>the Python code to clean the dataframe</u>** ‚ùó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb74fea9-7ab2-4c45-816a-2e5c326fe786",
   "metadata": {
    "id": "cb74fea9-7ab2-4c45-816a-2e5c326fe786"
   },
   "source": [
    "1Ô∏è‚É£ The file `dati_sporchi.csv`<br>\n",
    "üëâ It contains:\n",
    "- delimiter `;` instead of `,`\n",
    "- mixed encoding (accents and special characters)\n",
    "- confused decimal separators (`,`, `.`)\n",
    "- missing values or `N/A`\n",
    "- a row with inner quotes and a comma in the name\n",
    "...\n",
    "```python\n",
    "df = pd.read_csv(\"file.csv\", dtype=dtypes, usecols=cols)\n",
    "```\n",
    "Compare various versions and choose the fastest in your context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6285db6a-1d17-412c-900e-25bbad202383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T12:04:25.200361Z",
     "iopub.status.busy": "2025-10-21T12:04:25.200077Z",
     "iopub.status.idle": "2025-10-21T12:04:25.204300Z",
     "shell.execute_reply": "2025-10-21T12:04:25.203949Z",
     "shell.execute_reply.started": "2025-10-21T12:04:25.200345Z"
    },
    "id": "6285db6a-1d17-412c-900e-25bbad202383",
    "outputId": "333cdf73-4d74-4d50-d338-46297f3a4f3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File 'dati_sporchi.csv' creato.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1. CREA IL FILE CSV \"SPORCO\"\n",
    "# =========================\n",
    "\n",
    "csv_content = \"\"\"# Dati di esempio esportati da sistema legacy\n",
    "# Contengono errori di formato, encoding e separatori\n",
    "ID; Nome ; Et√† ; Data_nascita ; Stipendio ; Note\n",
    "0; \"Mario Rossi\"; 35 ; 12/05/1989 ; \"2.500,50\" ; \"Lavora a Roma, ottimo rendimento\"\n",
    "1; \"Anna Bianchi\"; 29 ; 01/09/1995 ; \"3.200,00\" ; \"Milano, nuovi progetti\"\n",
    "2; \"Jos√© √Ålvarez\"; 40 ; 15/02/1984 ; \"4.000,75\" ; \"Problemi di encoding √†√®√¨√≤√π\"\n",
    "3; \"Luigi Verdi\"; \"?\" ; 03/11/1990 ; \"2,800.00\" ; \"Errore nei separatori decimali\"\n",
    "4; \"Giulia Rossi\" ; 27 ; 31-08-1997 ; \"3.000,00\" ; \"Riga OK\"\n",
    "5; \"Paolo Bianchi\" ; 33 ; 02/04/1991 ; \"N/A\" ; \"Valore mancante stipendio\"\n",
    "6; \"Marco, Test\"; 38 ; 07/07/1986 ; \"2.900,00\" ; \"Virgola nel nome\"\n",
    "7 \"Sara Neri\" ; 31 ; 10/10/1993 ; \"3.200,00\" ; Riga con separatore mancante\n",
    "8; \"Laura Verdi\"; 25 ; 21/06/1999 ; \"3.000,00\"\n",
    "9; \"Andrea Neri\" ; ; ; ; \"Campi mancanti\"\n",
    "Unnamed: 0; \"Extra colonna inutile\"; ; ; ;\n",
    "\"\"\"\n",
    "\n",
    "with open(\"dati_sporchi.csv\", \"w\", encoding=\"latin1\") as f:\n",
    "    f.write(csv_content)\n",
    "\n",
    "print(\"‚úÖ File 'dati_sporchi.csv' creato.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b80af",
   "metadata": {
    "id": "b02b80af"
   },
   "source": [
    "‚Äî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7bc6d9-36b7-443d-9fd2-1c8485f44be5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T12:04:31.824870Z",
     "iopub.status.busy": "2025-10-21T12:04:31.824652Z",
     "iopub.status.idle": "2025-10-21T12:04:31.831028Z",
     "shell.execute_reply": "2025-10-21T12:04:31.830405Z",
     "shell.execute_reply.started": "2025-10-21T12:04:31.824854Z"
    },
    "id": "8f7bc6d9-36b7-443d-9fd2-1c8485f44be5",
    "outputId": "a36cf5be-bcfc-40f1-fc26-17b5a400371f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonne originali: ['ID', 'Nome ', 'Et√† ', 'Data_nascita ', 'Stipendio ', 'Note'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 2. LETTURA ROBUSTA\n",
    "# =========================\n",
    "df = pd.read_csv(\n",
    "    \"dati_sporchi.csv\",\n",
    "    sep=\";\",                     # separatore europeo\n",
    "    comment=\"# \",                 # ignora righe di commento\n",
    "    engine=\"python\",             # parser pi√π flessibile\n",
    "    encoding=\"latin1\",           # gestisce accenti\n",
    "    on_bad_lines=\"skip\",         # salta righe errate\n",
    "    skip_blank_lines=True,       # ignora righe vuote\n",
    "    skipinitialspace=True        # rimuove spazi dopo ;\n",
    ")\n",
    "\n",
    "print(\"Colonne originali:\", df.columns.tolist(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b471617-f89e-476b-835a-271212a655cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T12:04:39.705683Z",
     "iopub.status.busy": "2025-10-21T12:04:39.705409Z",
     "iopub.status.idle": "2025-10-21T12:04:39.713430Z",
     "shell.execute_reply": "2025-10-21T12:04:39.712972Z",
     "shell.execute_reply.started": "2025-10-21T12:04:39.705656Z"
    },
    "id": "0b471617-f89e-476b-835a-271212a655cf",
    "outputId": "0a4751d7-3952-4d59-f2fe-2fdeaea2fce6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Nome</th>\n",
       "      <th>Et√†</th>\n",
       "      <th>Data_nascita</th>\n",
       "      <th>Stipendio</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Laura Verdi</td>\n",
       "      <td>25.0</td>\n",
       "      <td>21/06/1999</td>\n",
       "      <td>3.000,00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>Extra colonna inutile</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                  Nome   Et√†  Data_nascita  Stipendio   Note\n",
       "0           8            Laura Verdi  25.0   21/06/1999    3.000,00   NaN\n",
       "1  Unnamed: 0  Extra colonna inutile   NaN           NaN        NaN   NaN"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a7a7c",
   "metadata": {
    "id": "230a7a7c"
   },
   "source": [
    "‚Äî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118730db-3a3f-4d62-8bf5-f9b1e5ab942f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T12:04:48.491769Z",
     "iopub.status.busy": "2025-10-21T12:04:48.491555Z",
     "iopub.status.idle": "2025-10-21T12:04:48.496177Z",
     "shell.execute_reply": "2025-10-21T12:04:48.495747Z",
     "shell.execute_reply.started": "2025-10-21T12:04:48.491754Z"
    },
    "id": "118730db-3a3f-4d62-8bf5-f9b1e5ab942f"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3.1 PULIZIA NOMI COLONNE\n",
    "# =========================\n",
    "df.columns = df.columns.str.strip()                          # rimuove spazi\n",
    "df.columns = df.columns.str.replace(\"√É\", \"√†\", regex=False)   # corregge accenti errati\n",
    "df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\", case=False)]  # rimuove colonne Unnamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a6018-7cb7-4435-9db7-46705faef766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T12:04:56.004391Z",
     "iopub.status.busy": "2025-10-21T12:04:56.004064Z",
     "iopub.status.idle": "2025-10-21T12:04:56.011034Z",
     "shell.execute_reply": "2025-10-21T12:04:56.010686Z",
     "shell.execute_reply.started": "2025-10-21T12:04:56.004375Z"
    },
    "id": "745a6018-7cb7-4435-9db7-46705faef766",
    "outputId": "e83cfdd0-1c2b-428e-8ffa-d3ff5e42ade6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Nome</th>\n",
       "      <th>Et√†</th>\n",
       "      <th>Data_nascita</th>\n",
       "      <th>Stipendio</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Laura Verdi</td>\n",
       "      <td>25.0</td>\n",
       "      <td>21/06/1999</td>\n",
       "      <td>3.000,00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>Extra colonna inutile</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                   Nome   Et√† Data_nascita Stipendio  Note\n",
       "0           8            Laura Verdi  25.0  21/06/1999   3.000,00   NaN\n",
       "1  Unnamed: 0  Extra colonna inutile   NaN          NaN       NaN   NaN"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4789159d-f4a0-46c6-a439-ab4ce656d8c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T12:05:45.040706Z",
     "iopub.status.busy": "2025-10-21T12:05:45.040481Z",
     "iopub.status.idle": "2025-10-21T12:05:45.055201Z",
     "shell.execute_reply": "2025-10-21T12:05:45.054688Z",
     "shell.execute_reply.started": "2025-10-21T12:05:45.040693Z"
    },
    "id": "4789159d-f4a0-46c6-a439-ab4ce656d8c5"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3.2 TRASFORMAZIONI TIPICHE\n",
    "# =========================\n",
    "\n",
    "# -- Colonna Et√†\n",
    "if \"Et√†\" in df.columns:\n",
    "    df[\"Et√†\"] = pd.to_numeric(df[\"Et√†\"], errors=\"coerce\")\n",
    "\n",
    "# -- Colonna Stipendio\n",
    "if \"Stipendio\" in df.columns:\n",
    "    df[\"Stipendio\"] = (\n",
    "        df[\"Stipendio\"]\n",
    "        .astype(str)\n",
    "        .str.replace(\".\", \"\", regex=False)  # rimuove i punti (migliaia)\n",
    "        .str.replace(\",\", \".\", regex=False) # converte virgola in punto\n",
    "    )\n",
    "    df[\"Stipendio\"] = pd.to_numeric(df[\"Stipendio\"], errors=\"coerce\")\n",
    "\n",
    "# -- Colonna Data_nascita\n",
    "if \"Data_nascita\" in df.columns:\n",
    "    df[\"Data_nascita\"] = pd.to_datetime(df[\"Data_nascita\"], dayfirst=True, errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892219e-17e2-4df9-b29c-3fde757bac0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T12:05:53.083670Z",
     "iopub.status.busy": "2025-10-21T12:05:53.083365Z",
     "iopub.status.idle": "2025-10-21T12:05:53.090487Z",
     "shell.execute_reply": "2025-10-21T12:05:53.090121Z",
     "shell.execute_reply.started": "2025-10-21T12:05:53.083652Z"
    },
    "id": "4892219e-17e2-4df9-b29c-3fde757bac0c",
    "outputId": "6fc40745-a405-4de2-9e43-61a02d2db86d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Nome</th>\n",
       "      <th>Et√†</th>\n",
       "      <th>Data_nascita</th>\n",
       "      <th>Stipendio</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Laura Verdi</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1999-06-21</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>Extra colonna inutile</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                   Nome   Et√† Data_nascita  Stipendio  Note\n",
       "0           8            Laura Verdi  25.0   1999-06-21     3000.0   NaN\n",
       "1  Unnamed: 0  Extra colonna inutile   NaN          NaT        NaN   NaN"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf1ebe6-5367-40e4-8e63-bac6b1be4704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T12:06:12.546286Z",
     "iopub.status.busy": "2025-10-21T12:06:12.546096Z",
     "iopub.status.idle": "2025-10-21T12:06:12.551472Z",
     "shell.execute_reply": "2025-10-21T12:06:12.551153Z",
     "shell.execute_reply.started": "2025-10-21T12:06:12.546272Z"
    },
    "id": "bcf1ebe6-5367-40e4-8e63-bac6b1be4704",
    "outputId": "564e153e-671b-4e2e-ed5b-da939fff589b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File caricato e pulito correttamente!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Nome</th>\n",
       "      <th>Et√†</th>\n",
       "      <th>Data_nascita</th>\n",
       "      <th>Stipendio</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Laura Verdi</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1999-06-21</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>Extra colonna inutile</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                   Nome   Et√† Data_nascita  Stipendio  Note\n",
       "0           8            Laura Verdi  25.0   1999-06-21     3000.0   NaN\n",
       "1  Unnamed: 0  Extra colonna inutile   NaN          NaT        NaN   NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tipi di dato:\n",
      " ID                      object\n",
      "Nome                    object\n",
      "Et√†                    float64\n",
      "Data_nascita    datetime64[ns]\n",
      "Stipendio              float64\n",
      "Note                   float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 3.3 RISULTATO FINALE\n",
    "# =========================\n",
    "print(\"‚úÖ File caricato e pulito correttamente!\\n\")\n",
    "display(df)\n",
    "print(\"\\nTipi di dato:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a9a845",
   "metadata": {
    "id": "75a9a845"
   },
   "source": [
    "‚Äî"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085db616",
   "metadata": {
    "id": "085db616"
   },
   "source": [
    "‚Äî"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f510b8c8-a527-4e6b-bfbd-47b128111ca6",
   "metadata": {
    "id": "f510b8c8-a527-4e6b-bfbd-47b128111ca6"
   },
   "source": [
    "# Application to the financial file `FinancialIndicators`\n",
    "The file *Credit_ISLR* is very small. Let‚Äôs use the more substantial csv file *FinancialIndicators.csv*:\n",
    "- about 7000 rows\n",
    "- 73 columns\n",
    "- about 2.4 GB\n",
    "- separator = ',' (American file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4e2971-36fe-4ace-a716-9e0a34b5118c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:06:43.677547Z",
     "iopub.status.busy": "2025-10-24T19:06:43.677309Z",
     "iopub.status.idle": "2025-10-24T19:06:43.739704Z",
     "shell.execute_reply": "2025-10-24T19:06:43.739205Z",
     "shell.execute_reply.started": "2025-10-24T19:06:43.677533Z"
    },
    "id": "ab4e2971-36fe-4ace-a716-9e0a34b5118c",
    "outputId": "450114b3-416f-4123-91b6-427fc1cd4f88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo totale di esecuzione:  0.04415559768676758\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Industry Name</th>\n",
       "      <th>SIC</th>\n",
       "      <th>Exchange</th>\n",
       "      <th>Country</th>\n",
       "      <th>Stock Price</th>\n",
       "      <th>% Chg in last year</th>\n",
       "      <th>Trading Volume</th>\n",
       "      <th># of shares outstanding</th>\n",
       "      <th>Market Cap</th>\n",
       "      <th>...</th>\n",
       "      <th>Trailing Net Income</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Intangible Assets/Total Assets</th>\n",
       "      <th>Fixed Assets/Total Assets</th>\n",
       "      <th>Market D/E</th>\n",
       "      <th>Market Debt to Capital</th>\n",
       "      <th>Book Debt to Capital</th>\n",
       "      <th>Dividend Yield</th>\n",
       "      <th>Insider Holdings</th>\n",
       "      <th>Institutional Holdings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Road Inc</td>\n",
       "      <td>Telecom. Services</td>\n",
       "      <td>4810</td>\n",
       "      <td>NDQ</td>\n",
       "      <td>US</td>\n",
       "      <td>5.23</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>236397</td>\n",
       "      <td>54.8</td>\n",
       "      <td>319.60</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-800 Contacts Inc</td>\n",
       "      <td>Medical Supplies</td>\n",
       "      <td>8060</td>\n",
       "      <td>NDQ</td>\n",
       "      <td>US</td>\n",
       "      <td>11.70</td>\n",
       "      <td>0.03</td>\n",
       "      <td>57921</td>\n",
       "      <td>13.3</td>\n",
       "      <td>151.90</td>\n",
       "      <td>...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-800-ATTORNEY Inc</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>2700</td>\n",
       "      <td>NDQ</td>\n",
       "      <td>US</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-800-FLOWERS.COM</td>\n",
       "      <td>Internet</td>\n",
       "      <td>7370</td>\n",
       "      <td>NDQ</td>\n",
       "      <td>US</td>\n",
       "      <td>6.42</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>197850</td>\n",
       "      <td>65.2</td>\n",
       "      <td>422.90</td>\n",
       "      <td>...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1mage Software Inc</td>\n",
       "      <td>Computer Software/Svcs</td>\n",
       "      <td>3579</td>\n",
       "      <td>NDQ</td>\n",
       "      <td>US</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10200</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.12</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Company Name           Industry Name   SIC Exchange Country  \\\n",
       "0           @Road Inc       Telecom. Services  4810      NDQ      US   \n",
       "1  1-800 Contacts Inc        Medical Supplies  8060      NDQ      US   \n",
       "2  1-800-ATTORNEY Inc              Publishing  2700      NDQ      US   \n",
       "3   1-800-FLOWERS.COM                Internet  7370      NDQ      US   \n",
       "4  1mage Software Inc  Computer Software/Svcs  3579      NDQ      US   \n",
       "\n",
       "   Stock Price  % Chg in last year  Trading Volume  # of shares outstanding  \\\n",
       "0         5.23               -0.02          236397                     54.8   \n",
       "1        11.70                0.03           57921                     13.3   \n",
       "2         1.01                0.00            1438                      0.0   \n",
       "3         6.42               -0.01          197850                     65.2   \n",
       "4         0.01                0.00           10200                      3.3   \n",
       "\n",
       "   Market Cap  ...  Trailing Net Income  Dividends  \\\n",
       "0      319.60  ...                 27.0        0.0   \n",
       "1      151.90  ...                  3.3        0.0   \n",
       "2        0.00  ...                 -1.0        0.0   \n",
       "3      422.90  ...                  7.8        0.0   \n",
       "4        0.03  ...                 -0.8        0.0   \n",
       "\n",
       "   Intangible Assets/Total Assets  Fixed Assets/Total Assets  Market D/E  \\\n",
       "0                            0.00                       0.02        0.00   \n",
       "1                            0.48                       0.19        0.16   \n",
       "2                             NaN                        NaN         NaN   \n",
       "3                            0.31                       0.20        0.01   \n",
       "4                            0.00                       0.00       12.12   \n",
       "\n",
       "   Market Debt to Capital  Book Debt to Capital  Dividend Yield  \\\n",
       "0                    0.00                  0.00             0.0   \n",
       "1                    0.14                  0.29             0.0   \n",
       "2                     NaN                   NaN             0.0   \n",
       "3                    0.01                  0.03             0.0   \n",
       "4                    0.92                   NaN             0.0   \n",
       "\n",
       "   Insider Holdings  Institutional Holdings  \n",
       "0               NaN                    0.23  \n",
       "1               NaN                    0.39  \n",
       "2               NaN                    0.00  \n",
       "3              0.21                    0.88  \n",
       "4               NaN                    0.00  \n",
       "\n",
       "[5 rows x 73 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "df_FI = pd.read_csv('FinancialIndicators.csv')\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print ('Tempo totale di esecuzione: ', end_time - start_time)\n",
    "\n",
    "df_FI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d497ca-5f19-479e-97b0-b3317485e133",
   "metadata": {
    "id": "21d497ca-5f19-479e-97b0-b3317485e133"
   },
   "source": [
    "We apply the above parameters to the loading of this file:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed47c7b-75b6-4196-9eee-6643b567c9f4",
   "metadata": {
    "id": "fed47c7b-75b6-4196-9eee-6643b567c9f4"
   },
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a0074-88bb-4905-bd5d-e3cc60161f54",
   "metadata": {
    "id": "425a0074-88bb-4905-bd5d-e3cc60161f54"
   },
   "source": [
    "As for the **performance of the various formats** (both storage on disk and opening/reading) see the following useful study.\n",
    "\n",
    "The key message of the study is that:\n",
    "- the CSV format is much better than Excel (not even taken into consideration), is available in all *data‚Äëmanagement* environments\n",
    "- for big data (as we will see) the best format is parquet, especially in memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609bbdc-af9b-474c-81fb-ebaae9a984d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T19:21:22.342679Z",
     "iopub.status.busy": "2025-10-24T19:21:22.342407Z",
     "iopub.status.idle": "2025-10-24T19:21:22.346551Z",
     "shell.execute_reply": "2025-10-24T19:21:22.346201Z",
     "shell.execute_reply.started": "2025-10-24T19:21:22.342665Z"
    },
    "id": "d609bbdc-af9b-474c-81fb-ebaae9a984d6",
    "outputId": "13a8c84f-15e8-4964-da92-b2833f299446"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"600\"\n",
       "            src=\"I_O Optimization in Data Projects - by Avi Chawla.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2028c82ae90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Esempio d‚Äôuso:\n",
    "show_pdf(\"I_O Optimization in Data Projects - by Avi Chawla.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89bf0e3-903b-44ee-ab4a-9259cb996949",
   "metadata": {
    "id": "e89bf0e3-903b-44ee-ab4a-9259cb996949"
   },
   "source": [
    "# The data format for big data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d931f3-454c-48ce-8d18-e69d831b36a6",
   "metadata": {
    "id": "11d931f3-454c-48ce-8d18-e69d831b36a6"
   },
   "source": [
    "Is it possible to load big data of 5M rows in *pandas*? It depends.\n",
    "\n",
    "The short answer is: yes, pandas can handle even 5 million rows, **but** it depends on what you mean by ‚Äúhandle‚Äù and on how much RAM you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b133258d-da10-4f09-b750-a16307861065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:48:10.456975Z",
     "iopub.status.busy": "2025-10-24T18:48:10.456716Z",
     "iopub.status.idle": "2025-10-24T18:48:10.460041Z",
     "shell.execute_reply": "2025-10-24T18:48:10.459542Z",
     "shell.execute_reply.started": "2025-10-24T18:48:10.456959Z"
    },
    "id": "b133258d-da10-4f09-b750-a16307861065"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9d90ee-e5ff-40ff-bc57-722befe5677c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:48:11.173601Z",
     "iopub.status.busy": "2025-10-24T18:48:11.173406Z",
     "iopub.status.idle": "2025-10-24T18:48:11.177957Z",
     "shell.execute_reply": "2025-10-24T18:48:11.177335Z",
     "shell.execute_reply.started": "2025-10-24T18:48:11.173587Z"
    },
    "id": "1e9d90ee-e5ff-40ff-bc57-722befe5677c"
   },
   "outputs": [],
   "source": [
    "# Il prefisso r dice a Python di non interpretare \\ come escape.\n",
    "path = r'C:\\Users\\Utente\\Desktop\\salvataggi\\SALVATAGGIO DATI\\Documents\\Seminari\\Data Science (corsi)\\Corso Python base\\linkage\\file_csv'\n",
    "\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531d132-3f6b-412d-befc-378ef601c974",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:48:18.240362Z",
     "iopub.status.busy": "2025-10-24T18:48:18.240149Z",
     "iopub.status.idle": "2025-10-24T18:48:21.341634Z",
     "shell.execute_reply": "2025-10-24T18:48:21.341067Z",
     "shell.execute_reply.started": "2025-10-24T18:48:18.240346Z"
    },
    "id": "b531d132-3f6b-412d-befc-378ef601c974"
   },
   "outputs": [],
   "source": [
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eeada6-6166-4d45-a67a-cceb0c92f38d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:48:28.789133Z",
     "iopub.status.busy": "2025-10-24T18:48:28.788903Z",
     "iopub.status.idle": "2025-10-24T18:48:28.793928Z",
     "shell.execute_reply": "2025-10-24T18:48:28.793345Z",
     "shell.execute_reply.started": "2025-10-24T18:48:28.789116Z"
    },
    "id": "e7eeada6-6166-4d45-a67a-cceb0c92f38d",
    "outputId": "277c7c34-d5c1-4f0f-a659-d2e88d62e6cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5749132, 12)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c22ea-e5cf-49e8-a777-4b1d213c12b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:48:29.183826Z",
     "iopub.status.busy": "2025-10-24T18:48:29.183572Z",
     "iopub.status.idle": "2025-10-24T18:48:29.202116Z",
     "shell.execute_reply": "2025-10-24T18:48:29.201677Z",
     "shell.execute_reply.started": "2025-10-24T18:48:29.183810Z"
    },
    "id": "995c22ea-e5cf-49e8-a777-4b1d213c12b1",
    "outputId": "af1a84aa-0606-4ea6-f65f-db5acc1ea9c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>cmp_fname_c1</th>\n",
       "      <th>cmp_fname_c2</th>\n",
       "      <th>cmp_lname_c1</th>\n",
       "      <th>cmp_lname_c2</th>\n",
       "      <th>cmp_sex</th>\n",
       "      <th>cmp_bd</th>\n",
       "      <th>cmp_bm</th>\n",
       "      <th>cmp_by</th>\n",
       "      <th>cmp_plz</th>\n",
       "      <th>is_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37291</td>\n",
       "      <td>53113</td>\n",
       "      <td>0.833333333333333</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39086</td>\n",
       "      <td>47614</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70031</td>\n",
       "      <td>70237</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84795</td>\n",
       "      <td>97439</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36950</td>\n",
       "      <td>42116</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_1   id_2       cmp_fname_c1 cmp_fname_c2  cmp_lname_c1 cmp_lname_c2  \\\n",
       "0  37291  53113  0.833333333333333            ?           1.0            ?   \n",
       "1  39086  47614                  1            ?           1.0            ?   \n",
       "2  70031  70237                  1            ?           1.0            ?   \n",
       "3  84795  97439                  1            ?           1.0            ?   \n",
       "4  36950  42116                  1            ?           1.0            1   \n",
       "\n",
       "   cmp_sex cmp_bd cmp_bm cmp_by cmp_plz  is_match  \n",
       "0        1      1      1      1       0      True  \n",
       "1        1      1      1      1       1      True  \n",
       "2        1      1      1      1       1      True  \n",
       "3        1      1      1      1       1      True  \n",
       "4        1      1      1      1       1      True  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd49ea-916b-4b5c-a3cb-16cd72aebf25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:48:29.571961Z",
     "iopub.status.busy": "2025-10-24T18:48:29.571744Z",
     "iopub.status.idle": "2025-10-24T18:48:29.580914Z",
     "shell.execute_reply": "2025-10-24T18:48:29.580336Z",
     "shell.execute_reply.started": "2025-10-24T18:48:29.571945Z"
    },
    "id": "59cd49ea-916b-4b5c-a3cb-16cd72aebf25",
    "outputId": "523b34f3-f0fe-4173-ef10-1b336157609a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>cmp_fname_c1</th>\n",
       "      <th>cmp_fname_c2</th>\n",
       "      <th>cmp_lname_c1</th>\n",
       "      <th>cmp_lname_c2</th>\n",
       "      <th>cmp_sex</th>\n",
       "      <th>cmp_bd</th>\n",
       "      <th>cmp_bm</th>\n",
       "      <th>cmp_by</th>\n",
       "      <th>cmp_plz</th>\n",
       "      <th>is_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5749127</th>\n",
       "      <td>47892</td>\n",
       "      <td>98941</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749128</th>\n",
       "      <td>53346</td>\n",
       "      <td>74894</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749129</th>\n",
       "      <td>18058</td>\n",
       "      <td>99971</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749130</th>\n",
       "      <td>84934</td>\n",
       "      <td>95688</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749131</th>\n",
       "      <td>20985</td>\n",
       "      <td>57829</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id_1   id_2 cmp_fname_c1 cmp_fname_c2  cmp_lname_c1 cmp_lname_c2  \\\n",
       "5749127  47892  98941            1            ?      0.166667            ?   \n",
       "5749128  53346  74894            1            ?      0.222222            ?   \n",
       "5749129  18058  99971            0            ?      1.000000            ?   \n",
       "5749130  84934  95688            1            ?      0.000000            ?   \n",
       "5749131  20985  57829            1            1      0.000000            ?   \n",
       "\n",
       "         cmp_sex cmp_bd cmp_bm cmp_by cmp_plz  is_match  \n",
       "5749127        1      0      0      1       0     False  \n",
       "5749128        1      0      0      1       0     False  \n",
       "5749129        1      0      0      0       0     False  \n",
       "5749130        1      0      1      0       0     False  \n",
       "5749131        1      0      1      1       0     False  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fdc05b-cd85-45cd-ba2e-2539460ca8ae",
   "metadata": {
    "id": "74fdc05b-cd85-45cd-ba2e-2539460ca8ae"
   },
   "source": [
    "Activation of the execution environment.\n",
    "The notebook works **indifferently** both on Jupyter Notebook/Lab and on Google Colab, as said, <u>apart from two aspects</u>:\n",
    "- loading datasets into the notebook\n",
    "- including *png* images in individual cells\n",
    "\n",
    "It is therefore useful to **determine the execution environment**, setting the binary variable `IN_COLAB` to `True` if we are in Google Colab, to `False` if we are in Jupyter Notebook).\n",
    "\n",
    "Those two operations will be executed differently depending on the value of the binary variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67f3b2d-f650-497d-a2d3-d473ba7ba953",
   "metadata": {
    "id": "a67f3b2d-f650-497d-a2d3-d473ba7ba953"
   },
   "source": [
    "üß† **2. Operazioni che pandas gestisce bene anche con 5M di righe**\n",
    "\n",
    "Con hardware \"decente\" (CPU moderna, 16 GB RAM) pandas gestisce tranquillamente:\n",
    "\n",
    "‚úÖ **Caricamento CSV**\n",
    "```python\n",
    "df = pd.read_csv(\"dati.csv\")\n",
    "```\n",
    "\n",
    "Si pu√≤ anche usare:\n",
    "- `dtype=` per tipizzare meglio le colonne (meno RAM);\n",
    "- `usecols=` per leggere solo alcune colonne;\n",
    "- `chunksize=` per leggere a blocchi.\n",
    "\n",
    "‚úÖ **Operazioni elementari e aggregazioni**\n",
    "- `df.describe()`, `df.mean()`, `df.groupby(\"col\").agg(...)`\n",
    "- `df.sort_values(\"col\")`\n",
    "- `df.query(\"x > 10 and y < 5\")`\n",
    "- `df.sample(100_000)`<br>\n",
    "tutte fattibili.\n",
    "\n",
    "‚úÖ **Join e merge moderati**<br>\n",
    "Fino a qualche milione di righe per tabella:\n",
    "```python\n",
    "pd.merge(df1, df2, on=\"id\", how=\"inner\")\n",
    "```\n",
    "funziona, ma attenzione ai picchi di memoria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d718ddf-6d75-48f4-86a2-8c2ed09d83d0",
   "metadata": {
    "id": "2d718ddf-6d75-48f4-86a2-8c2ed09d83d0"
   },
   "source": [
    "---\n",
    "üö´ **Operazioni che iniziano a diventare problematiche**\n",
    "\n",
    "Quando il dataset supera **i 5‚Äì10 milioni di righe o supera i 5 GB in RAM**, ecco cosa rallenta o esplode:\n",
    "\n",
    "‚ùå **ordinamenti multipli o sort complessi**\n",
    "```pythoon\n",
    "df.sort_values([\"col1\", \"col2\"])\n",
    "```\n",
    "Crea una copia in memoria grande quanto il DataFrame stesso.\n",
    "\n",
    "‚ùå **merge / join molto grandi**<br>\n",
    "se le due tabelle insieme superano la RAM disponibile.\n",
    "\n",
    "‚ùå **apply / lambda riga per riga**\n",
    "```python\n",
    "df.apply(lambda row: f(row.x), axis=1)\n",
    "```\n",
    "\n",
    "Molto lente: infatti sono eseguite in Python puro, non in C.<br>\n",
    "Meglio usare funzioni **vectorized** (`np.where`, `pd.Series.map`, ecc.).\n",
    "\n",
    "‚ùå **operazioni iterative**<br>\n",
    "Cicli `for row in df.itertuples()` su milioni di righe ‚Üí un disastro!\n",
    "\n",
    "‚ùå **Scrittura su CSV/parquet**\n",
    "```python\n",
    "df.to_csv(\"file.csv\")\n",
    "```\n",
    "\n",
    "Poco efficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cbedac-bee5-4376-9f2d-50b5df007d8f",
   "metadata": {
    "id": "a6cbedac-bee5-4376-9f2d-50b5df007d8f"
   },
   "source": [
    "---\n",
    "‚ö° **Alternative e strategie**\n",
    "\n",
    "**1. Usare il *chunking***\n",
    "\n",
    "Il seguente codice √® **rischioso**\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dati.csv\")\n",
    "df[\"media\"] = df[\"valore\"].mean()\n",
    "\n",
    "```\n",
    "\n",
    "Meglio leggere a blocchi e processare iterativamente:\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "chunksize = 500.000   # legge 500 mila righe per volta\n",
    "risultati = []        # lista dove accumulare i risultati\n",
    "\n",
    "for chunk in pd.read_csv(\"dati.csv\", chunksize=chunksize):\n",
    "    media_chunk = chunk[\"valore\"].mean()       # calcolo sulla parte letta\n",
    "    risultati.append(media_chunk)              # salvo il risultato parziale\n",
    "\n",
    "# dopo il ciclo puoi combinare i risultati\n",
    "media_totale = sum(risultati) / len(risultati)\n",
    "print(\"Media complessiva:\", media_totale)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "**2. Usare il formato dati *parquet***<br>\n",
    "Vedi il prossimo capitolo.\n",
    "\n",
    "**3. Usare `cuDF`**<br>\n",
    "Utilizza la GPU senza modifiche al codice Pandas\n",
    "\n",
    "**4. Usare `Spark`**<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660e3a2f-64c1-47f9-bc7c-7c9dbfc9bc41",
   "metadata": {
    "id": "660e3a2f-64c1-47f9-bc7c-7c9dbfc9bc41"
   },
   "source": [
    "# Un file CSV molto, molto grande\n",
    "Riusciamo a caricare un file CSV come [questo](https://www.kaggle.com/datasets/aadimator/nyc-realtime-traffic-speed-data/data)? √® quasi 30GB.<br>\n",
    "*Download* --> *Download dataset as zip (10 GBs)*.<br>\n",
    "Il suo nome √® **DOT_Traffic_Speeds_NBE.csv** ed √® relativo al traffico nella citt√† di NewYorl.\n",
    "\n",
    "Vediamone il significato:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a88614f-4843-4510-b051-561d2d866fff",
   "metadata": {
    "id": "1a88614f-4843-4510-b051-561d2d866fff"
   },
   "source": [
    "---\n",
    "That Kaggle dataset is an **export** of NYC DOT‚Äôs **real-time traffic speed feed** (‚ÄúDOT Traffic Speeds NBE‚Äù).\n",
    "\n",
    "Each row is a **timestamped observation for one road segment (a ‚Äúlink‚Äù)** with the average **speed** and **travel time** between the segment‚Äôs start and end points. It‚Äôs maintained by NYC DOT and mirrored to Kaggle. ([Kaggle][1])\n",
    "\n",
    "Here‚Äôs what the **fields mean** (names may appear in UPPER_CASE on Kaggle):\n",
    "\n",
    "* **ID / LINK_ID**\n",
    "  Unique identifier of the road **segment** (link) from TRANSCOM (regional traffic consortium). `LINK_ID` is the same as `ID`. Use this as **the key to group or join**. ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **SPEED**\n",
    "  **Average speed (mph)** vehicles traveled **across the whole segment** during the most recent interval. It‚Äôs not spot speed at a point‚Äîthink ‚Äúsegment travel speed.‚Äù Expect missing or zero values at times. ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **TRAVEL_TIME**\n",
    "  **Seconds** the average vehicle took to traverse the segment in that interval. Roughly `TRAVEL_TIME ‚âà segment_length / SPEED` (after converting units). Useful to derive segment length if you have a stable speed sample. ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **STATUS**\n",
    "  Marked as an **artifact / not useful** in NYC DOT‚Äôs own metadata. Most people ignore it. ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **DATA_AS_OF** (a.k.a. `DataAsOf`)\n",
    "  **Timestamp** when data for that link was last received. The feed updates **every few minutes**. Timezone is local (Eastern). Use this for time-series work and resampling. ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **LINK_POINTS**\n",
    "  **Plaintext sequence of lat/long pairs** describing the link geometry (start‚Üíend polyline). **Caveat:** some values are **truncated**‚Äîdon‚Äôt rely on this alone for precise mapping. ([Medium][3])\n",
    "\n",
    "* **ENCODED_POLY_LINE**\n",
    "  **Google-encoded polyline** version of the same geometry. This is usually the better field to decode for maps. (See Google‚Äôs polyline spec referenced by DOT.) ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **ENCODED_POLY_LINE_LVLS**\n",
    "  **Polyline ‚Äúlevels‚Äù** for Google‚Äôs legacy rendering (zoom levels). Often unused in modern tooling but included for completeness. ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **OWNER**\n",
    "  Owner of the detector producing this link‚Äôs data (administrative/operational). ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **TRANSCOM_ID / TRANSCOM_ID (artifact)**\n",
    "  Marked **not useful** by the publisher (redundant with ID). ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **BOROUGH**\n",
    "  NYC borough name (**Brooklyn, Bronx, Manhattan, Queens, Staten Island**). It can be blank for some links. Handy for rollups and filtering. ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **LINK_NAME / DESCRIPTION**\n",
    "  Human-readable description of the segment (e.g., ‚ÄúBQE N Atlantic Ave ‚Äî BKN Bridge Manhattan Side‚Äù). Note: **links are one-way**, and not every corridor has both directions in the feed. ([Medium][3])\n",
    "\n",
    "### How to interpret the dataset (what a ‚Äúrow‚Äù is)\n",
    "\n",
    "* One **segment (link)** √ó one **timestamp** ‚Üí **avg speed & travel time** for vehicles that **completed** that segment in the interval. It‚Äôs not per-vehicle data; it‚Äôs an **aggregate**. ([Medium][3])\n",
    "* The feed is **real-time / near-real-time**, updated several times per minute, and covers **major arterials & highways** in NYC. ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "### Practical notes / gotchas\n",
    "\n",
    "* **Geometry:** Prefer **`ENCODED_POLY_LINE`** over `LINK_POINTS`; the latter can be cut off. ([Medium][3])\n",
    "* **Aggregation grain:** Links are **directional**; do not assume two-way coverage for a corridor. ([Medium][3])\n",
    "* **Units:** SPEED = mph, TRAVEL_TIME = seconds; `BOROUGH` is a label, not a geometry. ([Sito Ufficiale di New York City][2])\n",
    "* **Quality:** Occasional zeros/missing values; treat **STATUS** as ignorable. ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "### Typical uses\n",
    "\n",
    "* Compute **p50/p90 speeds** by `BOROUGH`/`LINK_ID`/hour; detect slowdowns and incidents.\n",
    "* Map segments by decoding **`ENCODED_POLY_LINE`**; join with borough boundaries for choropleths.\n",
    "* Derive **segment length** via `median(SPEED)*median(TRAVEL_TIME)` (unit-converted) if length isn‚Äôt separately available.\n",
    "\n",
    "[1]: https://www.kaggle.com/datasets/aadimator/nyc-realtime-traffic-speed-data?utm_source=chatgpt.com \"NYC Real-Time Traffic Speed Data\"\n",
    "[2]: https://www.nyc.gov/html/dot/downloads/pdf/metadata-trafficspeeds.pdf?utm_source=chatgpt.com \"Traffic Sensors Metadata What does this data set describe? ...\"\n",
    "[3]: https://medium.com/qri-io/new-qri-dataset-s-nyc-real-time-traffic-speeds-c3e4c88f44be \"New Qri Dataset(s): NYC Real-Time Traffic Speeds | by Chris Whong | qri.io | Medium\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0514b618",
   "metadata": {
    "id": "0514b618"
   },
   "source": [
    "**NOTE su questa dimensione (circa 30 GB)**\n",
    "\n",
    "28 GB ‚âà 28.000.000.000 byte ‚âà 26 GiB (se lo guardiamo in termini ‚Äúinformatici‚Äù).\n",
    "\n",
    "Un file CSV √® testuale, quindi √® poco denso: gli stessi dati in **Parquet** starebbero spesso in **3‚Äì6 GB**.\n",
    "\n",
    "In RAM questo file, <u>se letto con *pandas*</u>, **occupa ben pi√π spazio di 28 GB**: pandas infatti deve:\n",
    "- leggere il testo,\n",
    "- fare il parsing,\n",
    "- creare gli array interni.\n",
    "\n",
    "Risultato: 28 GB di CSV con la lettura *pandas* possono diventare **50‚Äì80 GB di RAM** senza sforzarsi troppo (dipende da quante colonne stringa ci sono, da quanti NaN, da quanto sono lunghe le etichette, ecc.).\n",
    "\n",
    "**√à frequente in azienda una simile dimensione?**\n",
    "- un singolo CSV da 28 GB non √® la norma nei gestionali/contabilit√†/HR. In questi sistemi troviamo pi√π facilmente **50‚Äì500 MB, massimo 2‚Äì3 GB** quando fanno l‚Äôexport ‚Äúdi tutto‚Äù.\n",
    "- √® per√≤ normalissimo in contesti tipo:\n",
    "    - log applicativi / web / sicurezza,\n",
    "    - telco,\n",
    "    - mobility / trasporti (tipo il tuo caso),\n",
    "    - IoT,\n",
    "    - data lake ‚Äúbuttato gi√π‚Äù da un sistema legacy.\n",
    "\n",
    "Ma‚Ä¶ quasi mai le aziende vogliono avere un unico CSV da 28 GB. Di solito √® un ‚Äúdumpone‚Äù fatto cos√¨ perch√© ‚Äúera l‚Äôopzione di export‚Äù, oppure perch√© qualcuno ha fatto SELECT * su 3 anni e l‚Äôha mandato su S3. In produzione seria si spezza per data o per partizione e si va con il Parquet.\n",
    "\n",
    "**Quindi: non √® strano avere 28 GB di dati. √à un po‚Äô strano averli tutti in un solo CSV.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vgs_xWascu9u",
   "metadata": {
    "id": "Vgs_xWascu9u"
   },
   "source": [
    "## Determinazione dell'ambiente di esecuzione.\n",
    "Il notebook funziona **indifferentemente** sia su Jupyter Notebook / Visual Studio Code che su Google Colab, come detto, <u>a parte due aspetti</u>:\n",
    "- il caricamento dei dataset nel notebook\n",
    "- l'inclusione delle immagini *png* nelle singole celle\n",
    "\n",
    "E' quindi utile **determinare l'ambiente di esecuzione**, impostando una variabile binaria (a `True` se siamo in Google Colab, a `False` se siamo in Jupyter Notebook).\n",
    "\n",
    "Le due operazioni suddette saranno eseguite in modo differente a seconda del valore della variabile binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fVONjPrKcxTl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVONjPrKcxTl",
    "outputId": "32789101-9cc6-49c1-9c71-de84f3de4982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Colab: False\n"
     ]
    }
   ],
   "source": [
    "# impostazione del TOGGLE BINARIO:\n",
    "try:\n",
    "    import google.colab                      # package disponibile SOLO in Google Colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "print(\"Running on Colab:\", IN_COLAB)\n",
    "\n",
    "\n",
    "# IMPORT dei package necessari (necessari sia in JN che in Colab):\n",
    "from IPython.display import Image, display   # import of embed and image‚Äëdisplay packages (one‚Äëtime)\n",
    "                                             # Image and display are both needed in Jupyter Notebook\n",
    "                                             # Google Colab uses only Image\n",
    "import os                                    # needed in Google Colab to see from a code cell\n",
    "                                             # the contents of 'content'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H_CJX_iEiFOW",
   "metadata": {
    "id": "H_CJX_iEiFOW"
   },
   "source": [
    "## Loading the BIG csv file with Google Colab Pro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XZ1iJB7li9I-",
   "metadata": {
    "id": "XZ1iJB7li9I-"
   },
   "source": [
    "How much space do we have in the *session storage* of the VM? (with an L4‚ÄëGPU with 53GB of RAM, 22.5 GB of VRAM and 235.7 GB of disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fD1FoVI-h98u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fD1FoVI-h98u",
    "outputId": "834e970e-ce63-470e-f677-1d845ac790c8"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fKGa5X3zir9F",
   "metadata": {
    "id": "fKGa5X3zir9F"
   },
   "source": [
    "`overlay 236G 40G 197G 17% /`: this is **the root** of the Colab environment, i.e. what in Colab we see under `/content`.<br>\n",
    "What really matters is **Avail = 197G**.<br>\n",
    "\n",
    "Translated: we can create new files up to about 197 GB (then obviously it depends also on how much you use for notebooks, parquets, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PgNseeIPkMW-",
   "metadata": {
    "id": "PgNseeIPkMW-"
   },
   "source": [
    "And the other lines (`/dev/root`, `tmpfs`, `/dev/shm‚Ä¶`) of the previous output?\n",
    "Those are things of the Colab container.\n",
    "- `/dev/shm 26G` ‚Üí is the shared memory (useful for multiprocessing, for example, but not to save 28 GB).\n",
    "- `tmpfs 27G` ‚Üí temporary memories in RAM.\n",
    "...\n",
    "```python\n",
    "‚ö†Ô∏è rightly we added the comment: ‚Äúdo it only if it fits in VRAM‚Äù.<br>\n",
    "This is the part that often, on huge datasets, is not done, and you stop at saving by chunk.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YbLH32-6kjUu",
   "metadata": {
    "id": "YbLH32-6kjUu"
   },
   "source": [
    "Abbiamo quindi tantissimo spazio locale nella VM: **circa 197 GB liberi** ‚Üí quindi s√¨, un file da 28 GB ci sta tranquillamente.\n",
    "\n",
    "Tuttavia l'upload di un file cos√¨ grande nela *session storage* di Google colab √® lento e a rischio di failure. Molto meglio mettere il file su Google Drive e poi **montare il disco** (autorizzando la connessione Google con i soliti passi):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pnX91OR0fjFt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pnX91OR0fjFt",
    "outputId": "83c3c029-62cd-4a6b-f65c-f66e404872a2"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\", force_remount=True)  # the argument 'force_remount = True' allows multiple mounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FC3d8Vedf0l_",
   "metadata": {
    "id": "FC3d8Vedf0l_"
   },
   "source": [
    "Cos√¨ il file resta su Drive, non lo dobbiamo ‚Äúcaricare‚Äù nella sessione, lo leggiamo da l√¨ (`/content/drive/MyDrive/.../big.csv`), Colab non deve tenere 28 GB sul disco locale.\n",
    "\n",
    "‚ö†Ô∏è Attenzione: leggere 28 GB da Drive √® pi√π lento che leggere da disco locale. Per un CSV enorme pu√≤ voler dire **minuti di I/O**.\n",
    "\n",
    "NB. `drive/MyDrive` √® ora **disponibile anche sotto la `content` del *session storage***.\n",
    "\n",
    "Possiamo infatti vederlo rieseguendo il comando `!df -h`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "q3WskQszAurS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3WskQszAurS",
    "outputId": "e52fa047-e170-42d8-aff5-52b64d524aaa"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MCKpRa5IAdVw",
   "metadata": {
    "id": "MCKpRa5IAdVw"
   },
   "source": [
    "<u>Domanda</u>: ma ‚Äúoverlay‚Äù e ‚Äúdrive‚Äù hanno la stessa dimensione (236G) ü§î?\n",
    "\n",
    "S√¨, sembra cos√¨ perch√© Colab spesso **mostra lo stesso backing storage o comunque due volumi con taglia simile**. Quello che ci interessa √®: abbiamo ~200 GB liberi localmente e ~187 GB liberi su Drive ‚Üí entrambi > 28 GB ‚Üí siamo al sicuro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XiFlPeUdgVJW",
   "metadata": {
    "id": "XiFlPeUdgVJW"
   },
   "source": [
    "First of all, as a check, let‚Äôs **list the files on the drive**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "xIIb3zi6gRi2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIIb3zi6gRi2",
    "outputId": "819e67f8-10fd-4578-fe53-707e22267377"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    import os\n",
    "    base = \"/content/drive/MyDrive\"\n",
    "\n",
    "    for name in os.listdir(base):\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zv35IUn_ghq1",
   "metadata": {
    "id": "zv35IUn_ghq1"
   },
   "source": [
    "If we also want to see **the size** of the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "NERI4Rj0gmlV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NERI4Rj0gmlV",
    "outputId": "c5b2a464-e7c0-42a6-c6da-e0ba1d5ee3cc"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    for name in os.listdir(base):\n",
    "        path = os.path.join(base, name)\n",
    "        if os.path.isfile(path):\n",
    "            print(\"FILE \", name, os.path.getsize(path))\n",
    "        else:\n",
    "            print(\"DIR  \", name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K3N3gcTyrW9F",
   "metadata": {
    "id": "K3N3gcTyrW9F"
   },
   "source": [
    "Let‚Äôs check the size of the file `DOT_Traffic_Speeds_NBE.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "-oUOMAR5rI2-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-oUOMAR5rI2-",
    "outputId": "3c264c3c-6a21-4beb-9a97-6bf245a93d8e"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !ls -lh /content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eOGXJx70ByxB",
   "metadata": {
    "id": "eOGXJx70ByxB"
   },
   "source": [
    "Now we are ready to read the csv file with pandas (`pd.read_csv`) with **cuDF**.\n",
    "\n",
    "---\n",
    "\n",
    "**`cudf` is a version of pandas with CUDA acceleration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "qv0WrQY1DR1B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qv0WrQY1DR1B",
    "outputId": "dc031cd0-1dba-42b9-9074-9c3cbbb9a58a"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    %load_ext cudf.pandas\n",
    "    import pandas as pd\n",
    "    import cudf\n",
    "else:\n",
    "    import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r5z8pQe6h2BF",
   "metadata": {
    "id": "r5z8pQe6h2BF"
   },
   "source": [
    "‚Äî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "udWpoiyAdyyV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "udWpoiyAdyyV",
    "outputId": "1b8a54a9-1976-4aef-f71c-e27939d42970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pandas] letto chunk 0 con 250000 righe\n",
      "[cuDF] chunk 0 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 1 con 250000 righe\n",
      "[cuDF] chunk 1 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 2 con 250000 righe\n",
      "[cuDF] chunk 2 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 3 con 250000 righe\n",
      "[cuDF] chunk 3 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 4 con 250000 righe\n",
      "[cuDF] chunk 4 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 5 con 250000 righe\n",
      "[cuDF] chunk 5 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 6 con 250000 righe\n",
      "[cuDF] chunk 6 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 7 con 250000 righe\n",
      "[cuDF] chunk 7 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 8 con 250000 righe\n",
      "[cuDF] chunk 8 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 9 con 250000 righe\n",
      "[cuDF] chunk 9 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 10 con 250000 righe\n",
      "[cuDF] chunk 10 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 11 con 250000 righe\n",
      "[cuDF] chunk 11 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 12 con 250000 righe\n",
      "[cuDF] chunk 12 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 13 con 250000 righe\n",
      "[cuDF] chunk 13 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 14 con 250000 righe\n",
      "[cuDF] chunk 14 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 15 con 250000 righe\n",
      "[cuDF] chunk 15 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 16 con 250000 righe\n",
      "[cuDF] chunk 16 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 17 con 250000 righe\n",
      "[cuDF] chunk 17 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 18 con 250000 righe\n",
      "[cuDF] chunk 18 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 19 con 250000 righe\n",
      "[cuDF] chunk 19 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 20 con 250000 righe\n",
      "[cuDF] chunk 20 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 21 con 250000 righe\n",
      "[cuDF] chunk 21 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 22 con 250000 righe\n",
      "[cuDF] chunk 22 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 23 con 250000 righe\n",
      "[cuDF] chunk 23 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 24 con 250000 righe\n",
      "[cuDF] chunk 24 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 25 con 250000 righe\n",
      "[cuDF] chunk 25 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 26 con 250000 righe\n",
      "[cuDF] chunk 26 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 27 con 250000 righe\n",
      "[cuDF] chunk 27 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 28 con 250000 righe\n",
      "[cuDF] chunk 28 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 29 con 250000 righe\n",
      "[cuDF] chunk 29 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 30 con 250000 righe\n",
      "[cuDF] chunk 30 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 31 con 250000 righe\n",
      "[cuDF] chunk 31 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 32 con 250000 righe\n",
      "[cuDF] chunk 32 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 33 con 250000 righe\n",
      "[cuDF] chunk 33 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 34 con 250000 righe\n",
      "[cuDF] chunk 34 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 35 con 250000 righe\n",
      "[cuDF] chunk 35 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 36 con 250000 righe\n",
      "[cuDF] chunk 36 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 37 con 250000 righe\n",
      "[cuDF] chunk 37 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 38 con 250000 righe\n",
      "[cuDF] chunk 38 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 39 con 250000 righe\n",
      "[cuDF] chunk 39 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 40 con 250000 righe\n",
      "[cuDF] chunk 40 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 41 con 250000 righe\n",
      "[cuDF] chunk 41 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 42 con 250000 righe\n",
      "[cuDF] chunk 42 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 43 con 250000 righe\n",
      "[cuDF] chunk 43 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 44 con 250000 righe\n",
      "[cuDF] chunk 44 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 45 con 250000 righe\n",
      "[cuDF] chunk 45 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 46 con 250000 righe\n",
      "[cuDF] chunk 46 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 47 con 250000 righe\n",
      "[cuDF] chunk 47 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 48 con 250000 righe\n",
      "[cuDF] chunk 48 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 49 con 250000 righe\n",
      "[cuDF] chunk 49 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 50 con 250000 righe\n",
      "[cuDF] chunk 50 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 51 con 250000 righe\n",
      "[cuDF] chunk 51 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 52 con 250000 righe\n",
      "[cuDF] chunk 52 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 53 con 250000 righe\n",
      "[cuDF] chunk 53 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 54 con 250000 righe\n",
      "[cuDF] chunk 54 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 55 con 250000 righe\n",
      "[cuDF] chunk 55 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 56 con 250000 righe\n",
      "[cuDF] chunk 56 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 57 con 250000 righe\n",
      "[cuDF] chunk 57 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 58 con 250000 righe\n",
      "[cuDF] chunk 58 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 59 con 250000 righe\n",
      "[cuDF] chunk 59 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 60 con 250000 righe\n",
      "[cuDF] chunk 60 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 61 con 250000 righe\n",
      "[cuDF] chunk 61 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 62 con 250000 righe\n",
      "[cuDF] chunk 62 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 63 con 250000 righe\n",
      "[cuDF] chunk 63 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 64 con 250000 righe\n",
      "[cuDF] chunk 64 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 65 con 250000 righe\n",
      "[cuDF] chunk 65 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 66 con 250000 righe\n",
      "[cuDF] chunk 66 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 67 con 250000 righe\n",
      "[cuDF] chunk 67 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 68 con 250000 righe\n",
      "[cuDF] chunk 68 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 69 con 250000 righe\n",
      "[cuDF] chunk 69 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 70 con 250000 righe\n",
      "[cuDF] chunk 70 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 71 con 250000 righe\n",
      "[cuDF] chunk 71 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 72 con 250000 righe\n",
      "[cuDF] chunk 72 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 73 con 250000 righe\n",
      "[cuDF] chunk 73 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 74 con 250000 righe\n",
      "[cuDF] chunk 74 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 75 con 250000 righe\n",
      "[cuDF] chunk 75 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 76 con 250000 righe\n",
      "[cuDF] chunk 76 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 77 con 250000 righe\n",
      "[cuDF] chunk 77 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 78 con 250000 righe\n",
      "[cuDF] chunk 78 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 79 con 250000 righe\n",
      "[cuDF] chunk 79 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 80 con 250000 righe\n",
      "[cuDF] chunk 80 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 81 con 250000 righe\n",
      "[cuDF] chunk 81 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 82 con 250000 righe\n",
      "[cuDF] chunk 82 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 83 con 250000 righe\n",
      "[cuDF] chunk 83 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 84 con 250000 righe\n",
      "[cuDF] chunk 84 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 85 con 250000 righe\n",
      "[cuDF] chunk 85 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 86 con 250000 righe\n",
      "[cuDF] chunk 86 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 87 con 250000 righe\n",
      "[cuDF] chunk 87 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 88 con 250000 righe\n",
      "[cuDF] chunk 88 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 89 con 250000 righe\n",
      "[cuDF] chunk 89 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 90 con 250000 righe\n",
      "[cuDF] chunk 90 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 91 con 250000 righe\n",
      "[cuDF] chunk 91 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 92 con 250000 righe\n",
      "[cuDF] chunk 92 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 93 con 250000 righe\n",
      "[cuDF] chunk 93 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 94 con 250000 righe\n",
      "[cuDF] chunk 94 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 95 con 250000 righe\n",
      "[cuDF] chunk 95 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 96 con 250000 righe\n",
      "[cuDF] chunk 96 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 97 con 250000 righe\n",
      "[cuDF] chunk 97 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 98 con 250000 righe\n",
      "[cuDF] chunk 98 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 99 con 250000 righe\n",
      "[cuDF] chunk 99 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 100 con 250000 righe\n",
      "[cuDF] chunk 100 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 101 con 250000 righe\n",
      "[cuDF] chunk 101 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 102 con 250000 righe\n",
      "[cuDF] chunk 102 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 103 con 250000 righe\n",
      "[cuDF] chunk 103 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 104 con 250000 righe\n",
      "[cuDF] chunk 104 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 105 con 250000 righe\n",
      "[cuDF] chunk 105 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 106 con 250000 righe\n",
      "[cuDF] chunk 106 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 107 con 250000 righe\n",
      "[cuDF] chunk 107 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 108 con 250000 righe\n",
      "[cuDF] chunk 108 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 109 con 250000 righe\n",
      "[cuDF] chunk 109 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 110 con 250000 righe\n",
      "[cuDF] chunk 110 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 111 con 250000 righe\n",
      "[cuDF] chunk 111 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 112 con 250000 righe\n",
      "[cuDF] chunk 112 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 113 con 250000 righe\n",
      "[cuDF] chunk 113 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 114 con 250000 righe\n",
      "[cuDF] chunk 114 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 115 con 250000 righe\n",
      "[cuDF] chunk 115 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 116 con 250000 righe\n",
      "[cuDF] chunk 116 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 117 con 250000 righe\n",
      "[cuDF] chunk 117 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 118 con 250000 righe\n",
      "[cuDF] chunk 118 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 119 con 250000 righe\n",
      "[cuDF] chunk 119 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 120 con 250000 righe\n",
      "[cuDF] chunk 120 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 121 con 250000 righe\n",
      "[cuDF] chunk 121 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 122 con 250000 righe\n",
      "[cuDF] chunk 122 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 123 con 250000 righe\n",
      "[cuDF] chunk 123 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 124 con 250000 righe\n",
      "[cuDF] chunk 124 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 125 con 250000 righe\n",
      "[cuDF] chunk 125 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 126 con 250000 righe\n",
      "[cuDF] chunk 126 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 127 con 250000 righe\n",
      "[cuDF] chunk 127 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 128 con 250000 righe\n",
      "[cuDF] chunk 128 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 129 con 250000 righe\n",
      "[cuDF] chunk 129 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 130 con 250000 righe\n",
      "[cuDF] chunk 130 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 131 con 250000 righe\n",
      "[cuDF] chunk 131 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 132 con 250000 righe\n",
      "[cuDF] chunk 132 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 133 con 250000 righe\n",
      "[cuDF] chunk 133 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 134 con 250000 righe\n",
      "[cuDF] chunk 134 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 135 con 250000 righe\n",
      "[cuDF] chunk 135 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 136 con 250000 righe\n",
      "[cuDF] chunk 136 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 137 con 250000 righe\n",
      "[cuDF] chunk 137 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 138 con 250000 righe\n",
      "[cuDF] chunk 138 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 139 con 250000 righe\n",
      "[cuDF] chunk 139 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 140 con 250000 righe\n",
      "[cuDF] chunk 140 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 141 con 250000 righe\n",
      "[cuDF] chunk 141 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 142 con 250000 righe\n",
      "[cuDF] chunk 142 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 143 con 250000 righe\n",
      "[cuDF] chunk 143 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 144 con 250000 righe\n",
      "[cuDF] chunk 144 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 145 con 250000 righe\n",
      "[cuDF] chunk 145 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 146 con 250000 righe\n",
      "[cuDF] chunk 146 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 147 con 250000 righe\n",
      "[cuDF] chunk 147 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 148 con 250000 righe\n",
      "[cuDF] chunk 148 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 149 con 250000 righe\n",
      "[cuDF] chunk 149 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 150 con 250000 righe\n",
      "[cuDF] chunk 150 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 151 con 250000 righe\n",
      "[cuDF] chunk 151 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 152 con 250000 righe\n",
      "[cuDF] chunk 152 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 153 con 250000 righe\n",
      "[cuDF] chunk 153 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 154 con 250000 righe\n",
      "[cuDF] chunk 154 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 155 con 250000 righe\n",
      "[cuDF] chunk 155 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 156 con 250000 righe\n",
      "[cuDF] chunk 156 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 157 con 250000 righe\n",
      "[cuDF] chunk 157 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 158 con 250000 righe\n",
      "[cuDF] chunk 158 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 159 con 250000 righe\n",
      "[cuDF] chunk 159 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 160 con 250000 righe\n",
      "[cuDF] chunk 160 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 161 con 250000 righe\n",
      "[cuDF] chunk 161 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 162 con 250000 righe\n",
      "[cuDF] chunk 162 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 163 con 250000 righe\n",
      "[cuDF] chunk 163 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 164 con 250000 righe\n",
      "[cuDF] chunk 164 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 165 con 250000 righe\n",
      "[cuDF] chunk 165 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 166 con 250000 righe\n",
      "[cuDF] chunk 166 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 167 con 250000 righe\n",
      "[cuDF] chunk 167 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 168 con 250000 righe\n",
      "[cuDF] chunk 168 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 169 con 250000 righe\n",
      "[cuDF] chunk 169 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 170 con 250000 righe\n",
      "[cuDF] chunk 170 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 171 con 250000 righe\n",
      "[cuDF] chunk 171 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 172 con 250000 righe\n",
      "[cuDF] chunk 172 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 173 con 250000 righe\n",
      "[cuDF] chunk 173 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 174 con 250000 righe\n",
      "[cuDF] chunk 174 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 175 con 250000 righe\n",
      "[cuDF] chunk 175 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 176 con 250000 righe\n",
      "[cuDF] chunk 176 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 177 con 250000 righe\n",
      "[cuDF] chunk 177 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 178 con 250000 righe\n",
      "[cuDF] chunk 178 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 179 con 250000 righe\n",
      "[cuDF] chunk 179 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 180 con 250000 righe\n",
      "[cuDF] chunk 180 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 181 con 250000 righe\n",
      "[cuDF] chunk 181 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 182 con 250000 righe\n",
      "[cuDF] chunk 182 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 183 con 250000 righe\n",
      "[cuDF] chunk 183 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 184 con 250000 righe\n",
      "[cuDF] chunk 184 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 185 con 250000 righe\n",
      "[cuDF] chunk 185 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 186 con 250000 righe\n",
      "[cuDF] chunk 186 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 187 con 250000 righe\n",
      "[cuDF] chunk 187 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 188 con 250000 righe\n",
      "[cuDF] chunk 188 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 189 con 250000 righe\n",
      "[cuDF] chunk 189 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 190 con 250000 righe\n",
      "[cuDF] chunk 190 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 191 con 250000 righe\n",
      "[cuDF] chunk 191 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 192 con 250000 righe\n",
      "[cuDF] chunk 192 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 193 con 250000 righe\n",
      "[cuDF] chunk 193 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 194 con 250000 righe\n",
      "[cuDF] chunk 194 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 195 con 250000 righe\n",
      "[cuDF] chunk 195 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 196 con 250000 righe\n",
      "[cuDF] chunk 196 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 197 con 250000 righe\n",
      "[cuDF] chunk 197 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 198 con 250000 righe\n",
      "[cuDF] chunk 198 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 199 con 250000 righe\n",
      "[cuDF] chunk 199 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 200 con 250000 righe\n",
      "[cuDF] chunk 200 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 201 con 250000 righe\n",
      "[cuDF] chunk 201 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 202 con 250000 righe\n",
      "[cuDF] chunk 202 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 203 con 250000 righe\n",
      "[cuDF] chunk 203 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 204 con 250000 righe\n",
      "[cuDF] chunk 204 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 205 con 250000 righe\n",
      "[cuDF] chunk 205 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 206 con 250000 righe\n",
      "[cuDF] chunk 206 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 207 con 250000 righe\n",
      "[cuDF] chunk 207 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 208 con 250000 righe\n",
      "[cuDF] chunk 208 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 209 con 250000 righe\n",
      "[cuDF] chunk 209 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 210 con 250000 righe\n",
      "[cuDF] chunk 210 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 211 con 250000 righe\n",
      "[cuDF] chunk 211 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 212 con 250000 righe\n",
      "[cuDF] chunk 212 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 213 con 250000 righe\n",
      "[cuDF] chunk 213 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 214 con 250000 righe\n",
      "[cuDF] chunk 214 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 215 con 250000 righe\n",
      "[cuDF] chunk 215 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 216 con 250000 righe\n",
      "[cuDF] chunk 216 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 217 con 250000 righe\n",
      "[cuDF] chunk 217 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 218 con 250000 righe\n",
      "[cuDF] chunk 218 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 219 con 250000 righe\n",
      "[cuDF] chunk 219 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 220 con 250000 righe\n",
      "[cuDF] chunk 220 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 221 con 250000 righe\n",
      "[cuDF] chunk 221 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 222 con 250000 righe\n",
      "[cuDF] chunk 222 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 223 con 250000 righe\n",
      "[cuDF] chunk 223 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 224 con 250000 righe\n",
      "[cuDF] chunk 224 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 225 con 250000 righe\n",
      "[cuDF] chunk 225 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 226 con 250000 righe\n",
      "[cuDF] chunk 226 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 227 con 250000 righe\n",
      "[cuDF] chunk 227 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 228 con 250000 righe\n",
      "[cuDF] chunk 228 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 229 con 250000 righe\n",
      "[cuDF] chunk 229 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 230 con 250000 righe\n",
      "[cuDF] chunk 230 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 231 con 250000 righe\n",
      "[cuDF] chunk 231 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 232 con 250000 righe\n",
      "[cuDF] chunk 232 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 233 con 250000 righe\n",
      "[cuDF] chunk 233 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 234 con 250000 righe\n",
      "[cuDF] chunk 234 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 235 con 250000 righe\n",
      "[cuDF] chunk 235 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 236 con 250000 righe\n",
      "[cuDF] chunk 236 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 237 con 250000 righe\n",
      "[cuDF] chunk 237 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 238 con 250000 righe\n",
      "[cuDF] chunk 238 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 239 con 250000 righe\n",
      "[cuDF] chunk 239 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 240 con 250000 righe\n",
      "[cuDF] chunk 240 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 241 con 250000 righe\n",
      "[cuDF] chunk 241 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 242 con 250000 righe\n",
      "[cuDF] chunk 242 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 243 con 250000 righe\n",
      "[cuDF] chunk 243 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 244 con 250000 righe\n",
      "[cuDF] chunk 244 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 245 con 250000 righe\n",
      "[cuDF] chunk 245 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 246 con 250000 righe\n",
      "[cuDF] chunk 246 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 247 con 250000 righe\n",
      "[cuDF] chunk 247 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 248 con 250000 righe\n",
      "[cuDF] chunk 248 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 249 con 250000 righe\n",
      "[cuDF] chunk 249 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 250 con 250000 righe\n",
      "[cuDF] chunk 250 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 251 con 250000 righe\n",
      "[cuDF] chunk 251 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 252 con 250000 righe\n",
      "[cuDF] chunk 252 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 253 con 250000 righe\n",
      "[cuDF] chunk 253 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 254 con 250000 righe\n",
      "[cuDF] chunk 254 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 255 con 250000 righe\n",
      "[cuDF] chunk 255 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 256 con 250000 righe\n",
      "[cuDF] chunk 256 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 257 con 250000 righe\n",
      "[cuDF] chunk 257 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 258 con 250000 righe\n",
      "[cuDF] chunk 258 in GPU con shape (250000, 13)\n",
      "[pandas] letto chunk 259 con 164523 righe\n",
      "[cuDF] chunk 259 in GPU con shape (164523, 13)\n",
      "(64914523, 13)\n",
      "Tempo totale di esecuzione:  616.730731010437\n"
     ]
    }
   ],
   "source": [
    "CHUNK = 200_000_000  # 200 MB per volta\n",
    "offset = 0\n",
    "part = 0\n",
    "\n",
    "import time\n",
    "\n",
    "# l'avviamento del timer\n",
    "start_time = time.time()\n",
    "\n",
    "# // il codice da misurare\n",
    "\n",
    "# 1. percorso del file\n",
    "CSV_PATH = \"/content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv\"\n",
    "\n",
    "# 2. dimensione del chunk (si parte basso)\n",
    "ROWS_PER_CHUNK = 250_000   # ~200-300 MB a seconda delle colonne\n",
    "\n",
    "# 3. se si vogliono accumulare i chunk in GPU (solo se li possiamo tenere)\n",
    "gdf_parts = []\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(CSV_PATH, chunksize=ROWS_PER_CHUNK)):\n",
    "    print(f\"[pandas] letto chunk {i} con {len(chunk)} righe\")\n",
    "\n",
    "    # 4. converte il chunk pandas -> cuDF (qui usiamo la GPU)\n",
    "    gdf_chunk = cudf.from_pandas(chunk)\n",
    "    print(f\"[cuDF] chunk {i} in GPU con shape {gdf_chunk.shape}\")\n",
    "\n",
    "    # ‚¨áÔ∏è qui possiamo fare le nostre operazioni in GPU\n",
    "    # esempio: filtro\n",
    "    # gdf_chunk = gdf_chunk[gdf_chunk[\"BOROUGH\"] == \"MANHATTAN\"]\n",
    "\n",
    "    # esempio: salviamo subito in parquet per non tenere tutto in GPU\n",
    "    # gdf_chunk.to_parquet(f\"/content/out_part_{i:04d}.parquet\")\n",
    "\n",
    "    # se invece li teniamo per per unirli dopo:\n",
    "    gdf_parts.append(gdf_chunk)\n",
    "\n",
    "# 5. (opzionale) uniamo tutti i pezzi GPU in un unico DataFrame cuDF\n",
    "# ‚ö†Ô∏è facciamo solo se ci sta in VRAM\n",
    "if gdf_parts:\n",
    "    gdf_all = cudf.concat(gdf_parts, ignore_index=True)\n",
    "    print(gdf_all.shape)\n",
    "\n",
    "# // fine del codice da misurare\n",
    "\n",
    "# fine timer e stampa\n",
    "end_time = time.time()\n",
    "print ('Tempo totale di esecuzione: ', end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6ddab5",
   "metadata": {},
   "source": [
    "Let‚Äôs see line by line what the code of the previous cell does.\n",
    "\n",
    "**Timer**\n",
    "```python\n",
    "import time\n",
    "start_time = time.time()\n",
    "```\n",
    "where:\n",
    "- we import the `time` package\n",
    "- we save the starting instant\n",
    "- at the end we save the ending instant to say ‚Äúall this loop took X seconds‚Äù.\n",
    "...\n",
    "We print how long the whole loop took: chunk reading + conversion + possible concat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H_M4fMa-FtsD",
   "metadata": {
    "id": "H_M4fMa-FtsD"
   },
   "source": [
    "The following version has another approach: it ‚Äúdoes not accumulate, it processes and then discards‚Äù, it is even safer (**770 seconds on L4‚ÄëGPU**) üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZdodFJq4F56x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdodFJq4F56x",
    "outputId": "a2f11623-f636-41a0-b7a5-91bb13606999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo totale di esecuzione:  770.3484830856323\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# l'avviamento del timer\n",
    "start_time = time.time()\n",
    "\n",
    "CSV_PATH = \"/content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv\"\n",
    "ROWS_PER_CHUNK = 250_000\n",
    "\n",
    "# creazione della directory sul session storage\n",
    "out_dir = \"/content/parquet_parts\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(CSV_PATH, chunksize=ROWS_PER_CHUNK)):\n",
    "    gdf = cudf.from_pandas(chunk)\n",
    "    # fai le tue operazioni qui...\n",
    "    # e poi NON lo tieni in memoria\n",
    "    gdf.to_parquet(f\"{out_dir}/part_{i:04d}.parquet\")\n",
    "\n",
    "# fine timer e stampa\n",
    "end_time = time.time()\n",
    "print ('Tempo totale di esecuzione: ', end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kjFBw0SGpDle",
   "metadata": {
    "id": "kjFBw0SGpDle"
   },
   "source": [
    "`chunk` ‚Üí is the **pandas dataframe** that comes from the CSV.\n",
    "\n",
    "`gdf` ‚Üí is the **cuDF dataframe** (the ‚Äúreal‚Äù one we work on in the GPU).\n",
    "\n",
    "do we want to use pandas? ‚Üí we use `chunk`<br>\n",
    "do we want to use cuDF? ‚Üí we use `gdf` (this is ‚Äúthe dataframe‚Äù we are interested in for the GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7G8P4O6ysyyN",
   "metadata": {
    "id": "7G8P4O6ysyyN"
   },
   "source": [
    "We want to count the rows:\n",
    "...\n",
    "- timestamp ‚Üí the date/time of the measurement (every 30 minutes).\n",
    "- open, high, low, close, volume (OHLCV) ‚Üí classic trading fields.\n",
    "\n",
    "üìê Approximate size:\n",
    "- About **36 million rows**,\n",
    "- Size **~ 600‚Äì700 MB** in Parquet format,\n",
    "- **If converted to CSV it would become much heavier (even several GB)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "PYptJ8a8soQ2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PYptJ8a8soQ2",
    "outputId": "aec8b167-c6d5-483f-c993-12a205540005"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !wc -l /content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k3ckz0MWtNmu",
   "metadata": {
    "id": "k3ckz0MWtNmu"
   },
   "source": [
    "64,914,524 rows!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xr6vc01At3w2",
   "metadata": {
    "id": "Xr6vc01At3w2"
   },
   "source": [
    "Knowing the size in rows of the file we can now write the **most effective file‚Äëreading loop** (with `chunk` = 1,000,000):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "No59puLbt_B-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "No59puLbt_B-",
    "outputId": "23630120-d65a-4ff0-a90d-11bf3aaf3029"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cudf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pd\u001b[38;5;241m.\u001b[39mread_csv(path, chunksize\u001b[38;5;241m=\u001b[39mROWS_PER_CHUNK)):\n\u001b[1;32m---> 10\u001b[0m     gdf \u001b[38;5;241m=\u001b[39m cudf\u001b[38;5;241m.\u001b[39mfrom_pandas(chunk)\n\u001b[0;32m     11\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(gdf)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(gdf)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m righe, totale: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cudf' is not defined"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "    path = \"/content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv\"\n",
    "\n",
    "    ROWS_PER_CHUNK = 1_000_000   # 1 milione: ~65 giri\n",
    "\n",
    "    total = 0\n",
    "    for i, chunk in enumerate(pd.read_csv(path, chunksize=ROWS_PER_CHUNK)):\n",
    "        gdf = cudf.from_pandas(chunk)\n",
    "        total += len(gdf)\n",
    "        print(f\"chunk {i:03d} -> {len(gdf)} righe, totale: {total}\")\n",
    "\n",
    "    print(\"‚úÖ totale letto:\", total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1oyaldXrxaVG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1oyaldXrxaVG",
    "outputId": "7028812d-a4d3-4f2f-f691-1cc80b725f90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(914523, 13)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk.shape # l'ultimo chunk in memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cytbmIYoxfSe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cytbmIYoxfSe",
    "outputId": "117c8106-248e-469b-b2d6-6baa987e9359"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(914523, 13)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf.shape # l'ultimo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O7-F9giOy1Si",
   "metadata": {
    "id": "O7-F9giOy1Si"
   },
   "source": [
    "Before we read in chunks.<br>\n",
    "Now let‚Äôs TRY to read the whole file **in a single dataframe in memory**. There is the risk that **RAM explodes**, but here we have 53GB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1BW_EkvBy7V-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "1BW_EkvBy7V-",
    "outputId": "47d2bbc1-dc34-452e-cec5-a67d9b35d40f"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(path, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1_000_000\u001b[39m):\n\u001b[0;32m      3\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(chunk)\n\u001b[1;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[0;32m    685\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m    686\u001b[0m )\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:185\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m         values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 185\u001b[0m     values \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(values)\n\u001b[0;32m    187\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m values\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\core\\construction.py:481\u001b[0m, in \u001b[0;36mensure_wrapped_if_datetimelike\u001b[1;34m(arr)\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto_numpy()  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mensure_wrapped_if_datetimelike\u001b[39m(arr):\n\u001b[0;32m    482\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;124;03m    Wrap datetime64 and timedelta64 ndarrays in DatetimeArray/TimedeltaArray.\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, np\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "    dfs = []\n",
    "    for chunk in pd.read_csv(path, chunksize=1_000_000):\n",
    "        dfs.append(chunk)\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n1xWoleF1GVu",
   "metadata": {
    "id": "n1xWoleF1GVu"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7zZHnjRk1Osu",
   "metadata": {
    "id": "7zZHnjRk1Osu"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-vOgqkll0i6H",
   "metadata": {
    "id": "-vOgqkll0i6H"
   },
   "source": [
    "If we wanted instead to read back the first chunk we would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z5ip_NeZ1E2u",
   "metadata": {
    "id": "Z5ip_NeZ1E2u"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"/content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv\"\n",
    "\n",
    "reader = pd.read_csv(path, chunksize=1_000_000)  # creates the iterator\n",
    "first_chunk = next(reader)                       # takes ONLY the first piece\n",
    "\n",
    "first_chunk.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db88df-326e-44f1-b303-f6f1e7e64db3",
   "metadata": {
    "id": "e1db88df-326e-44f1-b303-f6f1e7e64db3"
   },
   "source": [
    "# The *parquet* format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b851181-6e65-4abc-ae55-0defc23b36e8",
   "metadata": {
    "id": "6b851181-6e65-4abc-ae55-0defc23b36e8"
   },
   "source": [
    "We will use the time series `usa_stocks_30m.parquet`: it is an OHLCV series of 514 Nasdaq and NYSE stocks (from 1998 to 2024).\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a21e07-e9a6-41ad-a212-58c4e9591723",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T11:07:44.454739Z",
     "iopub.status.busy": "2025-10-21T11:07:44.454485Z",
     "iopub.status.idle": "2025-10-21T11:08:48.677259Z",
     "shell.execute_reply": "2025-10-21T11:08:48.676694Z",
     "shell.execute_reply.started": "2025-10-21T11:07:44.454724Z"
    },
    "id": "42a21e07-e9a6-41ad-a212-58c4e9591723",
    "outputId": "b53971e6-1a21-4fa8-b725-a4611586cf0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scarico il file usa_stocks_30m.parquet...\n",
      "Download completato.\n"
     ]
    }
   ],
   "source": [
    "# Download of the big time series\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"usa_stocks_30m.parquet\"\n",
    "url = \"https://storage.googleapis.com/rapidsai/colab-data/usa_stocks_30m.parquet\"\n",
    "\n",
    "if not os.path.isfile(file_path):\n",
    "    print(f\"Scarico il file {file_path}...\")\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "    print(\"Download completato.\")\n",
    "else:\n",
    "    print(f\"{file_path} gi√† presente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980ec3c-2bb6-48ac-91f9-f3b51bfaf546",
   "metadata": {
    "id": "b980ec3c-2bb6-48ac-91f9-f3b51bfaf546"
   },
   "source": [
    "**The file `usa_stocks_30m.parquet`**<br>\n",
    "\n",
    "The file `usa_stocks_30m.parquet` is a dataset made available to experiment with financial time series with GPU‚Äëaccelerated libraries (like `cuDF`).\n",
    "\n",
    "üìå In practice:\n",
    "- It is a file in **Parquet format** (columnar, compressed, very efficient for big data).\n",
    "- It contains **US stock price** data (listed stocks) recorded at a **30‚Äëminute frequency**.\n",
    "- It is intended for demos: time‚Äëseries analysis, manipulation with pandas/cuDF, CPU vs GPU benchmarks.\n",
    "\n",
    "üìä Typically it includes:\n",
    "- ticker ‚Üí the stock symbol (e.g. AAPL, MSFT).\n",
    "- timestamp ‚Üí the date/time of the measurement (every 30 min).\n",
    "- open, high, low, close, volume (OHLCV) ‚Üí classic trading fields.\n",
    "\n",
    "üìê Approximate size:\n",
    "- About **36 million rows**,\n",
    "- Size **~ 600‚Äì700 MB** in Parquet format,\n",
    "- **If converted to CSV it would become much heavier (even several GB)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2bcc0d",
   "metadata": {
    "id": "0f2bcc0d"
   },
   "source": [
    "üëâ The Parquet format **is much more efficient than CSV**:\n",
    "- it is binary and compressed (takes up less space);\n",
    "- it is columnar ‚Üí pandas can read only the needed columns;\n",
    "- it preserves data types (no inference every time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f8500e-dc26-4e34-9d62-4c479d76d7fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T11:10:08.319709Z",
     "iopub.status.busy": "2025-10-21T11:10:08.319486Z",
     "iopub.status.idle": "2025-10-21T11:10:09.816654Z",
     "shell.execute_reply": "2025-10-21T11:10:09.816286Z",
     "shell.execute_reply.started": "2025-10-21T11:10:08.319695Z"
    },
    "id": "c8f8500e-dc26-4e34-9d62-4c479d76d7fb"
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"usa_stocks_30m.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c6387e-944b-4ac2-b893-ac306da0318b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T11:10:15.799215Z",
     "iopub.status.busy": "2025-10-21T11:10:15.799025Z",
     "iopub.status.idle": "2025-10-21T11:10:15.810892Z",
     "shell.execute_reply": "2025-10-21T11:10:15.810518Z",
     "shell.execute_reply.started": "2025-10-21T11:10:15.799201Z"
    },
    "id": "38c6387e-944b-4ac2-b893-ac306da0318b",
    "outputId": "16174d18-617d-43fb-813a-c3db214b535c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999-11-18 17:00:00</td>\n",
       "      <td>45.56</td>\n",
       "      <td>50.00</td>\n",
       "      <td>45.50</td>\n",
       "      <td>46.00</td>\n",
       "      <td>9275000</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999-11-18 17:30:00</td>\n",
       "      <td>46.00</td>\n",
       "      <td>47.69</td>\n",
       "      <td>45.82</td>\n",
       "      <td>46.57</td>\n",
       "      <td>3200900</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999-11-18 18:00:00</td>\n",
       "      <td>46.56</td>\n",
       "      <td>46.63</td>\n",
       "      <td>41.00</td>\n",
       "      <td>41.00</td>\n",
       "      <td>3830500</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999-11-18 18:30:00</td>\n",
       "      <td>41.00</td>\n",
       "      <td>43.38</td>\n",
       "      <td>40.37</td>\n",
       "      <td>42.38</td>\n",
       "      <td>3688600</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999-11-18 19:00:00</td>\n",
       "      <td>42.31</td>\n",
       "      <td>42.44</td>\n",
       "      <td>41.56</td>\n",
       "      <td>41.69</td>\n",
       "      <td>1584300</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime   open   high    low  close   volume ticker\n",
       "0 1999-11-18 17:00:00  45.56  50.00  45.50  46.00  9275000      A\n",
       "1 1999-11-18 17:30:00  46.00  47.69  45.82  46.57  3200900      A\n",
       "2 1999-11-18 18:00:00  46.56  46.63  41.00  41.00  3830500      A\n",
       "3 1999-11-18 18:30:00  41.00  43.38  40.37  42.38  3688600      A\n",
       "4 1999-11-18 19:00:00  42.31  42.44  41.56  41.69  1584300      A"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8727b398-e653-4ea0-8325-b267c373595a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T11:10:21.336437Z",
     "iopub.status.busy": "2025-10-21T11:10:21.335997Z",
     "iopub.status.idle": "2025-10-21T11:10:21.340274Z",
     "shell.execute_reply": "2025-10-21T11:10:21.339849Z",
     "shell.execute_reply.started": "2025-10-21T11:10:21.336418Z"
    },
    "id": "8727b398-e653-4ea0-8325-b267c373595a",
    "outputId": "69468dbd-03e4-431c-f0df-7bb77d4bfeca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36087094, 7)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c333f1a-b878-481c-9817-64f92d8d11ea",
   "metadata": {
    "id": "7c333f1a-b878-481c-9817-64f92d8d11ea"
   },
   "source": [
    "# Pickle and Feather: little used?\n",
    "\n",
    "- `Parquet`: default for large tabular data ‚Üí columnar, compressed, partitionable, cross‚Äëlanguage (Spark, DuckDB, BigQuery, etc.).\n",
    "- `CSV`: human/universal exchange, but heavy and slow.\n",
    "- `Feather (Arrow IPC file)`: super‚Äëfast for temporary exchanges between Python/R, but with fewer features (no partitioning, no append, little ‚Äúschema evolution‚Äù).\n",
    "- `Pickle`: Python‚Äëonly, not safe to load if you don‚Äôt trust the source, excellent for serialising Python objects (sklearn models, lists), not for durable tabular ‚Äúdata‚Äù.\n",
    "\n",
    "**Are `Feather` and `Pickle` ‚Äúlittle used‚Äù?**\n",
    "\n",
    "`Pickle`\n",
    "- üîí Security: pickle.load can execute code ‚Üí not recommended for shared files.\n",
    "- üß¨ Low portability: Python‚Äëonly and sometimes tied to versions/libraries.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e139d34b-1bdd-4a69-b5c0-cf5fd9cac163",
   "metadata": {
    "id": "e139d34b-1bdd-4a69-b5c0-cf5fd9cac163"
   },
   "source": [
    "# JSON format\n",
    "JSON is another very common format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2d437-b52a-4fcc-ad51-a50a03a1a58c",
   "metadata": {
    "id": "96b2d437-b52a-4fcc-ad51-a50a03a1a58c"
   },
   "source": [
    "1Ô∏è‚É£ **Python ‚Üí JSON**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06205a1-a18e-4b45-a2ed-209a2f2f774e",
   "metadata": {
    "id": "d06205a1-a18e-4b45-a2ed-209a2f2f774e"
   },
   "source": [
    "2Ô∏è‚É£ **JSON ‚Üí Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf021f1-f963-4e3f-8f6f-c7fb1b3a7506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:44:03.089523Z",
     "iopub.status.busy": "2025-10-24T18:44:03.089300Z",
     "iopub.status.idle": "2025-10-24T18:44:03.093787Z",
     "shell.execute_reply": "2025-10-24T18:44:03.093434Z",
     "shell.execute_reply.started": "2025-10-24T18:44:03.089509Z"
    },
    "id": "3cf021f1-f963-4e3f-8f6f-c7fb1b3a7506",
    "outputId": "835fd5ec-a5b0-4ddd-c3af-d5bdfb929d62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File JSON creato!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "dati = {\n",
    "    \"nome\": \"Antonio\",\n",
    "    \"eta\": 45,\n",
    "    \"linguaggi\": [\"Python\", \"SQL\", \"R\"],\n",
    "    \"attivo\": True\n",
    "}\n",
    "\n",
    "# Write to JSON file\n",
    "with open(\"dati.json\", \"w\") as f:\n",
    "    json.dump(dati, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ File JSON creato!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c83db7",
   "metadata": {
    "id": "83c83db7"
   },
   "source": [
    "We can also explore it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a854eae7-dcde-46e2-9d28-10eb000fc34f",
   "metadata": {
    "id": "a854eae7-dcde-46e2-9d28-10eb000fc34f"
   },
   "source": [
    "2Ô∏è‚É£ **Python ‚Üí CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b993e-98e6-4a78-8c5d-8f5465430e02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:44:22.161337Z",
     "iopub.status.busy": "2025-10-24T18:44:22.161108Z",
     "iopub.status.idle": "2025-10-24T18:44:22.165310Z",
     "shell.execute_reply": "2025-10-24T18:44:22.164829Z",
     "shell.execute_reply.started": "2025-10-24T18:44:22.161322Z"
    },
    "id": "4c3b993e-98e6-4a78-8c5d-8f5465430e02",
    "outputId": "85a74128-deec-48ae-e3bb-e947bed84282"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antonio\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"dati.json\", \"r\") as f:\n",
    "    dati_letti = json.load(f)\n",
    "\n",
    "print(dati_letti[\"nome\"])     # Antonio\n",
    "print(type(dati_letti))       # dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b35add1-62ec-4a31-b0b3-0339b610e752",
   "metadata": {
    "id": "1b35add1-62ec-4a31-b0b3-0339b610e752"
   },
   "source": [
    "3Ô∏è‚É£ **CSV ‚Üí JSON**\n",
    "\n",
    "We use the file `Credit_ISLR.csv`.<br>\n",
    "Let‚Äôs convert it to JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e3a0db-ca89-418c-98b7-a947e7457f0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:44:37.515423Z",
     "iopub.status.busy": "2025-10-24T18:44:37.515145Z",
     "iopub.status.idle": "2025-10-24T18:44:37.531262Z",
     "shell.execute_reply": "2025-10-24T18:44:37.530696Z",
     "shell.execute_reply.started": "2025-10-24T18:44:37.515406Z"
    },
    "id": "91e3a0db-ca89-418c-98b7-a947e7457f0d",
    "outputId": "03455685-a179-43ef-fe3f-de6a5c35d1c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CSV convertito in JSON!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "with open(\"Credit_ISLR.csv\", \"r\") as f_csv:\n",
    "    reader = csv.DictReader(f_csv)\n",
    "    dati = list(reader)\n",
    "\n",
    "with open(\"Credit_ISLR.json\", \"w\") as f_json:\n",
    "    json.dump(dati, f_json, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ CSV convertito in JSON!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fccb97-c1a3-4b7d-95a1-eaff91f594b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:45:11.763598Z",
     "iopub.status.busy": "2025-10-24T18:45:11.763267Z",
     "iopub.status.idle": "2025-10-24T18:45:11.767654Z",
     "shell.execute_reply": "2025-10-24T18:45:11.767036Z",
     "shell.execute_reply.started": "2025-10-24T18:45:11.763583Z"
    },
    "id": "94fccb97-c1a3-4b7d-95a1-eaff91f594b4"
   },
   "source": [
    "4Ô∏è‚É£ **JSON ‚Üí CSV**\n",
    "\n",
    "Now let‚Äôs do the opposite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ca8e7-8336-41bd-b16b-b597ca3d77f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:46:42.239974Z",
     "iopub.status.busy": "2025-10-24T18:46:42.239453Z",
     "iopub.status.idle": "2025-10-24T18:46:42.247525Z",
     "shell.execute_reply": "2025-10-24T18:46:42.247163Z",
     "shell.execute_reply.started": "2025-10-24T18:46:42.239958Z"
    },
    "id": "af9ca8e7-8336-41bd-b16b-b597ca3d77f6",
    "outputId": "064bbe6f-e0a9-4607-f77a-c52bfc9d7d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ JSON convertito in CSV!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "with open(\"Credit_ISLR.json\", \"r\") as f_json:\n",
    "    dati = json.load(f_json)\n",
    "\n",
    "with open(\"Credit_ISLR_out.csv\", \"w\", newline=\"\") as f_csv:\n",
    "    writer = csv.DictWriter(f_csv, fieldnames=dati[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(dati)\n",
    "\n",
    "print(\"‚úÖ JSON convertito in CSV!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5bc086-2d2e-4322-b78d-d57ba9b6e637",
   "metadata": {
    "id": "2c5bc086-2d2e-4322-b78d-d57ba9b6e637"
   },
   "source": [
    "![](json_sintesi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fb19b5-028d-4477-bae3-cc6149e75317",
   "metadata": {
    "id": "c7fb19b5-028d-4477-bae3-cc6149e75317"
   },
   "source": [
    "# Technical note on PDFs\n",
    "In VS Code the rendering of PDF files is different from that of Jupyter Notebook/Lab and from that of Google Colab.<br>\n",
    "The following function `show_pdf` detects which IDE is active and ‚Äúrenders‚Äù the PDF differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab319e3-b6e2-49d9-9221-66b650ba6f8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:29:25.166858Z",
     "iopub.status.busy": "2025-10-24T18:29:25.166658Z",
     "iopub.status.idle": "2025-10-24T18:29:25.172138Z",
     "shell.execute_reply": "2025-10-24T18:29:25.171545Z",
     "shell.execute_reply.started": "2025-10-24T18:29:25.166842Z"
    },
    "id": "4ab319e3-b6e2-49d9-9221-66b650ba6f8b"
   },
   "outputs": [],
   "source": [
    "def show_pdf(pdf_path, width=1000, height=600):\n",
    "    \"\"\"\n",
    "    Mostra un PDF nel modo pi√π appropriato per l'ambiente attuale:\n",
    "    - In Jupyter: visualizza inline con IFrame.\n",
    "    - In Colab: usa IFrame (gestisce bene i file caricati).\n",
    "    - In VS Code o altri ambienti: apre nel browser predefinito.\n",
    "    \"\"\"\n",
    "    import os, webbrowser, sys\n",
    "    from pathlib import Path\n",
    "\n",
    "    pdf_path = Path(pdf_path)\n",
    "    if not pdf_path.exists():\n",
    "        raise FileNotFoundError(f\"File non trovato: {pdf_path}\")\n",
    "\n",
    "    # Detect environment\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "    except NameError:\n",
    "        shell = None\n",
    "\n",
    "    if shell == 'ZMQInteractiveShell':  # Jupyter or Colab\n",
    "        from IPython.display import IFrame, display\n",
    "        display(IFrame(str(pdf_path), width=width, height=height))\n",
    "    elif \"vscode\" in sys.executable.lower() or \"vscode\" in os.getcwd().lower():\n",
    "        # VS Code environment ‚Üí opens in the browser\n",
    "        webbrowser.open(pdf_path.resolve().as_uri())\n",
    "        print(f\"üìÇ PDF aperto nel browser: {pdf_path}\")\n",
    "    else:\n",
    "        # Other environments (terminals, scripts)\n",
    "        webbrowser.open(pdf_path.resolve().as_uri())\n",
    "        print(f\"üìÇ PDF aperto nel browser: {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7edba68",
   "metadata": {
    "id": "d7edba68"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0934c5-46f1-4c9e-99f3-9e848be3782b",
   "metadata": {
    "id": "6b0934c5-46f1-4c9e-99f3-9e848be3782b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
