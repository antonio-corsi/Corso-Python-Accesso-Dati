{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d5a227-e219-418b-8d04-f62224e656c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:35:39.679553Z",
     "iopub.status.busy": "2025-10-21T08:35:39.679228Z",
     "iopub.status.idle": "2025-10-21T08:35:39.683523Z",
     "shell.execute_reply": "2025-10-21T08:35:39.683015Z",
     "shell.execute_reply.started": "2025-10-21T08:35:39.679537Z"
    },
    "id": "d9d5a227-e219-418b-8d04-f62224e656c5"
   },
   "source": [
    "<font size=\"6\">**Reading tabular files in Python**</font><br>\n",
    "\n",
    "> (c) 2025 Antonio Piemontese"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd80352",
   "metadata": {},
   "source": [
    "<p style=\"color:red; font-size:18px; font-weight:bold;\">\n",
    "üö® Questo notebook √® una traduzione in inglese cella per cella del notebook 'Leggere file tabellari in Python' üö®\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b6f42-42b8-4cc7-94ea-92ec013eb6a5",
   "metadata": {
    "id": "4b0b6f42-42b8-4cc7-94ea-92ec013eb6a5"
   },
   "source": [
    "# Data in Python (in Data Science)\n",
    "In Data Science we are often interested in:\n",
    "- analysing **past data**, not in real time\n",
    "- analysing a **single file**, not the DB\n",
    "\n",
    "These data are usually **tabular files**, so called because they are <u>made of rows and columns</u> (2 dimensions). They are usually **created by users**, received **from other companies**, or **simply exported from a DB** (as an export).\n",
    "\n",
    "One of the most widespread tabular formats for importing data into Python is the [**CSV**](https://en.wikipedia.org/wiki/Comma-separated_values) format. It is practically **ubiquitous in all tabular data‚Äëmanagement tools and environments**: Excel, Google Sheets, all relational DBs, etc.\n",
    "\n",
    "Even though in Python you can load **any kind of file** (xml, json, PDF, txt, etc.) and access **any kind of database** (Oracle, PostgreSQL, MySQL, etc.), the simplest and most efficient format to load in memory (into a `pandas` dataframe) is CSV.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data.csv')\n",
    "df = pd.read_excel('data.xlsx')\n",
    "```\n",
    "\n",
    "If you want **more performant** alternatives, nowadays `cuDF` (with GPU) or `Polars` are used a lot.\n",
    "\n",
    "<p style=\"color:red; font-size:18px; font-weight:bold;\">\n",
    "üö® Tabular files (and also the SQL tables we will see later) are usually loaded in a pandas dataframe üö®\n",
    "</p>\n",
    "Pandas is convenient but it is not the only way to import tabular files in Python.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae679e73-95ba-4316-836a-6186867428c8",
   "metadata": {
    "id": "ae679e73-95ba-4316-836a-6186867428c8"
   },
   "source": [
    "> ‚ÄúA CSV or Excel file (**a *tabular* file**) may look like a table as in the database, but it is only a container of raw data.\n",
    "An SQL table instead is a controlled structure, with rules, types and relations, which the database manages in a consistent, safe and transactional way.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e0dab-91fb-4180-90ef-6a9ad3acbba1",
   "metadata": {
    "id": "0f7e0dab-91fb-4180-90ef-6a9ad3acbba1"
   },
   "source": [
    "# Reading tabular files through specific libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fabb53-a6c8-4127-902f-e4e40e57bdaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T14:10:19.887968Z",
     "iopub.status.busy": "2025-10-21T14:10:19.887715Z",
     "iopub.status.idle": "2025-10-21T14:10:19.891475Z",
     "shell.execute_reply": "2025-10-21T14:10:19.890966Z",
     "shell.execute_reply.started": "2025-10-21T14:10:19.887952Z"
    },
    "id": "a1fabb53-a6c8-4127-902f-e4e40e57bdaf"
   },
   "source": [
    "To read <u>csv</u> tabular files a **first option** is the **`csv` library**, i.e. Python‚Äôs **built‚Äëin CSV**.<br>\n",
    "The reason for this choice, which however has many limits, is to avoid loading the *pandas* package (heavy) and to avoid dependency problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09fef0bb-3656-4cab-b542-baeef33d366c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:15:17.495587Z",
     "iopub.status.busy": "2025-10-24T18:15:17.495287Z",
     "iopub.status.idle": "2025-10-24T18:15:17.500288Z",
     "shell.execute_reply": "2025-10-24T18:15:17.499950Z",
     "shell.execute_reply.started": "2025-10-24T18:15:17.495572Z"
    },
    "id": "09fef0bb-3656-4cab-b542-baeef33d366c",
    "outputId": "705fc084-5d20-4861-e33b-6633a9eb95e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'ID', 'Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education', 'Gender', 'Student', 'Married', 'Ethnicity', 'Balance']\n",
      "['1', '1', '14.891', '3606', '283', '2', '34', '11', ' Male', 'No', 'Yes', 'Caucasian', '333']\n",
      "['2', '2', '106.025', '6645', '483', '3', '82', '15', 'Female', 'Yes', 'Yes', 'Asian', '903']\n",
      "['3', '3', '104.593', '7075', '514', '4', '71', '11', ' Male', 'No', 'No', 'Asian', '580']\n",
      "['4', '4', '148.924', '9504', '681', '3', '36', '11', 'Female', 'No', 'No', 'Asian', '964']\n",
      "['5', '5', '55.882', '4897', '357', '2', '68', '16', ' Male', 'No', 'Yes', 'Caucasian', '331']\n",
      "['6', '6', '80.18', '8047', '569', '4', '77', '10', ' Male', 'No', 'No', 'Caucasian', '1151']\n",
      "['7', '7', '20.996', '3388', '259', '2', '37', '12', 'Female', 'No', 'No', 'African American', '203']\n",
      "['8', '8', '71.408', '7114', '512', '2', '87', '9', ' Male', 'No', 'No', 'Asian', '872']\n",
      "['9', '9', '15.125', '3300', '266', '5', '66', '13', 'Female', 'No', 'No', 'Caucasian', '279']\n",
      "['10', '10', '71.061', '6819', '491', '3', '41', '19', 'Female', 'Yes', 'Yes', 'African American', '1350']\n",
      "['11', '11', '63.095', '8117', '589', '4', '30', '14', ' Male', 'No', 'Yes', 'Caucasian', '1407']\n",
      "['12', '12', '15.045', '1311', '138', '3', '64', '16', ' Male', 'No', 'No', 'Caucasian', '0']\n",
      "['13', '13', '80.616', '5308', '394', '1', '57', '7', 'Female', 'No', 'Yes', 'Asian', '204']\n",
      "['14', '14', '43.682', '6922', '511', '1', '49', '9', ' Male', 'No', 'Yes', 'Caucasian', '1081']\n",
      "['15', '15', '19.144', '3291', '269', '2', '75', '13', 'Female', 'No', 'No', 'African American', '148']\n",
      "['16', '16', '20.089', '2525', '200', '3', '57', '15', 'Female', 'No', 'Yes', 'African American', '0']\n",
      "['17', '17', '53.598', '3714', '286', '3', '73', '17', 'Female', 'No', 'Yes', 'African American', '0']\n",
      "['18', '18', '36.496', '4378', '339', '3', '69', '15', 'Female', 'No', 'Yes', 'Asian', '368']\n",
      "['19', '19', '49.57', '6384', '448', '1', '28', '9', 'Female', 'No', 'Yes', 'Asian', '891']\n",
      "['20', '20', '42.079', '6626', '479', '2', '44', '9', ' Male', 'No', 'No', 'Asian', '1048']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"Credit_ISLR.csv\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        print(row)\n",
    "        if i >= 20:   # print only the first 20 rows (0‚Äì19)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf0b1aa-8afa-40e2-8549-a4a916c5d04d",
   "metadata": {
    "id": "1cf0b1aa-8afa-40e2-8549-a4a916c5d04d"
   },
   "source": [
    "It is **very lightweight**, but you have to handle **everything manually** (types, header, encoding...):\n",
    "- type handling (everything is a string) ‚Äì the `csv` module does not automatically convert types: everything it reads is a string.\n",
    "- header handled manually ‚Äì the `csv` module does not know by itself whether the first line is a header or data.\n",
    "- encoding issues ‚Äì if the file is not UTF‚Äë8 (e.g. it is `latin-1` or `windows-1252`), `open()` will raise an error or show strange characters.\n",
    "- different separators (`,` or `;` or `\\t`) ‚Äì CSV files are not always comma‚Äëseparated ‚Äî in Italy often `;` or tabs.\n",
    "- quotes and special characters ‚Äì if a field contains a comma or a newline, parsing can break if you don‚Äôt use the right parameters.\n",
    "- missing data (empty cells) ‚Äì there is no concept of `NaN`.\n",
    "- with millions of rows, `csv.reader` is faster than pandas at reading raw rows, but then you cannot easily filter, join, or operate on data.\n",
    "\n",
    "**In short**:<br>\n",
    "Using plain `csv` is like reading the file ‚Äúby hand‚Äù: we have full control but all the work is on us. *pandas* (or *Polars*) instead understand header, types, separators, encoding, missing, etc. automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c553916-4ccd-46c2-b099-4dd448006f6a",
   "metadata": {
    "id": "3c553916-4ccd-46c2-b099-4dd448006f6a"
   },
   "source": [
    "A **second option** is the **`openpyxl`** module (for Excel **.xlsx** files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "447e03bf-b3ed-4816-bfd1-830e2031f9b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:15:26.579532Z",
     "iopub.status.busy": "2025-10-24T18:15:26.579333Z",
     "iopub.status.idle": "2025-10-24T18:15:27.063881Z",
     "shell.execute_reply": "2025-10-24T18:15:27.063340Z",
     "shell.execute_reply.started": "2025-10-24T18:15:26.579518Z"
    },
    "id": "447e03bf-b3ed-4816-bfd1-830e2031f9b3",
    "outputId": "2e7ecdf9-e515-413e-9b0f-1cf2df2ceb30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Column1', 'ID', 'Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education', 'Gender', 'Student', 'Married', 'Ethnicity', 'Balance')\n",
      "(1, 1, 14891, 3606, 283, 2, 34, 11, ' Male', 'No', 'Yes', 'Caucasian', 333)\n",
      "(2, 2, 106025, 6645, 483, 3, 82, 15, 'Female', 'Yes', 'Yes', 'Asian', 903)\n",
      "(3, 3, 104593, 7075, 514, 4, 71, 11, ' Male', 'No', 'No', 'Asian', 580)\n",
      "(4, 4, 148924, 9504, 681, 3, 36, 11, 'Female', 'No', 'No', 'Asian', 964)\n",
      "(5, 5, 55882, 4897, 357, 2, 68, 16, ' Male', 'No', 'Yes', 'Caucasian', 331)\n",
      "(6, 6, 8018, 8047, 569, 4, 77, 10, ' Male', 'No', 'No', 'Caucasian', 1151)\n",
      "(7, 7, 20996, 3388, 259, 2, 37, 12, 'Female', 'No', 'No', 'African American', 203)\n",
      "(8, 8, 71408, 7114, 512, 2, 87, 9, ' Male', 'No', 'No', 'Asian', 872)\n",
      "(9, 9, 15125, 3300, 266, 5, 66, 13, 'Female', 'No', 'No', 'Caucasian', 279)\n",
      "(10, 10, 71061, 6819, 491, 3, 41, 19, 'Female', 'Yes', 'Yes', 'African American', 1350)\n",
      "(11, 11, 63095, 8117, 589, 4, 30, 14, ' Male', 'No', 'Yes', 'Caucasian', 1407)\n",
      "(12, 12, 15045, 1311, 138, 3, 64, 16, ' Male', 'No', 'No', 'Caucasian', 0)\n",
      "(13, 13, 80616, 5308, 394, 1, 57, 7, 'Female', 'No', 'Yes', 'Asian', 204)\n",
      "(14, 14, 43682, 6922, 511, 1, 49, 9, ' Male', 'No', 'Yes', 'Caucasian', 1081)\n",
      "(15, 15, 19144, 3291, 269, 2, 75, 13, 'Female', 'No', 'No', 'African American', 148)\n",
      "(16, 16, 20089, 2525, 200, 3, 57, 15, 'Female', 'No', 'Yes', 'African American', 0)\n",
      "(17, 17, 53598, 3714, 286, 3, 73, 17, 'Female', 'No', 'Yes', 'African American', 0)\n",
      "(18, 18, 36496, 4378, 339, 3, 69, 15, 'Female', 'No', 'Yes', 'Asian', 368)\n",
      "(19, 19, 4957, 6384, 448, 1, 28, 9, 'Female', 'No', 'Yes', 'Asian', 891)\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "wb = load_workbook(\"Credit_ISLR.xlsx\")\n",
    "ws = wb.active\n",
    "\n",
    "for i, row in enumerate(ws.iter_rows(values_only=True)):\n",
    "    print(row)\n",
    "    if i >= 19:    # index starts from 0 ‚Üí 0‚Äì19 = 20 rows\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e637d41-8fbc-4d8e-a75d-5525141c1167",
   "metadata": {
    "id": "0e637d41-8fbc-4d8e-a75d-5525141c1167"
   },
   "source": [
    "A **third** option is the `xlrd` or `xlwt` module ‚Äì still for Excel ‚Äì for reading and writing `.xls`.<br>\n",
    "‚ö†Ô∏è Deprecated for `.xlsx`, so today they are less recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e830e7-b719-4de9-a97a-833f78a85cd2",
   "metadata": {
    "id": "83e830e7-b719-4de9-a97a-833f78a85cd2"
   },
   "source": [
    "So, in a nutshell:<br>\n",
    "![](sintesi_formati_tabellari_en.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4687b0-f8d9-414a-a71e-05b9fc4226a6",
   "metadata": {
    "id": "2e4687b0-f8d9-414a-a71e-05b9fc4226a6"
   },
   "source": [
    "# Loading into pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d825a892-c123-438e-83f0-ef3b519f894c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T14:46:39.234929Z",
     "iopub.status.busy": "2025-10-21T14:46:39.234603Z",
     "iopub.status.idle": "2025-10-21T14:46:39.238351Z",
     "shell.execute_reply": "2025-10-21T14:46:39.237877Z",
     "shell.execute_reply.started": "2025-10-21T14:46:39.234914Z"
    },
    "id": "d825a892-c123-438e-83f0-ef3b519f894c"
   },
   "source": [
    "If you only want to read/write (tabular) files without installing heavy external libraries, you can use `csv` or `openpyxl`, as shown before.\n",
    "\n",
    "Otherwise, you use the [**dataframes**](https://en.wikipedia.org/wiki/Pandas_(software)#DataFrames), which are the **most used data structure in Data Science**:\n",
    "- they live in memory\n",
    "- they can be loaded from disk or saved to disk with the `pd.read_***` functions or with the `df.to_XXX` methods for the following formats:<br>\n",
    "*clipboard, csv, excel, html, json, parquet, pickle, sas, sql, spss, stata, xml*.\n",
    "\n",
    "To load a *csv* or *xlsx* tabular file into a dataframe you use these two *pandas* functions:\n",
    "```python\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('data.csv')\n",
    "    df = pd.read_excel('data.xlsx')\n",
    "```\n",
    "\n",
    "If you want **more performant** alternatives, nowadays `cuDF` (with GPU) or `Polars` are used a lot.\n",
    "\n",
    "<p style=\"color:red; font-size:18px; font-weight:bold;\">\n",
    "üö® Tabular files (and also the SQL tables we will see later) are usually loaded in a pandas dataframe üö®\n",
    "</p>\n",
    "Pandas is convenient but it is not the only way to import tabular files in Python.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad194c6-cfc5-4629-acff-81f2ed49d549",
   "metadata": {
    "id": "aad194c6-cfc5-4629-acff-81f2ed49d549"
   },
   "source": [
    "# Excel or csv?\n",
    "What is the best format to import tabular files? Excel or csv?<br>\n",
    "It depends on the goal and on the context, but **in most cases CSV is more efficient, transparent and robust, whereas Excel is more convenient for the human user**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d64c30a-84f9-4f1d-a18f-842a6a4b31d9",
   "metadata": {
    "id": "9d64c30a-84f9-4f1d-a18f-842a6a4b31d9"
   },
   "source": [
    "---\n",
    "\n",
    "Let‚Äôs compare **CSV vs Excel** from the <u>technical</u> point of view and from the <u>human</u> point of view.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5d500-d1ee-4b7d-bb33-87f8d95e15d5",
   "metadata": {
    "id": "95b5d500-d1ee-4b7d-bb33-87f8d95e15d5"
   },
   "source": [
    "---\n",
    "**\"External libraries\"**: what does it mean?<br>\n",
    "The function `pd.read_excel()` is built into pandas, but it does not do *everything* by itself: for some Excel formats it relies on **external libraries** that actually handle the Excel format.<br>\n",
    "That is, how does `pd.read_excel()` really work?<br>\n",
    "When you call:\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_excel(\"data.xlsx\")\n",
    "```\n",
    "pandas:\n",
    "- recognises the file format (e.g. `.xls`, `.xlsx`, `.xlsb`)\n",
    "- uses an external ‚Äúengine‚Äù to read the data\n",
    "- turns what it reads into a `DataFrame`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693a008-8fd0-47b2-ba08-c65f68a81034",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T15:06:22.273393Z",
     "iopub.status.busy": "2025-10-21T15:06:22.273077Z",
     "iopub.status.idle": "2025-10-21T15:06:22.291626Z",
     "shell.execute_reply": "2025-10-21T15:06:22.291290Z",
     "shell.execute_reply.started": "2025-10-21T15:06:22.273373Z"
    },
    "id": "7693a008-8fd0-47b2-ba08-c65f68a81034"
   },
   "source": [
    "**3. Performance: indicative comparison**:<br>\n",
    "![](performance_csv_excel_en.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e55ed59-708f-4da5-913c-3c84dd371dde",
   "metadata": {
    "id": "6e55ed59-708f-4da5-913c-3c84dd371dde"
   },
   "source": [
    "**4. In practice**:<br>\n",
    "üëâ If the file **comes from a system or an application** (ERP, CRM, management, accounting, ‚Ä¶) it is almost always better to receive it as **CSV**.<br>\n",
    "üëâ If the file **is made by a person** and must be read by a person, Excel is nicer, but as soon as you need to automate the processing, CSV (or another machine‚Äëfriendly format) becomes preferable.<br>\n",
    "\n",
    "üëâ In any case you can always convert Excel ‚Üí csv with `pandas.to_csv()` or `to_parquet()` for internal / efficient storage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cfda21-e530-405e-8f22-4d3672241961",
   "metadata": {
    "id": "33cfda21-e530-405e-8f22-4d3672241961"
   },
   "source": [
    "# The importance and spread of the *csv* format\n",
    "Let‚Äôs see in more detail **why the CSV format is so ubiquitous in the data world**: practically one cannot imagine a tool for managing tabular data (Excel, Google Sheets, LibreOffice Calc, most reporting, BI and relational DB tools, etc.) that does not allow csv import/export."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e00a387",
   "metadata": {
    "id": "9e00a387"
   },
   "source": [
    "# The importance and diffusion of the *csv* format\n",
    "Let‚Äôs see in more detail **why the CSV format is so ubiquitous in the data world** (it is found everywhere, always).\n",
    "It is a format supported by almost all tools (Excel, DBs, BI, reporting, data‚Äëscience tools...).\n",
    "![](importanza_csv_en.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4810542f",
   "metadata": {
    "id": "4810542f"
   },
   "source": [
    "# Technical reasons for the diffusion of csv\n",
    "There are several **very concrete technical reasons** that explain why the CSV format is so omnipresent in the data world.<br>\n",
    "Here is a clear and technical summary table:\n",
    "\n",
    "![](diffusione_csv_en.png)\n",
    "\n",
    "üí° In short:<br>\n",
    "CSV is the **‚Äúlowest common denominator‚Äù of tabular data**: simple, textual, line‚Äëbased, without dependencies and compatible with everything ‚Äî from Excel to Spark.<br>\n",
    "It is not perfect (no types, schema, compression or metadata), but exactly **its structural poverty is its strength**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d45255-be27-4a5d-9932-790901b179e2",
   "metadata": {
    "id": "99d45255-be27-4a5d-9932-790901b179e2"
   },
   "source": [
    "# Reading CSV files in pandas\n",
    "\n",
    "As said, in Data Science we often are NOT interested in the online DB but in **a local file**, which can be of various formats (csv, json, parquet, etc.).\n",
    "\n",
    "## 3 technical notes on the CSV format\n",
    "\n",
    "* the two main arguments of the pandas `read_csv` method are the **column separator** (default `,`) and the **presence** (and possible number) of heading rows (header).\n",
    "* there are different csv formats available from Excel; you must choose the correct one (see the YouTube video of *Excel Tutorials by EasyClick Academy*).\n",
    "  \n",
    "  ![](tipi_csv_en.png)\n",
    "* [pros and cons](https://towardsdatascience.com/why-i-stopped-dumping-dataframes-to-a-csv-and-why-you-should-too-c0954c410f8f) of the csv format\n",
    "\n",
    "A csv file is textual and therefore can also be read in Notepad.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae4ec6c-ed02-4ecc-b78c-b8feb1efda58",
   "metadata": {
    "id": "cae4ec6c-ed02-4ecc-b78c-b8feb1efda58"
   },
   "source": [
    "Let‚Äôs load in *pandas* the well‚Äëknown banking file `Credit_ISLR`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64f8f076-dc8b-488e-8659-e63b4e9f43d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:43:33.738874Z",
     "iopub.status.busy": "2025-10-21T08:43:33.738657Z",
     "iopub.status.idle": "2025-10-21T08:43:33.761516Z",
     "shell.execute_reply": "2025-10-21T08:43:33.761072Z",
     "shell.execute_reply.started": "2025-10-21T08:43:33.738860Z"
    },
    "id": "64f8f076-dc8b-488e-8659-e63b4e9f43d8",
    "outputId": "b5f5374a-7219-4142-9354-234ed26c8527"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.891</td>\n",
       "      <td>3606</td>\n",
       "      <td>283</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>106.025</td>\n",
       "      <td>6645</td>\n",
       "      <td>483</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Asian</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>104.593</td>\n",
       "      <td>7075</td>\n",
       "      <td>514</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>148.924</td>\n",
       "      <td>9504</td>\n",
       "      <td>681</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>55.882</td>\n",
       "      <td>4897</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>396</td>\n",
       "      <td>396</td>\n",
       "      <td>12.096</td>\n",
       "      <td>4100</td>\n",
       "      <td>307</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>397</td>\n",
       "      <td>397</td>\n",
       "      <td>13.364</td>\n",
       "      <td>3838</td>\n",
       "      <td>296</td>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "      <td>17</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>African American</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>398</td>\n",
       "      <td>398</td>\n",
       "      <td>57.872</td>\n",
       "      <td>4171</td>\n",
       "      <td>321</td>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>399</td>\n",
       "      <td>399</td>\n",
       "      <td>37.728</td>\n",
       "      <td>2525</td>\n",
       "      <td>192</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>13</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>400</td>\n",
       "      <td>400</td>\n",
       "      <td>18.701</td>\n",
       "      <td>5524</td>\n",
       "      <td>415</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows √ó 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0   ID   Income  Limit  Rating  Cards  Age  Education  Gender  \\\n",
       "0             1    1   14.891   3606     283      2   34         11    Male   \n",
       "1             2    2  106.025   6645     483      3   82         15  Female   \n",
       "2             3    3  104.593   7075     514      4   71         11    Male   \n",
       "3             4    4  148.924   9504     681      3   36         11  Female   \n",
       "4             5    5   55.882   4897     357      2   68         16    Male   \n",
       "..          ...  ...      ...    ...     ...    ...  ...        ...     ...   \n",
       "395         396  396   12.096   4100     307      3   32         13    Male   \n",
       "396         397  397   13.364   3838     296      5   65         17    Male   \n",
       "397         398  398   57.872   4171     321      5   67         12  Female   \n",
       "398         399  399   37.728   2525     192      1   44         13    Male   \n",
       "399         400  400   18.701   5524     415      5   64          7  Female   \n",
       "\n",
       "    Student Married         Ethnicity  Balance  \n",
       "0        No     Yes         Caucasian      333  \n",
       "1       Yes     Yes             Asian      903  \n",
       "2        No      No             Asian      580  \n",
       "3        No      No             Asian      964  \n",
       "4        No     Yes         Caucasian      331  \n",
       "..      ...     ...               ...      ...  \n",
       "395      No     Yes         Caucasian      560  \n",
       "396      No      No  African American      480  \n",
       "397      No     Yes         Caucasian      138  \n",
       "398      No     Yes         Caucasian        0  \n",
       "399      No      No             Asian      966  \n",
       "\n",
       "[400 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_credit = pd.read_csv(\"Credit_ISLR.csv\",header=0)\n",
    "df_credit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f89a5-9591-40a8-8bfb-75d1643f41ae",
   "metadata": {
    "id": "973f89a5-9591-40a8-8bfb-75d1643f41ae"
   },
   "source": [
    "As you can see, the *pandas* `read_csv` function automatically created the column with index, so the original index `ID` is now <u>redundant</u>, and it added a column `Unnamed: 0` (we will see later why). It is good to drop both because they are useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adedb20e-72fa-4c93-95ed-4a48b2853d83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:51:08.184431Z",
     "iopub.status.busy": "2025-10-21T08:51:08.183921Z",
     "iopub.status.idle": "2025-10-21T08:51:08.191669Z",
     "shell.execute_reply": "2025-10-21T08:51:08.191323Z",
     "shell.execute_reply.started": "2025-10-21T08:51:08.184414Z"
    },
    "id": "adedb20e-72fa-4c93-95ed-4a48b2853d83"
   },
   "outputs": [],
   "source": [
    "df_credit.drop(columns=['Unnamed: 0', 'ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64d44fbb-2b10-4079-9d36-05411f3ce003",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:51:24.310829Z",
     "iopub.status.busy": "2025-10-21T08:51:24.310609Z",
     "iopub.status.idle": "2025-10-21T08:51:24.318242Z",
     "shell.execute_reply": "2025-10-21T08:51:24.317874Z",
     "shell.execute_reply.started": "2025-10-21T08:51:24.310813Z"
    },
    "id": "64d44fbb-2b10-4079-9d36-05411f3ce003",
    "outputId": "d4613669-fb74-4ea0-940d-e6dcd22ae105"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.891</td>\n",
       "      <td>3606</td>\n",
       "      <td>283</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106.025</td>\n",
       "      <td>6645</td>\n",
       "      <td>483</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Asian</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104.593</td>\n",
       "      <td>7075</td>\n",
       "      <td>514</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148.924</td>\n",
       "      <td>9504</td>\n",
       "      <td>681</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.882</td>\n",
       "      <td>4897</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Income  Limit  Rating  Cards  Age  Education  Gender Student Married  \\\n",
       "0   14.891   3606     283      2   34         11    Male      No     Yes   \n",
       "1  106.025   6645     483      3   82         15  Female     Yes     Yes   \n",
       "2  104.593   7075     514      4   71         11    Male      No      No   \n",
       "3  148.924   9504     681      3   36         11  Female      No      No   \n",
       "4   55.882   4897     357      2   68         16    Male      No     Yes   \n",
       "\n",
       "   Ethnicity  Balance  \n",
       "0  Caucasian      333  \n",
       "1      Asian      903  \n",
       "2      Asian      580  \n",
       "3      Asian      964  \n",
       "4  Caucasian      331  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5011c2",
   "metadata": {},
   "source": [
    "## Is pandas ‚Äúheavy‚Äù?\n",
    "\n",
    "‚öôÔ∏è What does it mean that ‚Äúpandas is heavy‚Äù? It is, in several ways:\n",
    "\n",
    "1Ô∏è‚É£ **Size and complexity**\n",
    "\n",
    "* Pandas is not a tiny library:<br>\n",
    "\n",
    "  * installing it brings in a lot of dependencies:\n",
    "  * `numpy`, `dateutil`, `pytz`, `tzdata`, `matplotlib`, `openpyxl`, `xlrd`, etc.\n",
    "* The package size is **dozens of MB**.\n",
    "* Loading it into memory at startup is **slower** than a simple `import csv`.\n",
    "\n",
    "üëâ So for a script that just has to read a CSV file and print 10 rows, importing the whole of pandas is like using a truck to deliver a letter.\n",
    "\n",
    "2Ô∏è‚É£ **External dependencies**<br>\n",
    "\n",
    "To work well with many formats, pandas uses external libraries (as mentioned above):\n",
    "\n",
    "* `openpyxl` for *.xlsx* files\n",
    "* `xlrd` for *.xls* files\n",
    "* `pyarrow` for *.parquet* files\n",
    "* `numexpr` for numeric operations\n",
    "* `matplotlib` for `.plot()`<br>\n",
    "\n",
    "üëâ These dependencies are convenient in a data-science environment,\n",
    "but excessive in a system script or a microservice.\n",
    "\n",
    "3Ô∏è‚É£ **Impact on small environments**<br>\n",
    "\n",
    "In contexts such as:\n",
    "\n",
    "* Docker microservices\n",
    "* lightweight CLI scripts\n",
    "* serverless functions (AWS Lambda, GCP Functions)\n",
    "* systems with memory constraints\n",
    "\n",
    "importing pandas can:\n",
    "\n",
    "* slow down the script startup,\n",
    "* increase the Docker image by tens or even hundreds of MB,\n",
    "* lead to incompatibilities or long cold-start times.\n",
    "\n",
    "**That‚Äôs why sometimes you want to ‚Äúavoid pandas‚Äù:**\n",
    "\n",
    "* You don‚Äôt need its advanced analytics features.\n",
    "* You just want to read a file and iterate over its rows.\n",
    "* You want to reduce dependencies and startup time.\n",
    "\n",
    "In that case it makes more sense to use:\n",
    "\n",
    "```python\n",
    "  import csv      # for CSV files\n",
    "  import openpyxl # for modern Excel files\n",
    "```\n",
    "\n",
    "which are much lighter modules.\n",
    "\n",
    "üß† **Metaphor**<br>\n",
    "Pandas is like Excel or a full ERP system.<br>\n",
    "If you just need to open a text file and print two columns, Notepad is enough.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf41df-1eaf-473d-849a-d7eddca92f06",
   "metadata": {
    "id": "f2bf41df-1eaf-473d-849a-d7eddca92f06"
   },
   "source": [
    "## Converting Excel files to CSV format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d281f-0e94-42e6-956f-122e767c9aaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T08:43:41.545329Z",
     "iopub.status.busy": "2025-10-21T08:43:41.545074Z",
     "iopub.status.idle": "2025-10-21T08:43:41.549360Z",
     "shell.execute_reply": "2025-10-21T08:43:41.548809Z",
     "shell.execute_reply.started": "2025-10-21T08:43:41.545312Z"
    },
    "id": "e06d281f-0e94-42e6-956f-122e767c9aaa"
   },
   "source": [
    "Conversely, to view a CSV file **in the standard Excel format** you can do this (there are other ways too):\n",
    "* open a **new file**\n",
    "* `Data` tab\n",
    "* button at the top left `Get Data`\n",
    "* `From file` --> `From text/CSV`\n",
    "* in the preview make any changes (on loading) ‚Äì maybe it‚Äôs enough ‚Äì and then press the \"Load\" button at the bottom right.\n",
    "\n",
    "This process is well described in the video [How to Convert CSV to Excel (Simple and Quick)](https://www.youtube.com/watch?v=jw1DSuqr3ew) from *Excel Tutorials by EasyClick Academy*. Subtitles are available. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433355c2",
   "metadata": {},
   "source": [
    "If a file is created in Excel and then converted to CSV, saved, and later reopened in Excel, Excel shows it ‚Äúas Excel‚Äù (i.e. with the grid). Why?<br>\n",
    "\n",
    "Excel doesn‚Äôt open the CSV as a real Excel file ‚Äî it **parses it as a text table** and displays it in the same UI (cells, rows, columns).<br>\n",
    "That makes it *look* like Excel, but the file actually doesn‚Äôt contain any of the typical `.xlsx` features:\n",
    "\n",
    "* no formatting,\n",
    "* no formulas,\n",
    "* no multiple sheets,\n",
    "* no complex data types.\n",
    "\n",
    "Excel is simply showing **a grid on top of a text file**.<br>\n",
    "It‚Äôs a bit like opening a `.txt` file in Word: the content is just text, but the environment is Word, with its ‚Äúrich‚Äù look.\n",
    "\n",
    "üí° One-sentence summary:<br>\n",
    "\n",
    "> Excel detects the `.csv` extension, interprets the data as tabular, and shows it in its interface ‚Äî but the file remains plain structured text, not a real Excel workbook.\n",
    "\n",
    "**Objection**: if I open in Excel a CSV that was generated independently of Excel, the grid is not applied.<br>\n",
    "True, Excel doesn‚Äôt parse CSV ‚Äúsmartly‚Äù based on the content ‚Äî it uses the system‚Äôs **regional settings** (the Windows ‚Äúlocale‚Äù or ‚Äúlist separator‚Äù).<br>\n",
    "For example:\n",
    "\n",
    "* in Europe, the default list separator is `;` (semicolon);\n",
    "* in the US/UK, it‚Äôs `,` (comma).\n",
    "\n",
    "So:\n",
    "\n",
    "* if the CSV comes from Excel, it uses the same separator as your regional settings ‚Üí Excel ‚Äúrecognizes‚Äù it and shows the table.\n",
    "* if the CSV comes from another program (e.g. Python `to_csv()`, MySQL, or international systems) that uses the comma, Excel doesn‚Äôt know that‚Äôs the separator and puts everything in a single cell.\n",
    "\n",
    "The user can fix it manually:\n",
    "\n",
    "```text\n",
    "Data ‚Üí From Text/CSV ‚Üí choose the correct delimiter (comma, semicolon, tab)\n",
    "```\n",
    "\n",
    "or change the system‚Äôs regional setting.\n",
    "\n",
    "üí° In short:\n",
    "\n",
    "> Excel ‚Äútreats as a table‚Äù only those CSV files that follow its regional conventions (separator and encoding).<br>\n",
    "> If the file is generated elsewhere with other standards, Excel shows it as text in one column.\n",
    "\n",
    "---\n",
    "## Summary table ‚Äî Ways to open a CSV with the correct grid\n",
    "\n",
    "| # | Method           | Path in Excel | Advantage | When to use it |\n",
    "|---|------------------|---------------|-----------|----------------|\n",
    "| 1 | **Guided import (recommended)** | **Data ‚Üí From Text/CSV ‚Üí** select the file ‚Üí choose **Delimiter** (comma, semicolon, tab) ‚Üí **Load** | Detects separator, encoding and shows a preview | Always, if the CSV was **not** generated by Excel |\n",
    "| 2 | **Set system default separator (Windows)** | **Control Panel ‚Üí Region ‚Üí Additional settings ‚Üí ‚ÄúList separator‚Äù ‚Üí** set `,` or `;` | Excel opens CSV correctly with double-click | Useful if you often open CSVs with the **same** delimiter |\n",
    "| 3 | **Rename file and change extension (trick)** | Rename `.csv` to `.txt`, then **Data ‚Üí From Text ‚Üí** follow the wizard | Forces you to choose the delimiter | Useful if Excel keeps putting everything into **one column** |\n",
    "| 4 | **Open Excel first, then ‚ÄúOpen ‚Üí File ‚Üí CSV‚Äù** | **File ‚Üí Open ‚Üí Browse ‚Üí** file type: **All files** ‚Üí pick the CSV ‚Üí the import window appears | Lets you choose encoding and separator | Alternative to importing from **Data** |\n",
    "| 5 | **Change Excel language/locale (optional)** | **File ‚Üí Options ‚Üí Advanced ‚Üí List separator** | Aligns Excel to the file format (e.g. US = `,`, Italy = `;`) | For mixed international / cloud usage |\n",
    "\n",
    "---\n",
    "üí¨ **Short explanation**\n",
    "\n",
    "When Excel shows everything in one column, it‚Äôs because:\n",
    "\n",
    "* the file delimiter (e.g. `,`)\n",
    "  ‚â†\n",
    "* the system list separator (e.g. `;` in Italy).\n",
    "\n",
    "üëâ Fix: use the import wizard, which lets you choose the delimiter and encoding (UTF-8 recommended).<br>\n",
    "After that choice, Excel immediately shows the correct grid and you can save as `.xlsx` if you want to keep it stable.\n",
    "\n",
    "üí° Practical tip for those who often work with Python or external CSVs:\n",
    "\n",
    "* use\n",
    "\n",
    "  ```python\n",
    "    df.to_csv(\"file.csv\", sep=\";\", encoding=\"utf-8-sig\")\n",
    "  ```\n",
    "\n",
    "  ‚Üí this way Excel (Italian version) opens it already ‚Äúas a grid‚Äù with no manual steps.\n",
    "* `utf-8-sig` is a variant of UTF-8 encoding commonly used so that Excel correctly recognizes CSV files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3651627f",
   "metadata": {},
   "source": [
    "# Input parameters of `pd.read_csv` function\n",
    "\n",
    "We have already mentioned the two **fundamental input arguments** of the pandas `read_csv` function: `sep` and `header`. They are **critical**:\n",
    "\n",
    "- `header=1` tells pandas to skip the first row of the file and use the second row as the header.<br>\n",
    "  If our CSVs do **not** have two header rows, or if the first file has a slightly different format from the others (spaces, separator, BOM, etc.), pandas will **misinterpret the columns**.\n",
    "- `sep=';'` can break all the data (if the file is actually a real CSV with commas!).<br>\n",
    "  Be careful: many ‚ÄúCSV files‚Äù actually use `sep=';'`!\n",
    "\n",
    "In reality, the `read_csv` function has **many other input arguments**, as you can see from the help in the next cell ‚Äî we will later go deeper into the **main** ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d3b132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_csv in module pandas.io.parsers.readers:\n",
      "\n",
      "read_csv(\n",
      "    filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]',\n",
      "    *,\n",
      "    sep: 'str | None | lib.NoDefault' = <no_default>,\n",
      "    delimiter: 'str | None | lib.NoDefault' = None,\n",
      "    header: \"int | Sequence[int] | None | Literal['infer']\" = 'infer',\n",
      "    names: 'Sequence[Hashable] | None | lib.NoDefault' = <no_default>,\n",
      "    index_col: 'IndexLabel | Literal[False] | None' = None,\n",
      "    usecols: 'UsecolsArgType' = None,\n",
      "    dtype: 'DtypeArg | None' = None,\n",
      "    engine: 'CSVEngine | None' = None,\n",
      "    converters: 'Mapping[Hashable, Callable] | None' = None,\n",
      "    true_values: 'list | None' = None,\n",
      "    false_values: 'list | None' = None,\n",
      "    skipinitialspace: 'bool' = False,\n",
      "    skiprows: 'list[int] | int | Callable[[Hashable], bool] | None' = None,\n",
      "    skipfooter: 'int' = 0,\n",
      "    nrows: 'int | None' = None,\n",
      "    na_values: 'Hashable | Iterable[Hashable] | Mapping[Hashable, Iterable[Hashable]] | None' = None,\n",
      "    keep_default_na: 'bool' = True,\n",
      "    na_filter: 'bool' = True,\n",
      "    verbose: 'bool | lib.NoDefault' = <no_default>,\n",
      "    skip_blank_lines: 'bool' = True,\n",
      "    parse_dates: 'bool | Sequence[Hashable] | None' = None,\n",
      "    infer_datetime_format: 'bool | lib.NoDefault' = <no_default>,\n",
      "    keep_date_col: 'bool | lib.NoDefault' = <no_default>,\n",
      "    date_parser: 'Callable | lib.NoDefault' = <no_default>,\n",
      "    date_format: 'str | dict[Hashable, str] | None' = None,\n",
      "    dayfirst: 'bool' = False,\n",
      "    cache_dates: 'bool' = True,\n",
      "    iterator: 'bool' = False,\n",
      "    chunksize: 'int | None' = None,\n",
      "    compression: 'CompressionOptions' = 'infer',\n",
      "    thousands: 'str | None' = None,\n",
      "    decimal: 'str' = '.',\n",
      "    lineterminator: 'str | None' = None,\n",
      "    quotechar: 'str' = '\"',\n",
      "    quoting: 'int' = 0,\n",
      "    doublequote: 'bool' = True,\n",
      "    escapechar: 'str | None' = None,\n",
      "    comment: 'str | None' = None,\n",
      "    encoding: 'str | None' = None,\n",
      "    encoding_errors: 'str | None' = 'strict',\n",
      "    dialect: 'str | csv.Dialect | None' = None,\n",
      "    on_bad_lines: 'str' = 'error',\n",
      "    delim_whitespace: 'bool | lib.NoDefault' = <no_default>,\n",
      "    low_memory: 'bool' = True,\n",
      "    memory_map: 'bool' = False,\n",
      "    float_precision: \"Literal['high', 'legacy'] | None\" = None,\n",
      "    storage_options: 'StorageOptions | None' = None,\n",
      "    dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>\n",
      ") -> 'DataFrame | TextFileReader'\n",
      "    Read a comma-separated values (csv) file into DataFrame.\n",
      "\n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "\n",
      "    Additional help can be found in the online docs for\n",
      "    `IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : str, path object or file-like object\n",
      "        Any valid string path is acceptable. The string could be a URL. Valid\n",
      "        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
      "        expected. A local file could be: file://localhost/path/to/table.csv.\n",
      "\n",
      "        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "\n",
      "        By file-like object, we refer to objects with a ``read()`` method, such as\n",
      "        a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
      "    sep : str, default ','\n",
      "        Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n",
      "        C engine cannot automatically detect\n",
      "        the separator, but the Python parsing engine can, meaning the latter will\n",
      "        be used and automatically detect the separator from only the first valid\n",
      "        row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n",
      "        In addition, separators longer than 1 character and different from\n",
      "        ``'\\s+'`` will be interpreted as regular expressions and will also force\n",
      "        the use of the Python parsing engine. Note that regex delimiters are prone\n",
      "        to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
      "    delimiter : str, optional\n",
      "        Alias for ``sep``.\n",
      "    header : int, Sequence of int, 'infer' or None, default 'infer'\n",
      "        Row number(s) containing column labels and marking the start of the\n",
      "        data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n",
      "        are passed the behavior is identical to ``header=0`` and column\n",
      "        names are inferred from the first line of the file, if column\n",
      "        names are passed explicitly to ``names`` then the behavior is identical to\n",
      "        ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "        replace existing names. The header can be a list of integers that\n",
      "        specify row locations for a :class:`~pandas.MultiIndex` on the columns\n",
      "        e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n",
      "        skipped (e.g. 2 in this example is skipped). Note that this\n",
      "        parameter ignores commented lines and empty lines if\n",
      "        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
      "        data rather than the first line of the file.\n",
      "    names : Sequence of Hashable, optional\n",
      "        Sequence of column labels to apply. If the file contains a header row,\n",
      "        then you should explicitly pass ``header=0`` to override the column names.\n",
      "        Duplicates in this list are not allowed.\n",
      "    index_col : Hashable, Sequence of Hashable or False, optional\n",
      "      Column(s) to use as row label(s), denoted either by column labels or column\n",
      "      indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n",
      "      will be formed for the row labels.\n",
      "\n",
      "      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
      "      column as the index, e.g., when you have a malformed file with delimiters at\n",
      "      the end of each line.\n",
      "    usecols : Sequence of Hashable or Callable, optional\n",
      "        Subset of columns to select, denoted either by column labels or column indices.\n",
      "        If list-like, all elements must either\n",
      "        be positional (i.e. integer indices into the document columns) or strings\n",
      "        that correspond to column names provided either by the user in ``names`` or\n",
      "        inferred from the document header row(s). If ``names`` are given, the document\n",
      "        header row(s) are not taken into account. For example, a valid list-like\n",
      "        ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
      "        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "        To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n",
      "        preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n",
      "        for columns in ``['foo', 'bar']`` order or\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "        for ``['bar', 'foo']`` order.\n",
      "\n",
      "        If callable, the callable function will be evaluated against the column\n",
      "        names, returning names where the callable function evaluates to ``True``. An\n",
      "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "        parsing time and lower memory usage.\n",
      "    dtype : dtype or dict of {Hashable : dtype}, optional\n",
      "        Data type(s) to apply to either the whole dataset or individual columns.\n",
      "        E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n",
      "        Use ``str`` or ``object`` together with suitable ``na_values`` settings\n",
      "        to preserve and not interpret ``dtype``.\n",
      "        If ``converters`` are specified, they will be applied INSTEAD\n",
      "        of ``dtype`` conversion.\n",
      "\n",
      "        .. versionadded:: 1.5.0\n",
      "\n",
      "            Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n",
      "            the default determines the ``dtype`` of the columns which are not explicitly\n",
      "            listed.\n",
      "    engine : {'c', 'python', 'pyarrow'}, optional\n",
      "        Parser engine to use. The C and pyarrow engines are faster, while the python engine\n",
      "        is currently more feature-complete. Multithreading is currently only supported by\n",
      "        the pyarrow engine.\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "            The 'pyarrow' engine was added as an *experimental* engine, and some features\n",
      "            are unsupported, or may not work correctly, with this engine.\n",
      "    converters : dict of {Hashable : Callable}, optional\n",
      "        Functions for converting values in specified columns. Keys can either\n",
      "        be column labels or column indices.\n",
      "    true_values : list, optional\n",
      "        Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\n",
      "    false_values : list, optional\n",
      "        Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\n",
      "    skipinitialspace : bool, default False\n",
      "        Skip spaces after delimiter.\n",
      "    skiprows : int, list of int or Callable, optional\n",
      "        Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n",
      "        at the start of the file.\n",
      "\n",
      "        If callable, the callable function will be evaluated against the row\n",
      "        indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n",
      "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "    skipfooter : int, default 0\n",
      "        Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\n",
      "    nrows : int, optional\n",
      "        Number of rows of file to read. Useful for reading pieces of large files.\n",
      "    na_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n",
      "        Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n",
      "        per-column ``NA`` values.  By default the following values are interpreted as\n",
      "        ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n",
      "        \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n",
      "        \"n/a\", \"nan\", \"null \".\n",
      "\n",
      "    keep_default_na : bool, default True\n",
      "        Whether or not to include the default ``NaN`` values when parsing the data.\n",
      "        Depending on whether ``na_values`` is passed in, the behavior is as follows:\n",
      "\n",
      "        * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n",
      "          is appended to the default ``NaN`` values used for parsing.\n",
      "        * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n",
      "          the default ``NaN`` values are used for parsing.\n",
      "        * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n",
      "          the ``NaN`` values specified ``na_values`` are used for parsing.\n",
      "        * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n",
      "          strings will be parsed as ``NaN``.\n",
      "\n",
      "        Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n",
      "        ``na_values`` parameters will be ignored.\n",
      "    na_filter : bool, default True\n",
      "        Detect missing value markers (empty strings and the value of ``na_values``). In\n",
      "        data without any ``NA`` values, passing ``na_filter=False`` can improve the\n",
      "        performance of reading a large file.\n",
      "    verbose : bool, default False\n",
      "        Indicate number of ``NA`` values placed in non-numeric columns.\n",
      "\n",
      "        .. deprecated:: 2.2.0\n",
      "    skip_blank_lines : bool, default True\n",
      "        If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\n",
      "    parse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n",
      "        The behavior is as follows:\n",
      "\n",
      "        * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n",
      "          ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n",
      "        * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n",
      "          each as a separate date column.\n",
      "        * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n",
      "          as a single date column. Values are joined with a space before parsing.\n",
      "        * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n",
      "          result 'foo'. Values are joined with a space before parsing.\n",
      "\n",
      "        If a column or index cannot be represented as an array of ``datetime``,\n",
      "        say because of an unparsable value or a mixture of timezones, the column\n",
      "        or index will be returned unaltered as an ``object`` data type. For\n",
      "        non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n",
      "        :func:`~pandas.read_csv`.\n",
      "\n",
      "        Note: A fast-path exists for iso8601-formatted dates.\n",
      "    infer_datetime_format : bool, default False\n",
      "        If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n",
      "        format of the ``datetime`` strings in the columns, and if it can be inferred,\n",
      "        switch to a faster method of parsing them. In some cases this can increase\n",
      "        the parsing speed by 5-10x.\n",
      "\n",
      "        .. deprecated:: 2.0.0\n",
      "            A strict version of this argument is now the default, passing it has no effect.\n",
      "\n",
      "    keep_date_col : bool, default False\n",
      "        If ``True`` and ``parse_dates`` specifies combining multiple columns then\n",
      "        keep the original columns.\n",
      "    date_parser : Callable, optional\n",
      "        Function to use for converting a sequence of string columns to an array of\n",
      "        ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "        conversion. pandas will try to call ``date_parser`` in three different ways,\n",
      "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "        (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n",
      "        string values from the columns defined by ``parse_dates`` into a single array\n",
      "        and pass that; and 3) call ``date_parser`` once for each row using one or\n",
      "        more strings (corresponding to the columns defined by ``parse_dates``) as\n",
      "        arguments.\n",
      "\n",
      "        .. deprecated:: 2.0.0\n",
      "           Use ``date_format`` instead, or read in as ``object`` and then apply\n",
      "           :func:`~pandas.to_datetime` as-needed.\n",
      "    date_format : str or dict of column -> format, optional\n",
      "        Format to use for parsing dates when used in conjunction with ``parse_dates``.\n",
      "        The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n",
      "        `strftime documentation\n",
      "        <https://docs.python.org/3/library/datetime.html\n",
      "        #strftime-and-strptime-behavior>`_ for more information on choices, though\n",
      "        note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n",
      "        You can also pass:\n",
      "\n",
      "        - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n",
      "            time string (not necessarily in exactly the same format);\n",
      "        - \"mixed\", to infer the format for each element individually. This is risky,\n",
      "            and you should probably use it along with `dayfirst`.\n",
      "\n",
      "        .. versionadded:: 2.0.0\n",
      "    dayfirst : bool, default False\n",
      "        DD/MM format dates, international and European format.\n",
      "    cache_dates : bool, default True\n",
      "        If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n",
      "        conversion. May produce significant speed-up when parsing duplicate\n",
      "        date strings, especially ones with timezone offsets.\n",
      "\n",
      "    iterator : bool, default False\n",
      "        Return ``TextFileReader`` object for iteration or getting chunks with\n",
      "        ``get_chunk()``.\n",
      "    chunksize : int, optional\n",
      "        Number of lines to read from the file per chunk. Passing a value will cause the\n",
      "        function to return a ``TextFileReader`` object for iteration.\n",
      "        See the `IO Tools docs\n",
      "        <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "        for more information on ``iterator`` and ``chunksize``.\n",
      "\n",
      "    compression : str or dict, default 'infer'\n",
      "        For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n",
      "        path-like, then detect compression from the following extensions: '.gz',\n",
      "        '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
      "        (otherwise no compression).\n",
      "        If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n",
      "        Set to ``None`` for no decompression.\n",
      "        Can also be a dict with key ``'method'`` set\n",
      "        to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
      "        other key-value pairs are forwarded to\n",
      "        ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      "        ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n",
      "        ``tarfile.TarFile``, respectively.\n",
      "        As an example, the following could be passed for Zstandard decompression using a\n",
      "        custom compression dictionary:\n",
      "        ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n",
      "\n",
      "        .. versionadded:: 1.5.0\n",
      "            Added support for `.tar` files.\n",
      "\n",
      "        .. versionchanged:: 1.4.0 Zstandard support.\n",
      "\n",
      "    thousands : str (length 1), optional\n",
      "        Character acting as the thousands separator in numerical values.\n",
      "    decimal : str (length 1), default '.'\n",
      "        Character to recognize as decimal point (e.g., use ',' for European data).\n",
      "    lineterminator : str (length 1), optional\n",
      "        Character used to denote a line break. Only valid with C parser.\n",
      "    quotechar : str (length 1), optional\n",
      "        Character used to denote the start and end of a quoted item. Quoted\n",
      "        items can include the ``delimiter`` and it will be ignored.\n",
      "    quoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n",
      "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n",
      "        ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n",
      "        characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n",
      "        or ``lineterminator``.\n",
      "    doublequote : bool, default True\n",
      "       When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n",
      "       whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n",
      "       field as a single ``quotechar`` element.\n",
      "    escapechar : str (length 1), optional\n",
      "        Character used to escape other characters.\n",
      "    comment : str (length 1), optional\n",
      "        Character indicating that the remainder of line should not be parsed.\n",
      "        If found at the beginning\n",
      "        of a line, the line will be ignored altogether. This parameter must be a\n",
      "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "        fully commented lines are ignored by the parameter ``header`` but not by\n",
      "        ``skiprows``. For example, if ``comment='#'``, parsing\n",
      "        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n",
      "        treated as the header.\n",
      "    encoding : str, optional, default 'utf-8'\n",
      "        Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n",
      "        standard encodings\n",
      "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
      "\n",
      "    encoding_errors : str, optional, default 'strict'\n",
      "        How encoding errors are treated. `List of possible values\n",
      "        <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "    dialect : str or csv.Dialect, optional\n",
      "        If provided, this parameter will override values (default or not) for the\n",
      "        following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n",
      "        ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n",
      "        override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n",
      "        documentation for more details.\n",
      "    on_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n",
      "        Specifies what to do upon encountering a bad line (a line with too many fields).\n",
      "        Allowed values are :\n",
      "\n",
      "        - ``'error'``, raise an Exception when a bad line is encountered.\n",
      "        - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n",
      "        - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n",
      "\n",
      "        .. versionadded:: 1.3.0\n",
      "\n",
      "        .. versionadded:: 1.4.0\n",
      "\n",
      "            - Callable, function with signature\n",
      "              ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n",
      "              bad line. ``bad_line`` is a list of strings split by the ``sep``.\n",
      "              If the function returns ``None``, the bad line will be ignored.\n",
      "              If the function returns a new ``list`` of strings with more elements than\n",
      "              expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n",
      "              Only supported when ``engine='python'``\n",
      "\n",
      "        .. versionchanged:: 2.2.0\n",
      "\n",
      "            - Callable, function with signature\n",
      "              as described in `pyarrow documentation\n",
      "              <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n",
      "              #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n",
      "\n",
      "    delim_whitespace : bool, default False\n",
      "        Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n",
      "        used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n",
      "        is set to ``True``, nothing should be passed in for the ``delimiter``\n",
      "        parameter.\n",
      "\n",
      "        .. deprecated:: 2.2.0\n",
      "            Use ``sep=\"\\s+\"`` instead.\n",
      "    low_memory : bool, default True\n",
      "        Internally process the file in chunks, resulting in lower memory use\n",
      "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "        types either set ``False``, or specify the type with the ``dtype`` parameter.\n",
      "        Note that the entire file is read into a single :class:`~pandas.DataFrame`\n",
      "        regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n",
      "        chunks. (Only valid with C parser).\n",
      "    memory_map : bool, default False\n",
      "        If a filepath is provided for ``filepath_or_buffer``, map the file object\n",
      "        directly onto memory and access the data directly from there. Using this\n",
      "        option can improve performance because there is no longer any I/O overhead.\n",
      "    float_precision : {'high', 'legacy', 'round_trip'}, optional\n",
      "        Specifies which converter the C engine should use for floating-point\n",
      "        values. The options are ``None`` or ``'high'`` for the ordinary converter,\n",
      "        ``'legacy'`` for the original lower precision pandas converter, and\n",
      "        ``'round_trip'`` for the round-trip converter.\n",
      "\n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "        are forwarded to ``urllib.request.Request`` as header options. For other\n",
      "        URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      "        forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      "        details, and for more examples on storage options refer `here\n",
      "        <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      "        highlight=storage_options#reading-writing-remote-files>`_.\n",
      "\n",
      "    dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n",
      "        Back-end data type applied to the resultant :class:`DataFrame`\n",
      "        (still experimental). Behaviour is as follows:\n",
      "\n",
      "        * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n",
      "          (default).\n",
      "        * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n",
      "          DataFrame.\n",
      "\n",
      "        .. versionadded:: 2.0\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or TextFileReader\n",
      "        A comma-separated values (csv) file is returned as two-dimensional\n",
      "        data structure with labeled axes.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "    read_table : Read general delimited file into DataFrame.\n",
      "    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b128b8-189d-4145-a0f1-9bf4d943e1a9",
   "metadata": {
    "id": "02b128b8-189d-4145-a0f1-9bf4d943e1a9"
   },
   "source": [
    "> Reading a *csv* file is often one of the **first real difficulties** that those who start using pandas run into.\n",
    "> The reason is that csv reading is simple in theory but in practice often the data is dirty, mixed, encoded differently, irregular.\n",
    "> This makes the initial approach a bit **tricky**, a source of **not a few frustrations**, for two reasons:\n",
    "> - the many and non‚Äëtrivial arguments of the `read_csv` function\n",
    "> - the **irregularities** in the data in the files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5394f7a9-8061-400e-a1af-3a9a87099765",
   "metadata": {
    "id": "5394f7a9-8061-400e-a1af-3a9a87099765"
   },
   "source": [
    "## The mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c16f43-e516-48ac-95b9-1534a96430a2",
   "metadata": {
    "id": "f1c16f43-e516-48ac-95b9-1534a96430a2"
   },
   "source": [
    "The function `pd.read_csv` does an **automatic mapping** of CSV data into pandas, as described here:\n",
    "\n",
    "![](how_pandas_infers_CSV_datatypes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e93810-ba9e-41ed-be16-49f71508751d",
   "metadata": {
    "id": "a4e93810-ba9e-41ed-be16-49f71508751d"
   },
   "source": [
    "There is a problem that is not mentioned in the slide: the `read_csv` function **often fails to infer categorical variables** (when they are present in the CSV file as strings), so they are imported as `object`, the generic pandas string data type. As you can see:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9292db96-52aa-4873-ae2b-79451efae594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T09:32:27.750380Z",
     "iopub.status.busy": "2025-10-21T09:32:27.750139Z",
     "iopub.status.idle": "2025-10-21T09:32:27.754749Z",
     "shell.execute_reply": "2025-10-21T09:32:27.754389Z",
     "shell.execute_reply.started": "2025-10-21T09:32:27.750364Z"
    },
    "id": "9292db96-52aa-4873-ae2b-79451efae594",
    "outputId": "0dc7062c-bade-4afc-9dfe-2b8d559a6321"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Income       float64\n",
       "Limit          int64\n",
       "Rating         int64\n",
       "Cards          int64\n",
       "Age            int64\n",
       "Education      int64\n",
       "Gender        object\n",
       "Student       object\n",
       "Married       object\n",
       "Ethnicity     object\n",
       "Balance        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_credit.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99baa579-39ee-4f63-9bfc-4e8c39b72841",
   "metadata": {
    "id": "99baa579-39ee-4f63-9bfc-4e8c39b72841"
   },
   "source": [
    "The variables `Gender`, `Student`, `Married` and `Ethnicity` are **categorical** in nature: that is, they can only take on a small, fixed number of values.\n",
    "Other variables such as `Education`, if imported as text, could potentially take on an infinite number of values.\n",
    "\n",
    "Each cell of the variable, if imported as `object`, **points to a string in memory, often duplicated several times**.\n",
    "\n",
    "It is therefore necessary to **convert** these variables to the `category` format available from *pandas* (it does not exist in base Python), as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65df6bda-6ddc-4f71-a4f3-c5374269103f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T09:40:41.996556Z",
     "iopub.status.busy": "2025-10-21T09:40:41.996253Z",
     "iopub.status.idle": "2025-10-21T09:40:42.001137Z",
     "shell.execute_reply": "2025-10-21T09:40:42.000829Z",
     "shell.execute_reply.started": "2025-10-21T09:40:41.996542Z"
    },
    "id": "65df6bda-6ddc-4f71-a4f3-c5374269103f",
    "outputId": "9bfc1afa-b4bc-4f74-cc46-88085e8ba564"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Income        float64\n",
       "Limit           int64\n",
       "Rating          int64\n",
       "Cards           int64\n",
       "Age             int64\n",
       "Education       int64\n",
       "Gender       category\n",
       "Student        object\n",
       "Married        object\n",
       "Ethnicity      object\n",
       "Balance         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_credit['Gender'] = df_credit['Gender'].astype('category')\n",
    "df_credit.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b894adb0-45e6-47ef-b6d2-993f4163a6de",
   "metadata": {
    "id": "b894adb0-45e6-47ef-b6d2-993f4163a6de"
   },
   "source": [
    "The *pandas* method `astype('category')`:\n",
    "\n",
    "* creates an **internal encoding table** (the ‚Äúlevels‚Äù or ‚Äúcategories‚Äù),\n",
    "* represents the column as **internal integers** (0, 1, 2, ‚Ä¶) instead of repeated strings.\n",
    "\n",
    "The behavior of `category` is similar to the **factor** in R.\n",
    "\n",
    "üöÄ <font size=\"4\">**Main advantages** (of `category`)</font><br>\n",
    "\n",
    "üîπ **Memory efficiency**<br>\n",
    "Each value becomes **an integer**, and the string is **stored only once in the category table**.<br>\n",
    "üëâ On large datasets, the saving can reach 70‚Äì90% of RAM.\n",
    "Example:\n",
    "\n",
    "```python\n",
    "    df['citt√†'].memory_usage(deep=True)\n",
    "    df['citt√†'].astype('category').memory_usage(deep=True)\n",
    "```\n",
    "\n",
    "The second one takes much less space.\n",
    "\n",
    "üîπ **Processing speed**<br>\n",
    "Many pandas operations (`groupby`, `sort`, `value_counts`, `merges`) become **much faster**; in fact:\n",
    "\n",
    "* comparing integers is faster than comparing strings,\n",
    "* grouping and join algorithms work on numeric codes.\n",
    "  üí° Typical case: `df.groupby('categoria').agg(...)` is much faster if `categoria` is `category`.\n",
    "\n",
    "üîπ **Semantic meaning**<br>\n",
    "A categorical variable has **a finite and known number of levels**.<br>\n",
    "This is useful to:\n",
    "\n",
    "* ensure that ‚Äúout-of-list‚Äù values don‚Äôt appear (e.g. ‚ÄòFemmina‚Äô vs ‚ÄòF‚Äô),\n",
    "* keep the logical or hierarchical order (e.g. Low < Medium < High).<br>\n",
    "\n",
    "You can also explicitly define the order like this:\n",
    "\n",
    "```python\n",
    "    df['livello'] = pd.Categorical(df['livello'], categories=['basso','medio','alto'], ordered=True)\n",
    "```\n",
    "\n",
    "‚Üí useful for comparisons, sorting, or encoding in machine learning.\n",
    "\n",
    "üîπ **ML and preprocessing compatibility**<br>\n",
    "Many machine-learning algorithms or encoders (e.g. `sklearn.preprocessing.OrdinalEncoder`, `OneHotEncoder`) detect `category` and immediately treat it as a discrete variable, **without first having to convert it from object**.\n",
    "\n",
    "---\n",
    "\n",
    "‚ö†Ô∏è **When is `category` not convenient?**\n",
    "\n",
    "* if the column has **many unique values** (e.g. a unique code or a customer ID), the conversion brings no benefit: the category table would be as large as the column itself.\n",
    "* if values are frequently changed (adding new categories), the `category` type is less flexible.\n",
    "\n",
    "üîç **Practical example**:\n",
    "\n",
    "```python\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({\n",
    "        'sesso': ['M','F','M','F','F']*100000\n",
    "    })\n",
    "    print(df['sesso'].memory_usage(deep=True))   # object\n",
    "    df['sesso'] = df['sesso'].astype('category')\n",
    "    print(df['sesso'].memory_usage(deep=True))   # category (much less!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "> üí¨ In brief\n",
    "\n",
    "| Aspect           | `object`             | `category`                      |\n",
    "|------------------|----------------------|----------------------------------|\n",
    "| Base type        | Python strings       | integer codes + category list   |\n",
    "| Memory           | High                 | Very low                        |\n",
    "| Speed            | Slower               | Faster                          |\n",
    "| Semantics        | Free text            | Finite discrete values          |\n",
    "| Machine Learning | Must be encoded first| Already ready / suitable        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefdf89b-2214-4ed7-8b59-49939285ecfe",
   "metadata": {
    "id": "aefdf89b-2214-4ed7-8b59-49939285ecfe"
   },
   "source": [
    "## The arguments of the `pd.read_csv` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1fd02f-b8fe-4d70-9bbe-235863bef8d5",
   "metadata": {
    "id": "9b1fd02f-b8fe-4d70-9bbe-235863bef8d5"
   },
   "source": [
    "[Here](https://github.com/nikitaprasad21/ML-Cheat-Codes/blob/main/Data-Gathering/CSV-(Comma-Separated-Values)-Files/csv_file_cheatcodes.ipynb) is an excellent notebook that **illustrates the various arguments** of `pd.read_csv` ‚Äì **downloaded** in the directory of this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0786d-2382-40f2-8247-5b828130b23d",
   "metadata": {
    "id": "20a0786d-2382-40f2-8247-5b828130b23d"
   },
   "source": [
    "## The `Unnamed: 0` column\n",
    "See [this chat](https://chatgpt.com/share/68f74bca-554c-8012-a844-7260ce18391d) of ChatGPT. Ask for translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36889a3b-f5e6-406e-8905-83d23e15a66b",
   "metadata": {
    "id": "36889a3b-f5e6-406e-8905-83d23e15a66b"
   },
   "source": [
    "# Frequent problems when loading CSV files in pandas.\n",
    "\n",
    "Here is a list of the **most common problems** you encounter when loading a csv with `pandas.read_csv()`, together with **typical causes** and **solutions**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea061b7-be45-4711-af0a-04cbac19db08",
   "metadata": {
    "id": "bea061b7-be45-4711-af0a-04cbac19db08"
   },
   "source": [
    "üß© **1. Columns ‚ÄúUnnamed: 0‚Äù or ‚ÄúUnnamed: n‚Äù** ‚Äì already seen before\n",
    "\n",
    "<u>Problem</u>: an unwanted column called `Unnamed: 0` appears.<br>\n",
    "<u>Cause</u>: often the CSV includes an index saved from a previous `DataFrame.to_csv()` (i.e. `index=True` by default).<br>\n",
    "<u>Solution</u>:\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", index_col=0)\n",
    "    # oppure\n",
    "    pd.read_csv(\"file.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ca7d0b-38e8-4a23-99b8-5a8ff7749428",
   "metadata": {
    "id": "11ca7d0b-38e8-4a23-99b8-5a8ff7749428"
   },
   "source": [
    "‚öôÔ∏è **2. Wrong delimiters** ‚Äì already seen before\n",
    "\n",
    "<u>Problem</u>: the file is not split correctly (all columns end up in a single one).<br> <u>Cause</u>: the separator is not a comma, but a semicolon `;`, a tab `\\t`, or something else.<br> <u>Solution</u>:\n",
    "\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", sep=\";\")      # for European-style CSV\n",
    "    pd.read_csv(\"file.csv\", sep=\"\\t\")     # for TSV files\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "A TSV (*Tab-Separated Values*) is basically a CSV, but instead of `,` or `;` it uses the tab character `\\t` as the field separator.\n",
    "You can also detect it automatically:\n",
    "\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", sep=None, engine=\"python\")\n",
    "```\n",
    "\n",
    "üß© When do you use a TSV?\n",
    "\n",
    "* when the data contains lots of commas or semicolons (e.g. text descriptions).\n",
    "* when the file is exported by Unix systems or databases (e.g. PostgreSQL COPY TO, Excel ‚Üí ‚ÄúText (tab delimited)‚Äù).\n",
    "* when you want to avoid ambiguity between decimal separators and field separators.\n",
    "\n",
    "---\n",
    "\n",
    "The `engine` parameter in `pandas.read_csv()` is used to tell pandas which **parsing engine** to use to read and interpret the CSV file.<br>\n",
    "In practice, pandas has **two different parser engines** that do the same job (read the file and turn it into a DataFrame), but **with different features and performance**.\n",
    "\n",
    "1Ô∏è‚É£ **`engine=\"c\"`** ‚Üí the ‚Äúfast‚Äù parser (default)\n",
    "\n",
    "* written in C ‚Üí very fast\n",
    "* it is the default in almost all cases\n",
    "* great for clean, regular files\n",
    "* but‚Ä¶ it is less flexible: it doesn‚Äôt support every option and may fail on ‚Äúmessy‚Äù or complex CSVs\n",
    "\n",
    "Usage example:\n",
    "\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", engine=\"c\")\n",
    "```\n",
    "\n",
    "2Ô∏è‚É£ **`engine=\"python\"`** ‚Üí the ‚Äúrobust‚Äù parser\n",
    "\n",
    "* written in pure Python ‚Üí slower, but more tolerant\n",
    "* supports options that the C parser doesn‚Äôt handle well, such as:\n",
    "\n",
    "  * `sep=None` (i.e. **automatic separator detection**),\n",
    "  * multiple or irregular delimiters,\n",
    "  * malformed lines (`on_bad_lines`),\n",
    "  * complex quotes and special characters.\n",
    "\n",
    "Usage example:\n",
    "\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", sep=None, engine=\"python\")\n",
    "```\n",
    "\n",
    "üëâ Here pandas tries to guess the separator automatically (`,`, `;`, `\\t`, etc.) by looking at the first rows.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a39c444-17bf-457e-aed9-9ba843517807",
   "metadata": {
    "id": "4a39c444-17bf-457e-aed9-9ba843517807"
   },
   "source": [
    "Let‚Äôs go back to the list of problems and solutions of `read_csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee6a66-6045-429c-abfc-2e04eae898e9",
   "metadata": {
    "id": "f0ee6a66-6045-429c-abfc-2e04eae898e9"
   },
   "source": [
    "üî§ **3. Wrong encoding**\n",
    "\n",
    "<u>Problem</u>: accented characters or special symbols appear as ÔøΩ or raise `UnicodeDecodeError`.<br>\n",
    "<u>Cause</u>: the file is not in `UTF‚Äë8` but in `latin1`, `cp1252`, etc.<br>\n",
    "<u>Solution</u>:\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", encoding=\"latin1\")\n",
    "```\n",
    "\n",
    "> What is `latin1`?\n",
    "> - `latin1` (or `ISO‚Äë8859‚Äë1`) is a 1‚Äëbyte (8‚Äëbit) encoding **widely used in Western Europe** before UTF‚Äë8 became standard.\n",
    "> - It supports many Western European characters:\n",
    ">   - Italian accented letters: `√†`, `√®`, `√©`, `√¨`, `√≤`, `√π`, ...\n",
    ">   - Spanish/French/Portuguese letters: `√±`, `√ß`, `√°`, `√©`, `√µ`, ...\n",
    ">   - German umlaut vowels: `√§`, `√∂`, `√º`, ...\n",
    ">   - Nordic `√∏`, `√•`\n",
    ">\n",
    "> **BUT** latin1 does NOT support:\n",
    "> - emoji\n",
    "> - euro symbol `‚Ç¨`\n",
    "> - Greek, Cyrillic, Arabic, Chinese, etc.\n",
    ">\n",
    "> **So `latin1` is basically ‚ÄúWestern Europe in the 90s‚Äù.** If we encounter characters outside that set, pandas either shows the replacement char or throws `UnicodeDecodeError`.\n",
    "\n",
    "<img src=\"ascii_latin1_utf_8.png\" alt=\"image\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b34055",
   "metadata": {
    "id": "f6b34055"
   },
   "source": [
    "Test of **some errors**, in various steps:<br>\n",
    "1. we create a `DataFrame` with typical `latin1` characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25fe2780",
   "metadata": {
    "id": "25fe2780"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"ID\": [1, 2, 3],\n",
    "    \"Nome\": [\"Andr√©\", \"Jos√©\", \"Ana Mar√≠a\"],\n",
    "    \"Citt√†\": [\"Torino\", \"M√°laga\", \"Z√ºrich\"],\n",
    "    \"Note\": [\n",
    "        \"pagato 50$ gi√† fatturato\",              # 'latin1' extends ASCII, that contained character '$', so also 'latin1' accepts it\n",
    "        \"a√±o siguiente -> revisi√≥n t√©cnica\",\n",
    "        \"pi√π vecchio -> gi√† sostituito\"\n",
    "    ]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1558aca4",
   "metadata": {
    "id": "1558aca4"
   },
   "source": [
    "2. we save the `DataFrame` to a `latin1` (ISO‚Äë8859‚Äë1) CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1330ba6",
   "metadata": {
    "id": "b1330ba6",
    "outputId": "0282d025-b24a-41e8-c90d-36d2d411e565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creato file clienti_latin1.csv in encoding latin1\n"
     ]
    }
   ],
   "source": [
    "file_name = \"clienti_latin1.csv\"\n",
    "df.to_csv(\n",
    "    file_name,\n",
    "    index=False,\n",
    "    sep=\";\",             # we also set the ';' separator so it is even more realistic \"European style\"\n",
    "    encoding=\"latin1\"    # <-- key point\n",
    ")\n",
    "\n",
    "print(f\"Creato file {file_name} in encoding latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970238e5",
   "metadata": {
    "id": "970238e5"
   },
   "source": [
    "3. we try to read it again WITHOUT specifying the encoding.<br>\n",
    "This is what a distracted user usually does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1726235f",
   "metadata": {
    "id": "1726235f",
    "outputId": "3ed59a07-93ef-482c-acbd-f950644644f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Errore di decodifica previsto leggendo senza encoding esplicito:\n",
      "'utf-8' codec can't decode byte 0xe0 in position 12: invalid continuation byte\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_fail = pd.read_csv(file_name, sep=\";\")  # no encoding passed, i.e. default = UTF-8\n",
    "    print(\"Letto senza errori?! Ecco le prime righe:\")\n",
    "    print(df_fail.head())\n",
    "except UnicodeDecodeError as e:\n",
    "    print(\"‚ö†Ô∏è Errore di decodifica previsto leggendo senza encoding esplicito:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0545da",
   "metadata": {
    "id": "ae0545da"
   },
   "source": [
    "4. it raises an error: we want to read a `latin1` file as `utf‚Äë8`.<br>\n",
    "correct solution: we read specifying `encoding=latin1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d90885c5",
   "metadata": {
    "id": "d90885c5",
    "outputId": "2e42539d-4742-4268-f81b-da3c9d1d3edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Lettura corretta con encoding='latin1':\n",
      "   ID       Nome   Citt√†                               Note\n",
      "0   1      Andr√©  Torino           pagato 50$ gi√† fatturato\n",
      "1   2       Jos√©  M√°laga  a√±o siguiente -> revisi√≥n t√©cnica\n",
      "2   3  Ana Mar√≠a  Z√ºrich      pi√π vecchio -> gi√† sostituito\n"
     ]
    }
   ],
   "source": [
    "df_ok = pd.read_csv(file_name, sep=\";\", encoding=\"latin1\")\n",
    "print(\"\\nüí° Lettura corretta con encoding='latin1':\")\n",
    "print(df_ok.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02c27f7",
   "metadata": {
    "id": "e02c27f7"
   },
   "source": [
    "5. and what happens with the following dataframe, which contains the `‚Ç¨` character (instead of `$`), which is part of neither `Ascii` nor `latin1`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e670c2b",
   "metadata": {
    "id": "4e670c2b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"ID\": [1, 2, 3],\n",
    "    \"Nome\": [\"Andr√©\", \"Jos√©\", \"Ana Mar√≠a\"],\n",
    "    \"Citt√†\": [\"Torino\", \"M√°laga\", \"Z√ºrich\"],\n",
    "    \"Note\": [\n",
    "        \"pagato 50‚Ç¨ gi√† fatturato\",\n",
    "        \"a√±o siguiente -> revisi√≥n t√©cnica\",\n",
    "        \"pi√π vecchio -> gi√† sostituito\"\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d1936",
   "metadata": {},
   "source": [
    "This code goes wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1adf1fb",
   "metadata": {
    "id": "e1adf1fb",
    "outputId": "b38a7ab8-e8a3-4b43-908c-e10cfa350af2"
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'latin-1' codec can't encode character '\\u20ac' in position 24: ordinal not in range(256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclienti_latin1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m      3\u001b[0m     file_name,\n\u001b[0;32m      4\u001b[0m     index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m     sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m,             \u001b[38;5;66;03m# we also set the ';' separator so it is even more realistic \"European style\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m\"\u001b[39m    \u001b[38;5;66;03m# <-- key point\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreato file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in encoding latin1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3986\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3975\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3977\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3978\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3979\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3983\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3984\u001b[0m )\n\u001b[1;32m-> 3986\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3987\u001b[0m     path_or_buf,\n\u001b[0;32m   3988\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3989\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3990\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3991\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3992\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3993\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3994\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3995\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3996\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3997\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3998\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3999\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   4000\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   4001\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   4002\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   4003\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:270\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:275\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_to_save_header:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_header()\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_body()\n",
      "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:313\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end_i:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_chunk(start_i, end_i)\n",
      "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:324\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    321\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(res\u001b[38;5;241m.\u001b[39m_iter_column_arrays())\n\u001b[0;32m    323\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_index[slicer]\u001b[38;5;241m.\u001b[39m_get_values_for_csv(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n\u001b[1;32m--> 324\u001b[0m libwriters\u001b[38;5;241m.\u001b[39mwrite_csv_rows(\n\u001b[0;32m    325\u001b[0m     data,\n\u001b[0;32m    326\u001b[0m     ix,\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlevels,\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcols,\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter,\n\u001b[0;32m    330\u001b[0m )\n",
      "File \u001b[1;32mpandas/_libs/writers.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'latin-1' codec can't encode character '\\u20ac' in position 24: ordinal not in range(256)"
     ]
    }
   ],
   "source": [
    "file_name = \"clienti_latin1.csv\"\n",
    "df.to_csv(\n",
    "    file_name,\n",
    "    index=False,\n",
    "    sep=\";\",             # we also set the ';' separator so it is even more realistic \"European style\"\n",
    "    encoding=\"latin1\"    # <-- key point\n",
    ")\n",
    "\n",
    "print(f\"Creato file {file_name} in encoding latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23514c60",
   "metadata": {
    "id": "23514c60"
   },
   "source": [
    "6. it raises an error, because `latin1` does not contain the `‚Ç¨` character.<br>\n",
    "We must write in `utf‚Äë8`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bb81607",
   "metadata": {
    "id": "7bb81607",
    "outputId": "33c0e3e5-7a32-4cb8-ff8f-3fd2a96f0b74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creato file clienti_latin1.csv in encoding latin1\n"
     ]
    }
   ],
   "source": [
    "file_name = \"clienti_latin1.csv\"\n",
    "df.to_csv(\n",
    "    file_name,\n",
    "    index=False,\n",
    "    sep=\";\",             # we also set the ';' separator so it is even more realistic \"European style\"\n",
    "    encoding=\"utf-8\"    # <-- key point\n",
    ")\n",
    "\n",
    "print(f\"Creato file {file_name} in encoding latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e36a52b-1484-4355-9b4d-c85f6a1d80b6",
   "metadata": {
    "id": "7e36a52b-1484-4355-9b4d-c85f6a1d80b6"
   },
   "source": [
    "üìâ **4. Wrong data type**\n",
    "\n",
    "<u>Problem</u>: numeric columns imported as strings (`object`).<br>\n",
    "<u>Cause</u>: presence of thousand separators, symbols, or empty cells.<br>\n",
    "<u>Solution</u>:\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", thousands=\".\", decimal=\",\")\n",
    "```\n",
    "or after the read:\n",
    "```python\n",
    "    df[\"col\"] = pd.to_numeric(df[\"col\"], errors=\"coerce\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e15596c",
   "metadata": {
    "id": "0e15596c",
    "outputId": "a85ec4d1-90a5-41e3-f779-013590f79954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Creato file CSV 'prezzi_legacy.csv' con separatori migliaia '.' e decimali ','.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ###############################################\n",
    "# 1. CSV CREATION with EUROPEAN FORMATTING      #\n",
    "# ###############################################\n",
    "\n",
    "# Notes:\n",
    "# - \"1.234,50\" = one thousand two hundred thirtyfour comma fifty\n",
    "# - \"2.000\"    = two thousands (integer)\n",
    "# - \"\"         = empty cell (missing value)\n",
    "\n",
    "df_orig = pd.DataFrame({\n",
    "    \"Prodotto\": [\"A123\", \"B777\", \"C900\", \"D010\"],\n",
    "    \"PrezzoUnitario\": [\"1.234,50\", \"99,99\", \"\", \"2.000,00\"],\n",
    "    \"Quantit√†\": [\"1.000\", \"250\", \"\", \"1.500\"]\n",
    "})\n",
    "\n",
    "csv_name = \"prezzi_legacy.csv\"\n",
    "\n",
    "# We save as CSV with ';' because it is very common in Italian administrative exports\n",
    "df_orig.to_csv(\n",
    "    csv_name,\n",
    "    index=False,\n",
    "    sep=\";\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(f\"[OK] Creato file CSV '{csv_name}' con separatori migliaia '.' e decimali ','.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bb500b0",
   "metadata": {
    "id": "5bb500b0",
    "outputId": "b2cfea58-9393-4ad8-9e77-f3ca316c23fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LETTURA SBAGLIATA (senza thousands/decimal) ===\n",
      "\n",
      "DataFrame letto (sbagliato):\n",
      "  Prodotto PrezzoUnitario  Quantit√†\n",
      "0     A123       1.234,50       1.0\n",
      "1     B777          99,99     250.0\n",
      "2     C900            NaN       NaN\n",
      "3     D010       2.000,00       1.5\n",
      "\n",
      "Tipi di dato dopo lettura sbagliata:\n",
      "Prodotto           object\n",
      "PrezzoUnitario     object\n",
      "Quantit√†          float64\n",
      "dtype: object\n",
      "\n",
      "Provo a sommare la colonna Quantit√† (che √® testo):\n",
      "252.5\n"
     ]
    }
   ],
   "source": [
    "# ##################################\n",
    "# 2. WRONG READ (DEFAULT)          #\n",
    "# ##################################\n",
    "\n",
    "print(\"=== LETTURA SBAGLIATA (senza thousands/decimal) ===\")\n",
    "\n",
    "df_bad = pd.read_csv(\n",
    "    csv_name,\n",
    "    sep=\";\"          # we correctly read the column separator\n",
    "                     # but we do NOT tell pandas how to interpret numbers (thousands and decimals)\n",
    ")\n",
    "\n",
    "print(\"\\nDataFrame letto (sbagliato):\")\n",
    "print(df_bad)\n",
    "\n",
    "print(\"\\nTipi di dato dopo lettura sbagliata:\")\n",
    "print(df_bad.dtypes)\n",
    "\n",
    "# Test of numerical operations: here 'Quantit√†' e 'PrezzoUnitario' are still strings (object)\n",
    "print(\"\\nProvo a sommare la colonna Quantit√† (che √® testo):\")\n",
    "try:\n",
    "    print(df_bad[\"Quantit√†\"].sum())\n",
    "except Exception as e:\n",
    "    print(\"Errore durante la somma:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e73f674",
   "metadata": {
    "id": "3e73f674"
   },
   "source": [
    "**Why is `PrezzoUnitario` `object` in `df_bad.dtypes` (instead of `float`)?**\n",
    "\n",
    "For three reasons together:<br>\n",
    "\n",
    "**1. The decimal separator is a comma, not a dot**<br>\n",
    "*Example*: *1.234,50*<br>\n",
    "for pandas (with no extra instructions), *1.234,50* is not a valid number, **it‚Äôs a string**.<br>\n",
    "pandas actually expects *1234.50* (dot for decimals, no thousands separator).<br>\n",
    "So it leaves it as text (`object`).\n",
    "\n",
    "**2. There is a thousands separator `.`**<br>\n",
    "Look at *2.000,00*:<br>\n",
    "the ideal for pandas would be *2000.00*<br>\n",
    "instead it finds *2.000,00*, which looks like ‚Äú2 dot 000 comma 00‚Äù.<br>\n",
    "For the standard parser this is not a valid float ‚Üí it stays a string (`object`).\n",
    "\n",
    "Same thing for *1.000* in the `Quantit√†` column: pandas doesn‚Äôt know whether it‚Äôs ‚Äúone thousand‚Äù or ‚Äúone point zero zero zero‚Äù.<br>\n",
    "So it prefers **not** to guess and keeps it as text (`object`).\n",
    "\n",
    "**3. There are empty cells**<br>\n",
    "In the column you have values like \"\" (empty string).<br>\n",
    "So in the same column you have:\n",
    "\n",
    "* *1.234,50* (text)\n",
    "* *99,99* (text)\n",
    "* \"\" (empty text)\n",
    "* *2.000,00* (text)\n",
    "\n",
    "Heterogeneous column ‚Üí pandas says: ‚Äúok, everything is `object` (strings) and let‚Äôs move on‚Äù.\n",
    "\n",
    "If all the values were clear, English-style numbers (1234.50, 99.99, 2000.00, etc.) then pandas would have inferred `float64` by itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2560874",
   "metadata": {
    "id": "b2560874",
    "outputId": "8280c62e-f112-4ec7-d19e-2ff5567cddab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== LETTURA CORRETTA (thousands='.', decimal=',') ===\n",
      "\n",
      "DataFrame letto (corretto):\n",
      "  Prodotto  PrezzoUnitario  Quantit√†\n",
      "0     A123         1234.50    1000.0\n",
      "1     B777           99.99     250.0\n",
      "2     C900             NaN       NaN\n",
      "3     D010         2000.00    1500.0\n",
      "\n",
      "Tipi di dato dopo lettura corretta:\n",
      "Prodotto           object\n",
      "PrezzoUnitario    float64\n",
      "Quantit√†          float64\n",
      "dtype: object\n",
      "\n",
      "Somma Quantit√† (ora numerica):\n",
      "2750.0\n",
      "\n",
      "Somma PrezzoUnitario (notare i NaN dove c'erano celle vuote):\n",
      "3334.49\n"
     ]
    }
   ],
   "source": [
    "# ############################################\n",
    "# 3. RIGHT READ (input parsing)              #\n",
    "# ############################################\n",
    "\n",
    "print(\"\\n\\n=== LETTURA CORRETTA (thousands='.', decimal=',') ===\")\n",
    "\n",
    "df_good = pd.read_csv(\n",
    "    csv_name,\n",
    "    sep=\";\",\n",
    "    thousands=\".\",  # removes thousands separator\n",
    "    decimal=\",\"     # interprets comma as decimal separator\n",
    ")\n",
    "\n",
    "print(\"\\nDataFrame letto (corretto):\")\n",
    "print(df_good)\n",
    "\n",
    "print(\"\\nTipi di dato dopo lettura corretta:\")\n",
    "print(df_good.dtypes)\n",
    "\n",
    "print(\"\\nSomma Quantit√† (ora numerica):\")\n",
    "print(df_good[\"Quantit√†\"].sum())\n",
    "\n",
    "print(\"\\nSomma PrezzoUnitario (notare i NaN dove c'erano celle vuote):\")\n",
    "print(df_good[\"PrezzoUnitario\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84574a93-10bb-434d-8295-e9164b1e51a5",
   "metadata": {
    "id": "84574a93-10bb-434d-8295-e9164b1e51a5"
   },
   "source": [
    "üßæ **5. Header not on the first line** ‚Äì already seen before\n",
    "\n",
    "<u>Problem</u>: column names are not read correctly.<br>\n",
    "<u>Cause</u>: the file has descriptive lines or metadata at the beginning.<br>\n",
    "<u>Solution</u>:\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", header=2)   # if the header is on the third line\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63024a49",
   "metadata": {
    "id": "63024a49",
    "outputId": "04f41998-1b44-471d-e5ab-da20931dd9b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creato file dati_commentati.csv\n"
     ]
    }
   ],
   "source": [
    "# creates CSV file\n",
    "import csv\n",
    "\n",
    "file_name = \"dati_commentati.csv\"\n",
    "\n",
    "with open(file_name, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    # writes manually two commented lines\n",
    "    f.write(\"# Questo file contiene dati di esempio\\n\")\n",
    "    f.write(\"# Formato: ID,Nome,Valore\\n\")\n",
    "\n",
    "    writer = csv.writer(f, delimiter=\";\")\n",
    "\n",
    "    # Actual header\n",
    "    writer.writerow([\"ID\", \"Nome\", \"Valore\"])\n",
    "\n",
    "    # 3 data rows\n",
    "    writer.writerow([1, \"Alpha\", 10.5])\n",
    "    writer.writerow([2, \"Beta\", 20.0])\n",
    "    writer.writerow([3, \"Gamma\", 7.25])\n",
    "\n",
    "print(f\"Creato file {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1f8539e",
   "metadata": {
    "id": "f1f8539e",
    "outputId": "03939264-2b8b-48ea-b5e8-00899482436d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th># Questo file contiene dati di esempio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th># Formato: ID</th>\n",
       "      <th>Nome</th>\n",
       "      <td>Valore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID;Nome;Valore</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1;Alpha;10.5</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2;Beta;20.0</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3;Gamma;7.25</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    # Questo file contiene dati di esempio\n",
       "# Formato: ID  Nome                                 Valore\n",
       "ID;Nome;Valore NaN                                     NaN\n",
       "1;Alpha;10.5   NaN                                     NaN\n",
       "2;Beta;20.0    NaN                                     NaN\n",
       "3;Gamma;7.25   NaN                                     NaN"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wrong read\n",
    "pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19dcfd25",
   "metadata": {
    "id": "19dcfd25",
    "outputId": "2f5f4568-dac4-4b87-a455-32c4b78db711"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Nome</th>\n",
       "      <th>Valore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alpha</td>\n",
       "      <td>10.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Beta</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Gamma</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID   Nome  Valore\n",
       "0   1  Alpha   10.50\n",
       "1   2   Beta   20.00\n",
       "2   3  Gamma    7.25"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# right read\n",
    "pd.read_csv(\n",
    "    file_name,\n",
    "    sep=\";\",        # column separator\n",
    "    header=2)       # header is in third row (Python counts from 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "977f70f4",
   "metadata": {
    "id": "977f70f4",
    "outputId": "8b6c9fec-b7f0-4de1-de1d-a45da1e93ce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID   Nome  Valore\n",
      "0   1  Alpha   10.50\n",
      "1   2   Beta   20.00\n",
      "2   3  Gamma    7.25\n",
      "ID          int64\n",
      "Nome       object\n",
      "Valore    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# smart read\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"dati_commentati.csv\",\n",
    "    sep=\";\",         # field separator\n",
    "    comment=\"#\"      # ignores all lines beginning with '#'\n",
    ")\n",
    "\n",
    "print(df)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a34b8c9-0a5b-4333-82b7-1d87f085d1a5",
   "metadata": {
    "id": "3a34b8c9-0a5b-4333-82b7-1d87f085d1a5"
   },
   "source": [
    "ü™ì **6. File too large**\n",
    "\n",
    "<u>Problem</u>: `MemoryError` or very slow loading.<br>\n",
    "<u>Cause</u>: CSV much bigger than available RAM.<br>\n",
    "<u>Solution</u>:<br>\n",
    "\n",
    "**Chunk** loading:\n",
    "```python\n",
    "    for chunk in pd.read_csv(\"file.csv\", chunksize=100000):\n",
    "        process(chunk)                                        # 'process' is a user-defined function\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd48b972",
   "metadata": {
    "id": "dd48b972"
   },
   "source": [
    "Let‚Äôs see **how *chunks* work** in <u>two parts</u>:\n",
    "\n",
    "**1. the internal read**:\n",
    "\n",
    "```python\n",
    "pd. (..., chunksize=100000)\n",
    "```\n",
    "\n",
    "Normally, the function `pd.read_csv(\"file.csv\")` **reads the entire file into RAM** and returns **a single `DataFrame`**.<br>\n",
    "With `chunksize=100000`, instead, pandas does NOT load everything.<br>\n",
    "It returns an **`iterator`** (a generator) that yields one `DataFrame` at a time, each with **at most 100,000 rows**.\n",
    "\n",
    "So:\n",
    "\n",
    "* first loop ‚Üí rows 0‚Äì99,999\n",
    "* second loop ‚Üí rows 100,000‚Äì199,999\n",
    "* third loop ‚Üí etc.\n",
    "\n",
    "‚Ä¶until the end of the file.\n",
    "\n",
    "‚ö†Ô∏è This means that **in memory, at any given time, there are only 100k rows**, not millions/billions. This is perfect if **the file is too large to fit in RAM**.\n",
    "\n",
    "**2. the outer loop**:\n",
    "\n",
    "```python\n",
    "    for chunk in ... :\n",
    "```\n",
    "\n",
    "`chunk` is a ‚Äúpartial‚Äù pandas `DataFrame`, i.e. a slice of the CSV.<br>\n",
    "The `for` loops over all slices of the file, one after the other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773d63e",
   "metadata": {
    "id": "8773d63e"
   },
   "source": [
    "üß™ Let‚Äôs look at **a concrete example**, in two steps:\n",
    "\n",
    "* we define a **function to create the CSV file**, in <u>two variants</u>:\n",
    "\n",
    "  * the <u>small</u> version of the function (10 rows) ‚Äî useful to inspect by eye\n",
    "  * the <u>large</u> version of the function (1_000_000 rows) ‚Äî useful for real tests on `chunk`\n",
    "  * you can choose which version to use by changing only the `n_righe` argument in the call.\n",
    "* we process it in chunks to **compute the global sum of the `Importo` column**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28e27792",
   "metadata": {
    "id": "28e27792",
    "outputId": "0433c720-6aad-4cbb-f10f-b79360153a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creato file CSV 'transazioni_demo.csv' con 10 righe.\n",
      "Creato file CSV 'transazioni_grandi.csv' con 1000000 righe.\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: definition of a function that creates a big size CSV to test chunk loading\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "def crea_csv_grande(\n",
    "    file_name=\"transazioni_grandi.csv\",      # default\n",
    "    n_righe=1_000_000,                       # default  (1_000_000: sugar syntax to make the number more readable)\n",
    "    seed=42                                  # default\n",
    "):\n",
    "    \"\"\"\n",
    "    Crea un CSV con molte righe, con le colonne:\n",
    "    ID, DataOperazione, Categoria, Importo\n",
    "\n",
    "    - ID: intero progressivo\n",
    "    - DataOperazione: data fittizia\n",
    "    - Categoria: tipo transazione (es. Vendita / Rimborso / Spesa)\n",
    "    - Importo: float positivo o negativo\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    categorie = [\n",
    "        \"Vendita\",\n",
    "        \"Rimborso\",\n",
    "        \"Spesa Marketing\",\n",
    "        \"Spesa Fornitore\",\n",
    "        \"Abbonamento\",\n",
    "        \"Servizio\"\n",
    "    ]\n",
    "\n",
    "    start_date = datetime.date(2024, 1, 1)\n",
    "\n",
    "    # Creates the CSV file\n",
    "    with open(file_name, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f, delimiter=\",\")\n",
    "\n",
    "        # Header\n",
    "        writer.writerow([\"ID\", \"DataOperazione\", \"Categoria\", \"Importo\"])\n",
    "\n",
    "        for i in range(1, n_righe + 1):\n",
    "            # data = start_date + offset giorni\n",
    "            data_operazione = start_date + datetime.timedelta(days=i % 365)\n",
    "\n",
    "            categoria = random.choice(categorie)\n",
    "\n",
    "            # Importo (Amount):\n",
    "            # - positive sales between 10 and 500\n",
    "            # - negative returns between -200 and -5\n",
    "            # - negatives expenses between -1000 and -20\n",
    "            if categoria == \"Vendita\" or categoria == \"Abbonamento\" or categoria == \"Servizio\":\n",
    "                importo = round(random.uniform(10, 500), 2)\n",
    "            elif categoria == \"Rimborso\":\n",
    "                importo = round(random.uniform(-200, -5), 2)\n",
    "            else:\n",
    "                # Spesa Marketing / Spesa Fornitore\n",
    "                importo = round(random.uniform(-1000, -20), 2)\n",
    "\n",
    "            writer.writerow([\n",
    "                i,\n",
    "                data_operazione.isoformat(),  # tipo 2024-03-15\n",
    "                categoria,\n",
    "                importo\n",
    "            ])\n",
    "\n",
    "    print(f\"Creato file CSV '{file_name}' con {n_righe} righe.\")\n",
    "\n",
    "# Example of use of the function (the MAIN)\n",
    "# if __name__ == \"__main__\": tells:\n",
    "# - \"runs this code block just if I'm running it directly within this file, and NOT if I'm importing it from within another file\n",
    "# \n",
    "if __name__ == \"__main__\":\n",
    "    # small demo version (to look at it manually)\n",
    "    crea_csv_grande(\"transazioni_demo.csv\", n_righe=10)\n",
    "\n",
    "    # large demo version (to test chunksize ecc.)\n",
    "    # ATTENTION: this code creates ~1 million rows. Change this number as you prefer.\n",
    "    crea_csv_grande(\"transazioni_grandi.csv\", n_righe=1_000_000)   # 1_000_000: sugar synthax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1e97687",
   "metadata": {
    "id": "a1e97687",
    "outputId": "1f7bdbda-f5d3-4093-97c7-3cfcb7838110"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount: -59772293.91\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: process in chunks:\n",
    "\n",
    "totale = 0.0\n",
    "\n",
    "for chunk in pd.read_csv(\"transazioni_grandi.csv\", chunksize=100_000):   # numeric sugar syntax\n",
    "    totale += chunk[\"Importo\"].sum()\n",
    "\n",
    "print(\"Total amount:\", totale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ce36e",
   "metadata": {
    "id": "ab4ce36e"
   },
   "source": [
    "The code in the previous cell gives **the global sum of the `Importo` column** without ever loading the whole million rows into RAM at once ‚úÖ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3cb71a",
   "metadata": {
    "id": "8b3cb71a"
   },
   "source": [
    "---\n",
    "\n",
    "**Comment on the NUMERICAL result obtained**<br>\n",
    "\n",
    "The dataset we created earlier has both inflows and outflows.<br>\n",
    "In the CSV generator we had this logic:\n",
    "\n",
    "* Categories like `Vendita`, `Abbonamento`, `Servizio` ‚Üí **positive** amounts (revenues, +10 to +500)\n",
    "* Categories like `\"Rimborso\"` ‚Üí **negative** amounts (refunds to the customer, -5 to -200)\n",
    "* `Spesa Marketing` and `Spesa Fornitore` ‚Üí **large negative** amounts (costs, from -1000 to -20)\n",
    "\n",
    "So:\n",
    "\n",
    "* sales bring money in,\n",
    "* expenses and suppliers pull money out,\n",
    "* and often costs are, in absolute value, larger than sales.\n",
    "\n",
    "If in the sample there are many cost rows compared to sales, the final balance drops hard ‚Üí hence the very negative total like `-59,772,293.91`.\n",
    "\n",
    "In business terms: **we‚Äôre spending more than we‚Äôre earning** üòÖ.\n",
    "\n",
    "Is that a ‚Äúnormal‚Äù result?<br>\n",
    "**Yes, it‚Äôs consistent with the random generation**:\n",
    "\n",
    "* negative amounts can go down to -1000\n",
    "* positive amounts go only up to +500\n",
    "\n",
    "So even if half the rows were sales and half expenses, the expense side would still win in absolute value.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac0d14-6fb4-4909-9c4d-1935bc98df71",
   "metadata": {
    "id": "86ac0d14-6fb4-4909-9c4d-1935bc98df71"
   },
   "source": [
    "üßÆ **7. Columns with missing or misaligned values**\n",
    "\n",
    "<u>Problem</u>: rows with different number of columns, error like `ParserError: Error tokenizing data`.<br>\n",
    "<u>Cause</u>: unclosed quotes or separators inside fields.<br>\n",
    "<u>Solution</u>:\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", on_bad_lines=\"skip\", quoting=csv.QUOTE_NONE)\n",
    "```\n",
    "\n",
    "Or check the delimiters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc8974",
   "metadata": {
    "id": "20fc8974"
   },
   "source": [
    "Below is the code in 3 steps (for each of the two errors):\n",
    "- creation of the csv file\n",
    "- wrong (intentional) reading\n",
    "- robust reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "705f99a3",
   "metadata": {
    "id": "705f99a3",
    "outputId": "c6a78c3c-3fa8-4f8a-a09f-d86ea9a08794"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 2 test csv files created\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 1) CSV with non closed quotes     -\n",
    "# -----------------------------------\n",
    "content_unclosed = \"\"\"id,name,amount,notes\n",
    "1,Mario Rossi,1200,OK\n",
    "2,Luigi Bianchi,950,pagato\n",
    "3,Carla Verdi,800,\"nota con virgolette non chiuse\n",
    "4,Paolo Neri,700,ok\n",
    "\"\"\"\n",
    "\n",
    "file_unclosed = \"csv_virgolette_non_chiuse.csv\"\n",
    "with open(file_unclosed, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    f.write(content_unclosed)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2) CSV with separators within fields (non quoted)  -\n",
    "# ----------------------------------------------------\n",
    "# header: 4 colonne\n",
    "content_separators = \"\"\"id,name,city,amount\n",
    "1,Mario Rossi,Milano,1200\n",
    "2,Luigi Bianchi,Roma,900\n",
    "3,Carla Verdi,Milano, Italia,800\n",
    "4,Paolo Neri,Torino,700\n",
    "\"\"\"\n",
    "\n",
    "file_separators = \"csv_separatori_dentro_campi.csv\"\n",
    "with open(file_separators, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    f.write(content_separators)\n",
    "\n",
    "print(\"‚úÖ 2 test csv files created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a19bd7b",
   "metadata": {
    "id": "8a19bd7b",
    "outputId": "6058669c-063d-4a13-b2e3-10d7adba7550"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 1) TEST: virgolette non chiuse ===\n",
      "‚ùå errore atteso (virgolette non chiuse):\n",
      "Error tokenizing data. C error: EOF inside string starting at row 3\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Read test of file 1      =\n",
    "# ==========================\n",
    "\n",
    "print(\"\\n=== 1) TEST: virgolette non chiuse ===\")\n",
    "try:\n",
    "    df1 = pd.read_csv(file_unclosed)\n",
    "    print(df1)\n",
    "except Exception as e:\n",
    "    print(\"‚ùå errore atteso (virgolette non chiuse):\")\n",
    "    print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d30c4ef6",
   "metadata": {
    "id": "d30c4ef6",
    "outputId": "b4e9d0d8-fe58-4965-8482-18e6a0abc34a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ lettura robusta (file 1):\n",
      "   id           name  amount                            notes\n",
      "0   1    Mario Rossi    1200                               OK\n",
      "1   2  Luigi Bianchi     950                           pagato\n",
      "2   3    Carla Verdi     800  \"nota con virgolette non chiuse\n",
      "3   4     Paolo Neri     700                               ok\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 'robust' read of file 1       =\n",
    "# ===============================\n",
    "df1_ok = pd.read_csv(\n",
    "    file_unclosed,\n",
    "    on_bad_lines=\"skip\",\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    engine=\"python\",\n",
    ")\n",
    "print(\"\\n‚úÖ lettura robusta (file 1):\")\n",
    "print(df1_ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23db78eb",
   "metadata": {
    "id": "23db78eb"
   },
   "source": [
    "`quoting = csv.QUOTE_NONE`:<br>\n",
    "this tells it: ‚Äúdo not treat quotes (\") as something special, consider them normal text‚Äù.\n",
    "\n",
    "Why did we also put `engine=\"python\"`?<br>\n",
    "...\n",
    "**Practical rule:**<br>\n",
    "1 format ‚Üí `parse_dates`<br>\n",
    "few formats ‚Üí `to_datetime`<br>\n",
    "mixed/dirty formats ‚Üí **custom function** ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e797e8be",
   "metadata": {
    "id": "e797e8be",
    "outputId": "aecbf25c-1cbc-441f-c1ac-066d15b1dea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 2) TEST: separatori dentro i campi ===\n",
      "‚ùå errore atteso (troppi separatori):\n",
      "Error tokenizing data. C error: Expected 4 fields in line 4, saw 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# read test of file 2      =\n",
    "# ==========================\n",
    "\n",
    "print(\"\\n=== 2) TEST: separatori dentro i campi ===\")\n",
    "try:\n",
    "    df2 = pd.read_csv(file_separators)\n",
    "    print(df2)\n",
    "except Exception as e:\n",
    "    print(\"‚ùå errore atteso (troppi separatori):\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc2d57bb",
   "metadata": {
    "id": "dc2d57bb",
    "outputId": "3676d4e9-d253-416f-cd8e-ff2abb6a94f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ lettura robusta (file 2):\n",
      "   id           name    city  amount\n",
      "0   1    Mario Rossi  Milano    1200\n",
      "1   2  Luigi Bianchi    Roma     900\n",
      "2   4     Paolo Neri  Torino     700\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 'robust' read of file 2       =\n",
    "# ===============================\n",
    "\n",
    "df2_ok = pd.read_csv(\n",
    "    file_separators,\n",
    "    on_bad_lines=\"skip\",\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    engine=\"python\",\n",
    ")\n",
    "print(\"\\n‚úÖ lettura robusta (file 2):\")\n",
    "print(df2_ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab5064-1469-451a-8732-786123690fa4",
   "metadata": {
    "id": "caab5064-1469-451a-8732-786123690fa4"
   },
   "source": [
    "üß† **8. Dates not interpreted correctly**\n",
    "\n",
    "<u>Problem</u>: dates remain strings or are in the wrong format.<br>\n",
    "<u>Solution</u>:\n",
    "```python\n",
    "pd.read_csv(\"file.csv\", parse_dates=[\"data\"])\n",
    "```\n",
    "\n",
    "or:\n",
    "```python\n",
    "df[\"data\"] = pd.to_datetime(df[\"data\"], dayfirst=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c384078",
   "metadata": {
    "id": "2c384078"
   },
   "source": [
    "Let's create a CSV that deliberately contains mixed dates (Italian...hours) so that `read_csv` does not recognize them and leaves them as `object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac1a34ec",
   "metadata": {
    "id": "ac1a34ec",
    "outputId": "5fa1ec7e-7713-46d7-a41a-3087316baf8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ creato date_mischiate.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a CSV file with \"bad\" dates\n",
    "# \"bad\" CSV: different date formats ‚Üí pandas does not manage to make them uniform\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "csv_text = \"\"\"data;descrizione;importo\n",
    "01/02/2025;Fattura cliente A;120.50\n",
    "2025-02-01;Fattura cliente B;85.00\n",
    "12/31/2024;Formato USA;15.75\n",
    "31/12/2024;Chiusura anno;999.99\n",
    "2025/02/01 14:30;Con orario;50.00\n",
    ";Data mancante;0.00\n",
    "non-data;Valore sporco;5.25\n",
    "\"\"\"\n",
    "\n",
    "with open(\"date_mischiate.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    f.write(csv_text)\n",
    "\n",
    "print(\"‚úÖ creato date_mischiate.csv\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf127f",
   "metadata": {
    "id": "7adf127f"
   },
   "source": [
    "It‚Äôs a CSV file with mixed dates.\n",
    "\n",
    "Now let‚Äôs do the ‚Äúnaive‚Äù read (pandas doesn‚Äôt understand the dates and leaves them as `object`):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e92d4f6",
   "metadata": {
    "id": "2e92d4f6",
    "outputId": "3f316ca0-a12d-49e6-c1cf-876d248629c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¥ LETTURA INGENUA\n",
      "data            object\n",
      "descrizione     object\n",
      "importo        float64\n",
      "dtype: object\n",
      "               data        descrizione  importo\n",
      "0        01/02/2025  Fattura cliente A   120.50\n",
      "1        2025-02-01  Fattura cliente B    85.00\n",
      "2        12/31/2024        Formato USA    15.75\n",
      "3        31/12/2024      Chiusura anno   999.99\n",
      "4  2025/02/01 14:30         Con orario    50.00\n",
      "5               NaN      Data mancante     0.00\n",
      "6          non-data      Valore sporco     5.25 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. \"wrong\" / naive reading\n",
    "df_raw = pd.read_csv(\"date_mischiate.csv\", sep=\";\")\n",
    "print(\"üî¥ LETTURA INGENUA\")\n",
    "print(df_raw.dtypes)\n",
    "print(df_raw, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4da2ad3",
   "metadata": {
    "id": "f4da2ad3"
   },
   "source": [
    "`data` is `object` ‚Üí that is, a string.<br>\n",
    "Now the two solutions:\n",
    "\n",
    "Solution 1 ‚Äì directly in `read_csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47731e86",
   "metadata": {
    "id": "47731e86",
    "outputId": "c577319e-3795-4600-fcaf-d05e8d66a613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü† LETTURA CON parse_dates (pandas non ce la fa)\n",
      "data            object\n",
      "descrizione     object\n",
      "importo        float64\n",
      "dtype: object\n",
      "               data        descrizione  importo\n",
      "0        01/02/2025  Fattura cliente A   120.50\n",
      "1        2025-02-01  Fattura cliente B    85.00\n",
      "2        12/31/2024        Formato USA    15.75\n",
      "3        31/12/2024      Chiusura anno   999.99\n",
      "4  2025/02/01 14:30         Con orario    50.00\n",
      "5               NaN      Data mancante     0.00\n",
      "6          non-data      Valore sporco     5.25 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 3) Read with parse_dates\n",
    "# ‚Üí with these mixed dates, pandas gives up!\n",
    "# ‚Üí date remains object\n",
    "# =========================================\n",
    "df_auto = pd.read_csv(\n",
    "    \"date_mischiate.csv\",\n",
    "    sep=\";\",\n",
    "    parse_dates=[\"data\"],\n",
    "    dayfirst=True\n",
    ")\n",
    "print(\"üü† LETTURA CON parse_dates (pandas non ce la fa)\")\n",
    "print(df_auto.dtypes)\n",
    "print(df_auto, \"\\n\")\n",
    "# üëâ date = object\n",
    "# because in the file there are: dd/mm/yyyy, ISO, mm/dd/yyyy, with time, empty values, text, etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97cff7fc",
   "metadata": {
    "id": "97cff7fc",
    "outputId": "de08422a-dffb-4447-986e-4c2d6965281a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ ROBUST read (after apply)\n",
      "data           datetime64[ns]\n",
      "descrizione            object\n",
      "importo               float64\n",
      "dtype: object\n",
      "                 data        descrizione  importo\n",
      "0 2025-02-01 00:00:00  Fattura cliente A   120.50\n",
      "1 2025-02-01 00:00:00  Fattura cliente B    85.00\n",
      "2 2024-12-31 00:00:00        Formato USA    15.75\n",
      "3 2024-12-31 00:00:00      Chiusura anno   999.99\n",
      "4 2025-02-01 14:30:00         Con orario    50.00\n",
      "5                 NaT      Data mancante     0.00\n",
      "6                 NaT      Valore sporco     5.25\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 4) ROBUST read (it must work!)\n",
    "# ‚Üí now let's read as string\n",
    "# ‚Üí let's convert them one by one\n",
    "# =========================================\n",
    "\n",
    "# definition of a \"flexible\" parsing function \n",
    "def parse_flessibile(x: str):\n",
    "    if pd.isna(x) or x == \"\":\n",
    "        return pd.NaT\n",
    "    # proviamo pi√π formati noti\n",
    "    for fmt in (\"%d/%m/%Y\", \"%Y-%m-%d\", \"%m/%d/%Y\", \"%Y/%m/%d %H:%M\", \"%d/%m/%Y %H:%M\"):\n",
    "        try:\n",
    "            return datetime.strptime(x, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return pd.NaT   # this is NOT date\n",
    "\n",
    "# the function 'apply' on the column \"date\"\n",
    "df_ok = pd.read_csv(\"date_mischiate.csv\", sep=\";\")\n",
    "df_ok[\"data\"] = df_ok[\"data\"].apply(parse_flessibile)\n",
    "\n",
    "print(\"üü¢ ROBUST read (after apply)\")\n",
    "print(df_ok.dtypes)\n",
    "print(df_ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0853a23",
   "metadata": {},
   "source": [
    "Let's see **line by line** what the code in the previous cell does:\n",
    "\n",
    "1. `def parse_flessibile(x: str):`<br>\n",
    "defines a function that receives **a single cell** (a string) and returns a **date** or `NaT`.\n",
    "\n",
    "2. `if pd.isna(x) or x == \"\":`<br>\n",
    "if the cell is empty (`\"\"`) or it is an NA (`NaN` read by pandas) ‚Üí it doesn't even try ‚Üí **returns** `pd.NaT`.<br>\n",
    "(`pd.NaT` = Not A Time, the equivalent of `NaN` but for dates.)\n",
    "\n",
    "3. `for fmt in (...):`<br>\n",
    "here there is the list of the **formats it wants to try**, in order:\n",
    "    - `\"%d/%m/%Y\"` ‚Üí 31/12/2024 (Italian)\n",
    "    - `\"%Y-%m-%d\"` ‚Üí 2025-02-01 (ISO)\n",
    "    - `\"%m/%d/%Y\"` ‚Üí 12/31/2024 (US)\n",
    "    - `\"%Y/%m/%d %H:%M\"` ‚Üí 2025/02/01 14:30\n",
    "    - `\"%d/%m/%Y %H:%M\"` ‚Üí 31/12/2024 09:15\n",
    "if one of the formats works ‚Üí it exits the `for` with `return datetime.strptime(...)`.\n",
    "\n",
    "4. final `return pd.NaT`<br>\n",
    "if none of the formats worked ‚Üí it returns `NaT`.\n",
    "\n",
    "5. `df_ok = pd.read_csv(...)`<br>\n",
    "here we read the whole CSV **as text** (`object`).\n",
    "\n",
    "6. `df_ok[\"data\"] = df_ok[\"data\"].apply(parse_flessibile)`<br>\n",
    "here we let the function do the job: row by row ‚Üí cell by cell ‚Üí it tries all the formats.\n",
    "\n",
    "At the end, the `data` column is a **datetime** column üü¢\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c91813",
   "metadata": {},
   "source": [
    "**Why is there the apply**??\n",
    "\n",
    "```python\n",
    "df_ok = pd.read_csv(\"date_mischiate.csv\", sep=\";\")\n",
    "df_ok[\"data\"] = df_ok[\"data\"].apply(parse_flessibile)\n",
    "```\n",
    "\n",
    "* `read_csv(...)` reads the whole column as strings (because they were all different).\n",
    "* `df_ok[\"data\"].apply(parse_flessibile)` means:<br>\n",
    "  ‚Äúfor **each row** of the `data` column execute `parse_flessibile(...)`‚Äù.\n",
    "* the result is **a new Series of type `datetime`** (with some `NaT` inside).\n",
    "* it reassigns it to `df_ok[\"data\"]` ‚Üí the column really becomes `datetime64[ns]`.\n",
    "\n",
    "It‚Äôs the classic trick: **when `parse_dates` is not enough** ‚Üí you do the `apply`.\n",
    "\n",
    "---\n",
    "\n",
    "**Why didn‚Äôt we just use `pd.to_datetime(...)`?**\n",
    "\n",
    "We could have done:\n",
    "\n",
    "```python\n",
    "df_ok[\"data\"] = pd.to_datetime(df_ok[\"data\"], dayfirst=True, errors=\"coerce\")\n",
    "```\n",
    "\n",
    "and in many cases it‚Äôs fine.<br>\n",
    "Here, however, we had both Italian and American formats, both with time and without. `to_datetime` alone sometimes guesses right, sometimes not.<br>\n",
    "With this parsing function, instead, we decide the order of the formats.<br>\n",
    "Example: first try Italian, then American ‚Üí this way it doesn‚Äôt get 03/04/2025 wrong.\n",
    "\n",
    "---\n",
    "\n",
    "**What happens to the ‚Äúdirty‚Äù rows?**\n",
    "\n",
    "* `\"\"` ‚Üí `NaT`\n",
    "* `\"non-data\"` ‚Üí no format understands it ‚Üí `NaT`\n",
    "* `\"2025/02/01 14:30\"` ‚Üí it catches it at the 4th format ‚Üí becomes a real `datetime`\n",
    "* `\"12/31/2024\"` ‚Üí it catches it at the 3rd format ‚Üí ok\n",
    "* `\"31/12/2024\"` ‚Üí it catches it at the 1st format ‚Üí ok\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "print(df_ok.dtypes)\n",
    "```\n",
    "\n",
    "data           datetime64[ns]<br>\n",
    "descrizione            object<br>\n",
    "importo               float64<br>\n",
    "dtype: object<br>\n",
    "\n",
    "This is the result we wanted from the very beginning: the column is no longer `object`, it is a column of dates üí™\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a369dd",
   "metadata": {},
   "source": [
    "**FINAL SUMMARY of point 8**\n",
    "\n",
    "If the dates are **all in the same format** ‚Üí<br>\n",
    "`pd.read_csv(..., parse_dates=[\"data\"])` is perfectly fine.\n",
    "\n",
    "If the dates have **different but ‚Äúsimilar‚Äù formats** (all European, or all ISO) ‚Üí<br>\n",
    "you can read normally and then do:\n",
    "\n",
    "```python\n",
    "df[\"data\"] = pd.to_datetime(df[\"data\"], dayfirst=True, errors=\"coerce\")\n",
    "```\n",
    "\n",
    "often that's enough.\n",
    "\n",
    "If the dates are really **heterogeneous** (eu, us, with time, empty, text) ‚Üí<br>\n",
    "`parse_dates` alone is not enough, and sometimes not even a guessed `to_datetime(...)`;<br>\n",
    "in that case a **flexible parsing function** that tries multiple formats is convenient.\n",
    "\n",
    "**Practical rule:**<br>\n",
    "1 format ‚Üí `parse_dates`<br>\n",
    "few formats ‚Üí `to_datetime`<br>\n",
    "mixed/dirty formats ‚Üí **custom function** ‚úÖ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32062fb5",
   "metadata": {},
   "source": [
    "‚¨ú **9. Duplicates or whitespace in column names**\n",
    "\n",
    "<u>Problem</u>: names with spaces or duplicates (`'Nome '` ‚â† `'Nome'`).<br> <u>Solution</u>:\n",
    "\n",
    "```python\n",
    "    df.columns = df.columns.str.strip()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35cfd448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ creato con_spazi.csv\n"
     ]
    }
   ],
   "source": [
    "# 1) I create a CSV with column names with spaces\n",
    "csv_text = \"\"\"  data  ;  nome cliente ; importo ;  note\n",
    "01/02/2025;Mario Rossi;120.50;pagato\n",
    "02/02/2025;  Anna Bianchi ;89.00;ritardo\n",
    "03/02/2025;ACME S.p.A.;250.00;\n",
    "\"\"\"\n",
    "\n",
    "file_name = \"con_spazi.csv\"\n",
    "with open(file_name, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    f.write(csv_text)\n",
    "\n",
    "print(f\"‚úÖ creato {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7656d0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé colonne lette (sporche):\n",
      "['  data  ', '  nome cliente ', ' importo ', '  note']\n"
     ]
    }
   ],
   "source": [
    "# 2) \"normal\" read ‚Üí column names are dirty\n",
    "df = pd.read_csv(file_name, sep=\";\")\n",
    "print(\"üîé colonne lette (sporche):\")\n",
    "print(repr(df.columns.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62b9d1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ colonne dopo strip():\n",
      "['data', 'nome cliente', 'importo', 'note']\n",
      "\n",
      "üìÑ dataframe finale:\n",
      "         data     nome cliente  importo     note\n",
      "0  01/02/2025      Mario Rossi    120.5   pagato\n",
      "1  02/02/2025    Anna Bianchi      89.0  ritardo\n",
      "2  03/02/2025      ACME S.p.A.    250.0      NaN\n"
     ]
    }
   ],
   "source": [
    "# 3) cleans column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "print(\"\\n‚úÖ colonne dopo strip():\")\n",
    "print(repr(df.columns.tolist()))\n",
    "\n",
    "print(\"\\nüìÑ dataframe finale:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249c081",
   "metadata": {},
   "source": [
    "**What happened?**\n",
    "\n",
    "* before `strip()` the columns are like:<br>\n",
    "  [*'  data  ', '  nome cliente ', ' importo ', '  note ']*<br>\n",
    "* after:<br>\n",
    "  *['data', 'nome cliente', 'importo', 'note']*<br>\n",
    "\n",
    "So if you want to do:\n",
    "\n",
    "```python\n",
    "  df[\"data\"]\n",
    "```\n",
    "\n",
    "now it works, while before you would have had to write `df[\" data \"]` and that‚Äôs not nice üòÖ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f52b52",
   "metadata": {},
   "source": [
    "üß± **10. Quotes and special characters**\n",
    "\n",
    "<u>Problem</u>: CSV with inner quotes, double quotes, etc.<br> <u>Solution</u>:\n",
    "\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", quotechar='\"', escapechar='\\\\')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304949d5",
   "metadata": {},
   "source": [
    "Let‚Äôs look at the usual flow (like for the previous errors), in this case with 4 steps:\n",
    "\n",
    "* we create a CSV **deliberately dirty, but not enough --> it manages to read it\n",
    "  (with quotes inside, doubled quotes, backslash‚Ä¶)**\n",
    "* we try the **naive read** ‚Üí it gets messed up / errors out / splits the columns\n",
    "* we do the **robust read** ‚Üí `quotechar='\"'`, `escapechar='\\\\'`, and optionally also `engine=\"python\"` to be safe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2794f60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scritto csv_sporco_ok.csv\n",
      "id,descrizione,note\n",
      "1,\"Martello, 500g\",\"tutto ok\"\n",
      "2,\"Cacciavite \\\"piatto\\\"\",\"virgolette con backslash (\\\")\"\n",
      "3,\"Set \"\"professionale\"\" 24 pz\",\"virgolette raddoppiate nel campo descrizione\"\n",
      "4,\"C:\\\\attrezzi\\\\nuovo\",\"percorso Windows con backslash\"\n",
      "\n",
      "\n",
      "=== CASO A - LETTURA INGENUA (funziona) ===\n",
      "   id                descrizione                                          note\n",
      "0   1             Martello, 500g                                      tutto ok\n",
      "1   2      Cacciavite \\piatto\\\"\"                 virgolette con backslash (\\)\"\n",
      "2   3  Set \"professionale\" 24 pz  virgolette raddoppiate nel campo descrizione\n",
      "3   4        C:\\\\attrezzi\\\\nuovo                percorso Windows con backslash\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# =========================================================\n",
    "# CASE A - CSV \"dirty but readable\"\n",
    "# ---------------------------------------------------------\n",
    "# Here we want to show that: even if there are inner quotes\n",
    "# and backslashes, the naive read *might* still work.\n",
    "# =========================================================\n",
    "\n",
    "file_ok = \"csv_sporco_ok.csv\"\n",
    "\n",
    "csv_ok = (\n",
    "    # row 1\n",
    "    'id,descrizione,note\\n'\n",
    "    # row 2\n",
    "    '1,\"Martello, 500g\",\"tutto ok\"\\n'\n",
    "    # row 3 - here there is the backslash + quotes: \\\"piatto\\\"\n",
    "    '2,\"Cacciavite \\\\\"piatto\\\\\"\",\"virgolette con backslash (\\\\\")\"\\n'\n",
    "    # row 4 - doubled quotes, CSV style\n",
    "    '3,\"Set \"\"professionale\"\" 24 pz\",\"virgolette raddoppiate nel campo descrizione\"\\n'\n",
    "    # row 5 - Windows path\n",
    "    '4,\"C:\\\\\\\\attrezzi\\\\\\\\nuovo\",\"percorso Windows con backslash\"\\n'\n",
    ")\n",
    "\n",
    "Path(file_ok).write_text(csv_ok, encoding=\"utf-8\")\n",
    "print(f\"‚úÖ Scritto {file_ok}\")\n",
    "print(csv_ok)\n",
    "\n",
    "print(\"\\n=== CASO A - LETTURA INGENUA (funziona) ===\")\n",
    "# üëâ HERE we do *NOT* set either quotechar or escapechar\n",
    "df_ok_naive = pd.read_csv(file_ok)\n",
    "print(df_ok_naive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914f191b",
   "metadata": {},
   "source": [
    "As you can see:\n",
    "\n",
    "* the naive read of the `csv_sporco_ok.csv` file **worked** (even without `quotechar` + `escapechar`)\n",
    "* that is, the read ‚Äúdoes not always throw an error, but it‚Äôs better to specify‚Äù with `quotechar` + `escapechar`\n",
    "\n",
    "Obviously, a fortiori the robust read also works, as you can see from the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2f11f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CASE A - ROBUST READ ===\n",
      "   id                descrizione                                          note\n",
      "0   1             Martello, 500g                                      tutto ok\n",
      "1   2        Cacciavite \"piatto\"                  virgolette con backslash (\")\n",
      "2   3  Set \"professionale\" 24 pz  virgolette raddoppiate nel campo descrizione\n",
      "3   4          C:\\attrezzi\\nuovo                percorso Windows con backslash\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== CASE A - ROBUST READ ===\")\n",
    "df_ok_safe = pd.read_csv(\n",
    "    file_ok,\n",
    "    quotechar='\"',\n",
    "    escapechar='\\\\',\n",
    "    engine=\"python\",\n",
    ")\n",
    "print(df_ok_safe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e61aa10",
   "metadata": {},
   "source": [
    "Let‚Äôs now create, instead, a CSV file ‚Äúbroken‚Äù in another way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ccc80ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Scritto csv_sporco_rotto.csv\n",
      "id,descrizione,prezzo\n",
      "1,\"Martello\",12.5\n",
      "2,\"Cacciavite \\\"piatto\\\"\",8.9\n",
      "3,\"Set \"\"professionale\"\" 24 pz\",49.0\n",
      "4,\"Pinza con \"virgolette\" dentro, con virgola\",15.0\n",
      "\n",
      "\n",
      "=== CASO B - LETTURA INGENUA (DEVE FALLIRE) ===\n",
      "‚ùå Lettura ingenua fallita: Error tokenizing data. C error: Expected 3 fields in line 5, saw 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# CASE B - CSV BROKEN ON PURPOSE (unbalanced quotes)\n",
    "# ---------------------------------------------------------\n",
    "# Here we want to show the case in which the\n",
    "# naive read does NOT work.\n",
    "# The idea is to put a line with: \"Pliers with \"quotes\" inside\"\n",
    "# but WITHOUT escape and with a comma inside ‚Üí the C parser chokes.\n",
    "# =========================================================\n",
    "\n",
    "file_bad = \"csv_sporco_rotto.csv\"\n",
    "\n",
    "csv_bad = (\n",
    "    'id,descrizione,prezzo\\n'                    # row 1 (header)\n",
    "    '1,\"Martello\",12.5\\n'                        # row 2 ok\n",
    "    '2,\"Cacciavite \\\\\"piatto\\\\\"\",8.9\\n'          # row 3 ok (has \\\")\n",
    "    '3,\"Set \"\"professionale\"\" 24 pz\",49.0\\n'     # row 4 ok (has \"\")\n",
    "    # row 5 - THIS BREAKS IT:\n",
    "    #   - quoted field that contains other NON-escaped quotes\n",
    "    #   - and it also contains a comma ‚Üí the parser thinks another field starts/ends\n",
    "    '4,\"Pinza con \"virgolette\" dentro, con virgola\",15.0\\n'\n",
    ")\n",
    "\n",
    "Path(file_bad).write_text(csv_bad, encoding=\"utf-8\")\n",
    "print(f\"\\n‚úÖ Scritto {file_bad}\")\n",
    "print(csv_bad)\n",
    "\n",
    "\n",
    "print(\"\\n=== CASO B - LETTURA INGENUA (DEVE FALLIRE) ===\")\n",
    "try:\n",
    "    df_bad_naive = pd.read_csv(file_bad)\n",
    "    print(df_bad_naive)\n",
    "except Exception as e:\n",
    "    # here you expect something like:\n",
    "    # ParserError: Error tokenizing data. C error: Expected 3 fields in line 5, saw 4\n",
    "    print(\"‚ùå Lettura ingenua fallita:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a866de3",
   "metadata": {},
   "source": [
    "With the two arguments instead the robust read (with `quotechar='\"'` and `escapechar='\\\\'`) **works**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9bfc053f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CASO B - LETTURA ROBUSTA  ===\n",
      "   id                descrizione  prezzo\n",
      "0   1                   Martello    12.5\n",
      "1   2        Cacciavite \"piatto\"     8.9\n",
      "2   3  Set \"professionale\" 24 pz    49.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_3788\\3764594861.py:2: ParserWarning: Skipping line 5: ',' expected after '\"'\n",
      "\n",
      "  df_bad_safe = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== CASO B - LETTURA ROBUSTA  ===\")\n",
    "df_bad_safe = pd.read_csv(\n",
    "    file_bad,\n",
    "    quotechar='\"',\n",
    "    escapechar='\\\\',\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"warn\",   # or \"skip\" to skip the broken rows\n",
    ")\n",
    "print(df_bad_safe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384cec81",
   "metadata": {},
   "source": [
    "It read it, but it got a **warning**!<br>\n",
    "It is telling us something very precise: even with the ‚Äúmore tolerant‚Äù parser (`engine=\"python\"`) and even with `quotechar` / `escapechar`, line 5 is really broken at CSV level. It‚Äôs not just ‚Äúhard‚Äù, it is syntactically wrong: there are quotes opened and not closed, and on top of that there is a comma inside.\n",
    "\n",
    "So: it‚Äôs normal that it skips it. We made it like this on purpose to show the case in which ‚Äúnot even the robust read can save it‚Äù and pandas says ‚Äúok, I throw it away and go on‚Äù.\n",
    "\n",
    "Now there are **two ways** to avoid the warning (the row skip):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa5240f",
   "metadata": {},
   "source": [
    "<u>First way</u>: standard **CSV style** ‚Üí **we double the quotes**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4376f25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LETTURA ROBUSTA (CSV VALIDO, virgolette raddoppiate) ===\n",
      "   id                                 descrizione  prezzo\n",
      "0   1                                    Martello    12.5\n",
      "1   2                         Cacciavite \"piatto\"     8.9\n",
      "2   3                   Set \"professionale\" 24 pz    49.0\n",
      "3   4  Pinza con \"virgolette\" dentro, con virgola    15.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "file_good = \"csv_sporco_riparato_doppie.csv\"\n",
    "\n",
    "csv_good = (\n",
    "    'id,descrizione,prezzo\\n'\n",
    "    '1,\"Martello\",12.5\\n'\n",
    "    '2,\"Cacciavite \\\\\"piatto\\\\\"\",8.9\\n'\n",
    "    '3,\"Set \"\"professionale\"\" 24 pz\",49.0\\n'\n",
    "    # üëá here is right: internal quotes are doubled\n",
    "    '4,\"Pinza con \"\"virgolette\"\" dentro, con virgola\",15.0\\n'\n",
    ")\n",
    "\n",
    "Path(file_good).write_text(csv_good, encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n=== LETTURA ROBUSTA (CSV VALIDO, virgolette raddoppiate) ===\")\n",
    "df_ok = pd.read_csv(\n",
    "    file_good,\n",
    "    quotechar='\"',\n",
    "    escapechar='\\\\',\n",
    "    engine=\"python\",\n",
    ")\n",
    "print(df_ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76dc78b",
   "metadata": {},
   "source": [
    "There is no warning anymore! Why?<br> Because:\n",
    "\n",
    "* the field is quoted `\"...\"`,\n",
    "* inside there are quotes ‚Üí we rewrote them as `\"\"`,\n",
    "* inside there is also the comma ‚Üí but since the field is quoted, the comma is ok.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc3c00",
   "metadata": {},
   "source": [
    "<u>Second way</u>: **‚Äú`escapechar`‚Äù** style ‚Üí we use the backslash inside (if we really want to show the use of `escapechar='\\\\'`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f8a95e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LETTURA ROBUSTA (CSV VALIDO, escape con backslash) ===\n",
      "   id                                 descrizione  prezzo\n",
      "0   1                                    Martello    12.5\n",
      "1   2                         Cacciavite \"piatto\"     8.9\n",
      "2   3                   Set \"professionale\" 24 pz    49.0\n",
      "3   4  Pinza con \"virgolette\" dentro, con virgola    15.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "file_good2 = \"csv_sporco_riparato_escape.csv\"\n",
    "\n",
    "csv_good2 = (\n",
    "    'id,descrizione,prezzo\\n'\n",
    "    '1,\"Martello\",12.5\\n'\n",
    "    '2,\"Cacciavite \\\\\"piatto\\\\\"\",8.9\\n'\n",
    "    '3,\"Set \"\"professionale\"\" 24 pz\",49.0\\n'\n",
    "    # üëá qui ESCAPE ALL internal quotes\n",
    "    '4,\"Pinza con \\\\\"virgolette\\\\\" dentro, con virgola\",15.0\\n'\n",
    ")\n",
    "\n",
    "Path(file_good2).write_text(csv_good2, encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n=== LETTURA ROBUSTA (CSV VALIDO, escape con backslash) ===\")\n",
    "df_ok2 = pd.read_csv(\n",
    "    file_good2,\n",
    "    quotechar='\"',\n",
    "    escapechar='\\\\',   # üëà now it's really required\n",
    "    engine=\"python\",\n",
    ")\n",
    "print(df_ok2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8729d51",
   "metadata": {},
   "source": [
    "As you can see, again: no warning ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd070ff",
   "metadata": {},
   "source": [
    "**SUMMARY of point 10**:\n",
    "\n",
    "* ‚Äúnaive read‚Äù: `pd.read_csv(\"file.csv\")` ‚Üí if the CSV is formally correct, it reads; if it‚Äôs a bit dirty, sometimes it reads; if it‚Äôs really broken, it raises `ParserError`.\n",
    "* ‚Äúrobust read‚Äù: `pd.read_csv(\"file.csv\", quotechar='\"', escapechar='\\\\', engine=\"python\", on_bad_lines=\"warn\")` ‚Üí **it reads more, but it cannot invent quotes that aren‚Äôt there** ‚Üí and so it gives the **warning** we saw before.\n",
    "* if we do NOT want the warning ‚Üí the two reading styles we saw (or we rewrite the CSV in a valid way (doubling `\"\"` or escaping `\\\"`)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62b8f06",
   "metadata": {},
   "source": [
    "**Summary of the 10 cases**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d5d57",
   "metadata": {},
   "source": [
    "| Problem | Main cause | Pandas error / symptom example | Suggested solution |\n",
    "|--------|------------|---------------------------------|--------------------|\n",
    "| Column `Unnamed: 0` | The index was saved in the CSV (from `to_csv(index=True)`) | No error, but an extra column `\"Unnamed: 0\"` appears | `pd.read_csv(\"file.csv\", index_col=0)` or `df.drop(columns=[\"Unnamed: 0\"])` |\n",
    "| All columns merged into one | Wrong separator (`;`, `,`, `\\t`, etc.) | No error, but `df.shape` shows only 1 column | `pd.read_csv(\"file.csv\", sep=\";\")` or `sep=\"\\t\"` |\n",
    "| Strange characters (ÔøΩ) | Wrong encoding (UTF-8 vs Latin1 vs CP1252) | `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe0` | `pd.read_csv(\"file.csv\", encoding=\"latin1\")` or `encoding=\"cp1252\"` |\n",
    "| Numeric values read as strings | Decimal/thousands separators or symbols | No error, but `df.dtypes` shows `object` | `pd.read_csv(\"file.csv\", decimal=\",\", thousands=\".\")` or `pd.to_numeric(..., errors=\"coerce\")` |\n",
    "| Header not on the first line | Comment rows or metadata before the header | No error, but columns are numbered (0, 1, 2, ‚Ä¶) | `pd.read_csv(\"file.csv\", header=2)` or `skiprows=2` |\n",
    "| File too large / not enough memory | File > available RAM | `MemoryError` or kernel crash | `pd.read_csv(..., chunksize=100000)` or use Dask/Polars |\n",
    "| Rows with different number of columns | Unclosed quotes or irregular delimiters | `ParserError: Error tokenizing data. C error: Expected N fields in line X, saw M` | `pd.read_csv(..., on_bad_lines=\"skip\", engine=\"python\")` |\n",
    "| Dates not recognized | Non-standard format or day/month ambiguity | No error, but `object` instead of `datetime64` | `pd.read_csv(..., parse_dates=[\"Data1\"])` or `pd.to_datetime(..., dayfirst=True)` |\n",
    "| Column names with spaces / duplicates | Header with spaces, double tabs, or invisible characters | No error, but `KeyError` when accessing the name | `df.columns = df.columns.str.strip()` or `rename` |\n",
    "| Complex quotes / escapes | Fields with double quotes or inner separators | `ParserError: unexpected end of data` or partial parsing | `pd.read_csv(..., quotechar='\"', escapechar='\\\\')` |\n",
    "| Unknown separator | Non-standard file, mixed `, ; \\t` | `ParserError` or wrong columns | `pd.read_csv(\"file.csv\", sep=None, engine=\"python\")` |\n",
    "| Slow performance | `\"python\"` engine and large file | Very slow loading | `engine=\"c\"` or `chunksize` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff723f",
   "metadata": {},
   "source": [
    "**A second set of examples follows** with **data correction**:<br>\n",
    "\n",
    "1Ô∏è‚É£ creation of a **‚Äúdirty‚Äù** CSV file with various **real errors and inconsistencies**;<br>\n",
    "2Ô∏è‚É£ the full Python code to read it correctly with `pandas.read_csv()`;<br>\n",
    "3Ô∏è‚É£ **<u>the Python code to clean the dataframe</u>** ‚ùó\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "145a584a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File 'dati_sporchi.csv' creato.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1. CREATE THE \"DIRTY\" CSV FILE\n",
    "# =========================\n",
    "\n",
    "csv_content = \"\"\"# Example data experted from legacy system\n",
    "# Contain errors in formats, encodings and separators\n",
    "ID; Nome ; Et√† ; Data_nascita ; Stipendio ; Note\n",
    "0; \"Mario Rossi\"; 35 ; 12/05/1989 ; \"2.500,50\" ; \"Lavora a Roma, ottimo rendimento\"\n",
    "1; \"Anna Bianchi\"; 29 ; 01/09/1995 ; \"3.200,00\" ; \"Milano, nuovi progetti\"\n",
    "2; \"Jos√© √Ålvarez\"; 40 ; 15/02/1984 ; \"4.000,75\" ; \"Problemi di encoding √†√®√¨√≤√π\"\n",
    "3; \"Luigi Verdi\"; \"?\" ; 03/11/1990 ; \"2,800.00\" ; \"Errore nei separatori decimali\"\n",
    "4; \"Giulia Rossi\" ; 27 ; 31-08-1997 ; \"3.000,00\" ; \"Riga OK\"\n",
    "5; \"Paolo Bianchi\" ; 33 ; 02/04/1991 ; \"N/A\" ; \"Valore mancante stipendio\"\n",
    "6; \"Marco, Test\"; 38 ; 07/07/1986 ; \"2.900,00\" ; \"Virgola nel nome\"\n",
    "7 \"Sara Neri\" ; 31 ; 10/10/1993 ; \"3.200,00\" ; Riga con separatore mancante\n",
    "8; \"Laura Verdi\"; 25 ; 21/06/1999 ; \"3.000,00\"\n",
    "9; \"Andrea Neri\" ; ; ; ; \"Campi mancanti\"\n",
    "Unnamed: 0; \"Extra colonna inutile\"; ; ; ;\n",
    "\"\"\"\n",
    "\n",
    "with open(\"dati_sporchi.csv\", \"w\", encoding=\"latin1\") as f:\n",
    "    f.write(csv_content)\n",
    "\n",
    "print(\"‚úÖ File 'dati_sporchi.csv' creato.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e2644a",
   "metadata": {},
   "source": [
    "2Ô∏è‚É£ Robust read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e76d6f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns: ['ID', 'Nome ', 'Et√† ', 'Data_nascita ', 'Stipendio ', 'Note'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 2. ROBUST READING\n",
    "# =========================\n",
    "df = pd.read_csv(\n",
    "    \"dati_sporchi.csv\",\n",
    "    sep=\";\",                     # European separator\n",
    "    comment=\"#\",                 # ignore comment lines\n",
    "    engine=\"python\",             # more flexible parser\n",
    "    encoding=\"latin1\",           # handle accents\n",
    "    on_bad_lines=\"skip\",         # skip wrong rows\n",
    "    skip_blank_lines=True,       # ignore empty lines\n",
    "    skipinitialspace=True        # remove spaces after\n",
    ")\n",
    "\n",
    "print(\"Original columns:\", df.columns.tolist(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7985a812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Nome</th>\n",
       "      <th>Et√†</th>\n",
       "      <th>Data_nascita</th>\n",
       "      <th>Stipendio</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Laura Verdi</td>\n",
       "      <td>25.0</td>\n",
       "      <td>21/06/1999</td>\n",
       "      <td>3.000,00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>Extra colonna inutile</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                  Nome   Et√†  Data_nascita  Stipendio   Note\n",
       "0           8            Laura Verdi  25.0   21/06/1999    3.000,00   NaN\n",
       "1  Unnamed: 0  Extra colonna inutile   NaN           NaN        NaN   NaN"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbd5e85",
   "metadata": {},
   "source": [
    "3Ô∏è‚É£  Cleaning  the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64b193c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3.1 COLUMN NAME CLEANING\n",
    "# =========================\n",
    "df.columns = df.columns.str.strip()                               # remove spaces\n",
    "df.columns = df.columns.str.replace(\"√É\", \"√†\", regex=False)        # fix wrong accents\n",
    "df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\", case=False)]  # remove Unnamed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a3a0ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Nome</th>\n",
       "      <th>Et√†</th>\n",
       "      <th>Data_nascita</th>\n",
       "      <th>Stipendio</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Laura Verdi</td>\n",
       "      <td>25.0</td>\n",
       "      <td>21/06/1999</td>\n",
       "      <td>3.000,00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>Extra colonna inutile</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                   Nome   Et√† Data_nascita Stipendio  Note\n",
       "0           8            Laura Verdi  25.0  21/06/1999   3.000,00   NaN\n",
       "1  Unnamed: 0  Extra colonna inutile   NaN          NaN       NaN   NaN"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e35f2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3.2 TYPICAL TRANSFORMATIONS\n",
    "# =========================\n",
    "\n",
    "# -- \"Et√†\" column\n",
    "if \"Et√†\" in df.columns:\n",
    "    df[\"Et√†\"] = pd.to_numeric(df[\"Et√†\"], errors=\"coerce\")\n",
    "\n",
    "# -- \"Stipendio\" column\n",
    "if \"Stipendio\" in df.columns:\n",
    "    df[\"Stipendio\"] = (\n",
    "        df[\"Stipendio\"]\n",
    "        .astype(str)\n",
    "        .str.replace(\".\", \"\", regex=False)  # remove dots (thousands)\n",
    "        .str.replace(\",\", \".\", regex=False) # convert comma to dot\n",
    "    )\n",
    "    df[\"Stipendio\"] = pd.to_numeric(df[\"Stipendio\"], errors=\"coerce\")\n",
    "\n",
    "# -- \"Data_nascita\" column\n",
    "if \"Data_nascita\" in df.columns:\n",
    "    df[\"Data_nascita\"] = pd.to_datetime(df[\"Data_nascita\"], dayfirst=True, errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b07478d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Nome</th>\n",
       "      <th>Et√†</th>\n",
       "      <th>Data_nascita</th>\n",
       "      <th>Stipendio</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Laura Verdi</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1999-06-21</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>Extra colonna inutile</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                   Nome   Et√† Data_nascita  Stipendio  Note\n",
       "0           8            Laura Verdi  25.0   1999-06-21     3000.0   NaN\n",
       "1  Unnamed: 0  Extra colonna inutile   NaN          NaT        NaN   NaN"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "892cbc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File caricato e pulito correttamente!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Nome</th>\n",
       "      <th>Et√†</th>\n",
       "      <th>Data_nascita</th>\n",
       "      <th>Stipendio</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Laura Verdi</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1999-06-21</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>Extra colonna inutile</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                   Nome   Et√† Data_nascita  Stipendio  Note\n",
       "0           8            Laura Verdi  25.0   1999-06-21     3000.0   NaN\n",
       "1  Unnamed: 0  Extra colonna inutile   NaN          NaT        NaN   NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tipi di dato:\n",
      " ID                      object\n",
      "Nome                    object\n",
      "Et√†                    float64\n",
      "Data_nascita    datetime64[ns]\n",
      "Stipendio              float64\n",
      "Note                   float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 3.3 FINAL RESULT\n",
    "# =========================\n",
    "print(\"‚úÖ File caricato e pulito correttamente!\\n\")\n",
    "display(df)\n",
    "print(\"\\nTipi di dato:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d32b52a",
   "metadata": {},
   "source": [
    "# Speeding up loading a large CSV file in pandas\n",
    "\n",
    "How to speed up loading a very large CSV file in pandas with read_csv?<br>\n",
    "Here is the **recommended workflow**:\n",
    "\n",
    "```python\n",
    "    import pandas as pd\n",
    "\n",
    "    # First read, with basic optimizations\n",
    "    df_iter = pd.read_csv(\n",
    "        \"bigdata.csv\",\n",
    "        usecols=[\"A\", \"B\", \"C\"],\n",
    "        dtype={\"A\": \"int32\", \"B\": \"float32\"},\n",
    "        chunksize=1_000_000,\n",
    "        engine=\"pyarrow\"\n",
    "    )\n",
    "\n",
    "    # Incremental processing\n",
    "    df = pd.concat(df_iter)\n",
    "\n",
    "    # Save in optimized format\n",
    "    df.to_parquet(\"bigdata.parquet\")\n",
    "```\n",
    "\n",
    "üëâ Subsequent reads from Parquet or Feather will be **up to 50√ó faster**.\n",
    "\n",
    "**Small practical tricks**\n",
    "\n",
    "* pre-load the files into RAM (e.g. `cat file.csv > /dev/null` on Linux) if the bottleneck is the disk.\n",
    "* if you often work with the same data ‚Üí convert to Parquet right away.\n",
    "* if the file is remote ‚Üí use `storage_options` (e.g. S3 or GDrive) for direct reading.\n",
    "* if you don‚Äôt need the index ‚Üí `index_col=False` or `index_col=None`.\n",
    "* to measure the effect: use `%%time` in Jupyter or VSC or `timeit`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa11ac1",
   "metadata": {},
   "source": [
    "**Here is a practical guide to speed things up** üëá\n",
    "\n",
    "How to speed up pandas.read_csv() on large files\n",
    "\n",
    "1Ô∏è‚É£ **Specify data types (dtype)**<br>\n",
    "If you don‚Äôt declare them, pandas has to ‚Äúguess‚Äù types by scanning rows ‚Üí slow and memory-hungry.\n",
    "\n",
    "```python\n",
    "    dtypes = {\n",
    "        \"id\": \"int32\",\n",
    "        \"categoria\": \"category\",\n",
    "        \"prezzo\": \"float32\",\n",
    "        \"quantita\": \"int16\"\n",
    "    }\n",
    "    df = pd.read_csv(\"file.csv\", dtype=dtypes)\n",
    "```\n",
    "\n",
    "‚úÖ Advantages: much faster loading and a lighter dataframe.\n",
    "\n",
    "2Ô∏è‚É£ **Read only some columns**<br>\n",
    "If you don‚Äôt need them all, declare usecols:\n",
    "\n",
    "```python\n",
    "    df = pd.read_csv(\"file.csv\", usecols=[\"id\", \"prezzo\", \"quantita\"])\n",
    "```\n",
    "\n",
    "‚úÖ You save time and memory.\n",
    "\n",
    "3Ô∏è‚É£ **Disable what you don‚Äôt need**\n",
    "\n",
    "No index:\n",
    "\n",
    "```python\n",
    "    index_col=False\n",
    "```\n",
    "\n",
    "No complex missing-number detection:\n",
    "\n",
    "```python\n",
    "    keep_default_na=False\n",
    "    na_values=[\"\"]\n",
    "```\n",
    "\n",
    "No automatic date conversion:\n",
    "\n",
    "```python\n",
    "    parse_dates=False\n",
    "```\n",
    "\n",
    "‚úÖ All of this avoids expensive inference.\n",
    "\n",
    "4Ô∏è‚É£ **Use chunking (block reading)**\n",
    "\n",
    "If the file is too large for RAM, read it in pieces:\n",
    "\n",
    "```python\n",
    "    chunks = pd.read_csv(\"file.csv\", chunksize=1_000_000)\n",
    "    for chunk in chunks:\n",
    "        # process the chunk\n",
    "        process(chunk)\n",
    "```\n",
    "\n",
    "‚úÖ You keep memory usage low and can process in streaming.\n",
    "\n",
    "5Ô∏è‚É£ **Specify the engine**\n",
    "\n",
    "pandas can use two engines:\n",
    "\n",
    "* `engine='c'` (default, written in C) ‚Üí faster\n",
    "* `engine='python'` ‚Üí more flexible but slower\n",
    "\n",
    "Make sure you use:\n",
    "\n",
    "```python\n",
    "    pd.read_csv(\"file.csv\", engine=\"c\")\n",
    "```\n",
    "\n",
    "6Ô∏è‚É£ **‚ÄúWarm up‚Äù the disk cache**\n",
    "\n",
    "On Linux:\n",
    "\n",
    "```bash\n",
    "    cat file.csv > /dev/null\n",
    "```\n",
    "\n",
    "‚Üí this way the file is already in RAM cache and the next read_csv will be faster.\n",
    "(doesn‚Äôt improve the first read, but the following ones do)\n",
    "\n",
    "7Ô∏è‚É£ **Convert to Parquet as soon as you can**\n",
    "\n",
    "CSV ‚Üí Parquet once, then always work in Parquet:\n",
    "\n",
    "```python\n",
    "    df = pd.read_csv(\"file.csv\")\n",
    "    df.to_parquet(\"file.parquet\")\n",
    "```\n",
    "\n",
    "and later:\n",
    "\n",
    "```python\n",
    "    df = pd.read_parquet(\"file.parquet\")\n",
    "```\n",
    "\n",
    "‚úÖ Often 5‚Äì10√ó faster to read and 3‚Äì4√ó less disk space.\n",
    "\n",
    "8Ô∏è‚É£ Alternative: use Dask or Polars\n",
    "\n",
    "If the file is huge (tens of GB):\n",
    "\n",
    "```python\n",
    "    dask.dataframe.read_csv()\n",
    "```\n",
    "\n",
    "‚Üí parallel reading on multiple cores;\n",
    "\n",
    "```python\n",
    "    polars.read_csv()\n",
    "```\n",
    "\n",
    "‚Üí super-fast Rust engine (even 10√ó faster than pandas).\n",
    "\n",
    "9Ô∏è‚É£ Always measure with %%time\n",
    "\n",
    "In Jupyter or VS Code:\n",
    "\n",
    "```python\n",
    "    %%time\n",
    "    df = pd.read_csv(\"file.csv\", dtype=dtypes, usecols=cols)\n",
    "```\n",
    "\n",
    "Compare various versions and pick the fastest one in your context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee349df8",
   "metadata": {},
   "source": [
    "# Application to the financial file `FinancialIndicators`\n",
    "\n",
    "The *Credit_ISLR* file is very small. Let‚Äôs use the larger csv file *FinancialIndicators.csv*:\n",
    "\n",
    "* about 7000 rows\n",
    "* 73 columns\n",
    "* about 2.4 GB\n",
    "* separator = ',' (US file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9776effd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Total Time:  0.0381169319152832\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Industry Name</th>\n",
       "      <th>SIC</th>\n",
       "      <th>Exchange</th>\n",
       "      <th>Country</th>\n",
       "      <th>Stock Price</th>\n",
       "      <th>% Chg in last year</th>\n",
       "      <th>Trading Volume</th>\n",
       "      <th># of shares outstanding</th>\n",
       "      <th>Market Cap</th>\n",
       "      <th>...</th>\n",
       "      <th>Trailing Net Income</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Intangible Assets/Total Assets</th>\n",
       "      <th>Fixed Assets/Total Assets</th>\n",
       "      <th>Market D/E</th>\n",
       "      <th>Market Debt to Capital</th>\n",
       "      <th>Book Debt to Capital</th>\n",
       "      <th>Dividend Yield</th>\n",
       "      <th>Insider Holdings</th>\n",
       "      <th>Institutional Holdings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Road Inc</td>\n",
       "      <td>Telecom. Services</td>\n",
       "      <td>4810</td>\n",
       "      <td>NDQ</td>\n",
       "      <td>US</td>\n",
       "      <td>5.23</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>236397</td>\n",
       "      <td>54.8</td>\n",
       "      <td>319.60</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-800 Contacts Inc</td>\n",
       "      <td>Medical Supplies</td>\n",
       "      <td>8060</td>\n",
       "      <td>NDQ</td>\n",
       "      <td>US</td>\n",
       "      <td>11.70</td>\n",
       "      <td>0.03</td>\n",
       "      <td>57921</td>\n",
       "      <td>13.3</td>\n",
       "      <td>151.90</td>\n",
       "      <td>...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-800-ATTORNEY Inc</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>2700</td>\n",
       "      <td>NDQ</td>\n",
       "      <td>US</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-800-FLOWERS.COM</td>\n",
       "      <td>Internet</td>\n",
       "      <td>7370</td>\n",
       "      <td>NDQ</td>\n",
       "      <td>US</td>\n",
       "      <td>6.42</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>197850</td>\n",
       "      <td>65.2</td>\n",
       "      <td>422.90</td>\n",
       "      <td>...</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1mage Software Inc</td>\n",
       "      <td>Computer Software/Svcs</td>\n",
       "      <td>3579</td>\n",
       "      <td>NDQ</td>\n",
       "      <td>US</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10200</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.12</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Company Name           Industry Name   SIC Exchange Country  \\\n",
       "0           @Road Inc       Telecom. Services  4810      NDQ      US   \n",
       "1  1-800 Contacts Inc        Medical Supplies  8060      NDQ      US   \n",
       "2  1-800-ATTORNEY Inc              Publishing  2700      NDQ      US   \n",
       "3   1-800-FLOWERS.COM                Internet  7370      NDQ      US   \n",
       "4  1mage Software Inc  Computer Software/Svcs  3579      NDQ      US   \n",
       "\n",
       "   Stock Price  % Chg in last year  Trading Volume  # of shares outstanding  \\\n",
       "0         5.23               -0.02          236397                     54.8   \n",
       "1        11.70                0.03           57921                     13.3   \n",
       "2         1.01                0.00            1438                      0.0   \n",
       "3         6.42               -0.01          197850                     65.2   \n",
       "4         0.01                0.00           10200                      3.3   \n",
       "\n",
       "   Market Cap  ...  Trailing Net Income  Dividends  \\\n",
       "0      319.60  ...                 27.0        0.0   \n",
       "1      151.90  ...                  3.3        0.0   \n",
       "2        0.00  ...                 -1.0        0.0   \n",
       "3      422.90  ...                  7.8        0.0   \n",
       "4        0.03  ...                 -0.8        0.0   \n",
       "\n",
       "   Intangible Assets/Total Assets  Fixed Assets/Total Assets  Market D/E  \\\n",
       "0                            0.00                       0.02        0.00   \n",
       "1                            0.48                       0.19        0.16   \n",
       "2                             NaN                        NaN         NaN   \n",
       "3                            0.31                       0.20        0.01   \n",
       "4                            0.00                       0.00       12.12   \n",
       "\n",
       "   Market Debt to Capital  Book Debt to Capital  Dividend Yield  \\\n",
       "0                    0.00                  0.00             0.0   \n",
       "1                    0.14                  0.29             0.0   \n",
       "2                     NaN                   NaN             0.0   \n",
       "3                    0.01                  0.03             0.0   \n",
       "4                    0.92                   NaN             0.0   \n",
       "\n",
       "   Insider Holdings  Institutional Holdings  \n",
       "0               NaN                    0.23  \n",
       "1               NaN                    0.39  \n",
       "2               NaN                    0.00  \n",
       "3              0.21                    0.88  \n",
       "4               NaN                    0.00  \n",
       "\n",
       "[5 rows x 73 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "df_FI = pd.read_csv('FinancialIndicators.csv')\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print ('Execution Total Time: ', end_time - start_time)\n",
    "\n",
    "df_FI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4220567c",
   "metadata": {},
   "source": [
    " Applichiamo gli argomenti sopra elencati al caricamento di questo file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a1486",
   "metadata": {},
   "source": [
    "# Performance\n",
    "\n",
    "As for the **performance of the various formats** (in terms of memory usage, saving to disk, and opening/reading) see the following useful study.\n",
    "\n",
    "The key message of the study is that:\n",
    "\n",
    "* the CSV format is much better than Excel (not even taken into consideration in the comparison), and it is available in all *data management* environments\n",
    "* for big data (as we will see) the best format is Parquet, especially in terms of memory usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed67b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use example:\n",
    "show_pdf(\"I_O Optimization in Data Projects - by Avi Chawla.pdf\")   # seethe last chapter on reading PDF in VSC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4541a",
   "metadata": {},
   "source": [
    "# Data format for big data\n",
    "\n",
    "Is it possible to load big data with 5M rows in *pandas*? It depends.\n",
    "\n",
    "The short answer is: yes, pandas can also handle 5 million rows, but it depends on what you mean by ‚Äúhandle‚Äù and on how much RAM you have available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ced89d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bbbcdd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r prefix tells Python NOT to interpret \\ as escape\n",
    "path = r'C:\\Users\\Utente\\Desktop\\salvataggi\\SALVATAGGIO DATI\\Documents\\Seminari\\Data Science (corsi)\\Corso Python base\\linkage\\file_csv'\n",
    "\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7743160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba57af37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5749132, 12)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "73610e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>cmp_fname_c1</th>\n",
       "      <th>cmp_fname_c2</th>\n",
       "      <th>cmp_lname_c1</th>\n",
       "      <th>cmp_lname_c2</th>\n",
       "      <th>cmp_sex</th>\n",
       "      <th>cmp_bd</th>\n",
       "      <th>cmp_bm</th>\n",
       "      <th>cmp_by</th>\n",
       "      <th>cmp_plz</th>\n",
       "      <th>is_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37291</td>\n",
       "      <td>53113</td>\n",
       "      <td>0.833333333333333</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39086</td>\n",
       "      <td>47614</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70031</td>\n",
       "      <td>70237</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84795</td>\n",
       "      <td>97439</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36950</td>\n",
       "      <td>42116</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_1   id_2       cmp_fname_c1 cmp_fname_c2  cmp_lname_c1 cmp_lname_c2  \\\n",
       "0  37291  53113  0.833333333333333            ?           1.0            ?   \n",
       "1  39086  47614                  1            ?           1.0            ?   \n",
       "2  70031  70237                  1            ?           1.0            ?   \n",
       "3  84795  97439                  1            ?           1.0            ?   \n",
       "4  36950  42116                  1            ?           1.0            1   \n",
       "\n",
       "   cmp_sex cmp_bd cmp_bm cmp_by cmp_plz  is_match  \n",
       "0        1      1      1      1       0      True  \n",
       "1        1      1      1      1       1      True  \n",
       "2        1      1      1      1       1      True  \n",
       "3        1      1      1      1       1      True  \n",
       "4        1      1      1      1       1      True  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a5811fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>cmp_fname_c1</th>\n",
       "      <th>cmp_fname_c2</th>\n",
       "      <th>cmp_lname_c1</th>\n",
       "      <th>cmp_lname_c2</th>\n",
       "      <th>cmp_sex</th>\n",
       "      <th>cmp_bd</th>\n",
       "      <th>cmp_bm</th>\n",
       "      <th>cmp_by</th>\n",
       "      <th>cmp_plz</th>\n",
       "      <th>is_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5749127</th>\n",
       "      <td>47892</td>\n",
       "      <td>98941</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749128</th>\n",
       "      <td>53346</td>\n",
       "      <td>74894</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749129</th>\n",
       "      <td>18058</td>\n",
       "      <td>99971</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749130</th>\n",
       "      <td>84934</td>\n",
       "      <td>95688</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749131</th>\n",
       "      <td>20985</td>\n",
       "      <td>57829</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id_1   id_2 cmp_fname_c1 cmp_fname_c2  cmp_lname_c1 cmp_lname_c2  \\\n",
       "5749127  47892  98941            1            ?      0.166667            ?   \n",
       "5749128  53346  74894            1            ?      0.222222            ?   \n",
       "5749129  18058  99971            0            ?      1.000000            ?   \n",
       "5749130  84934  95688            1            ?      0.000000            ?   \n",
       "5749131  20985  57829            1            1      0.000000            ?   \n",
       "\n",
       "         cmp_sex cmp_bd cmp_bm cmp_by cmp_plz  is_match  \n",
       "5749127        1      0      0      1       0     False  \n",
       "5749128        1      0      0      1       0     False  \n",
       "5749129        1      0      0      0       0     False  \n",
       "5749130        1      0      1      0       0     False  \n",
       "5749131        1      0      1      1       0     False  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412ba2d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "‚öôÔ∏è **1. It depends on the total size in memory**\n",
    "\n",
    "Pandas works entirely in RAM.\n",
    "\n",
    "Example:\n",
    "\n",
    "* 5 million rows √ó 50 columns\n",
    "* each cell takes ~8 bytes (`float64`)<br>\n",
    "  üëâ $5{,}000{,}000 √ó 50 √ó 8 ‚âà 2 \\text{ GB}$\n",
    "\n",
    "So a 200 MB CSV file can become **2‚Äì3 GB in RAM** once loaded, because of conversion to numeric types, indexes, metadata, etc.\n",
    "\n",
    "If you have a PC with **16 GB of RAM**, it‚Äôs fine; if you have 8 GB, pandas can do it but it will be slow and you may see ‚Äúswap‚Äù or crashes due to lack of memory.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ffe5c2",
   "metadata": {},
   "source": [
    "üß† **2. Operations that pandas handles well even with 5M rows**\n",
    "\n",
    "With ‚Äúdecent‚Äù hardware (modern CPU, 16 GB RAM) pandas comfortably handles:\n",
    "\n",
    "‚úÖ **CSV loading**\n",
    "\n",
    "```python\n",
    "    df = pd.read_csv(\"dati.csv\")\n",
    "```\n",
    "\n",
    "You can also use:\n",
    "\n",
    "* `dtype=` to better type the columns (less RAM);\n",
    "* `usecols=` to read only some columns;\n",
    "* `chunksize=` to read in blocks.\n",
    "\n",
    "‚úÖ **Elementary operations and aggregations**\n",
    "\n",
    "* `df.describe()`, `df.mean()`, `df.groupby(\"col\").agg(...)`\n",
    "* `df.sort_values(\"col\")`\n",
    "* `df.query(\"x > 10 and y < 5\")`\n",
    "* `df.sample(100_000)`<br>\n",
    "  all doable.\n",
    "\n",
    "‚úÖ **Moderate joins and merges**<br>\n",
    "Up to a few million rows per table:\n",
    "\n",
    "```python\n",
    "    pd.merge(df1, df2, on=\"id\", how=\"inner\")\n",
    "```\n",
    "\n",
    "works, but watch out for memory spikes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6e1e52",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "üö´ **Operations that start to become problematic**\n",
    "\n",
    "When the dataset exceeds **5‚Äì10 million rows or goes over 5 GB in RAM**, here‚Äôs what slows down or blows up:\n",
    "\n",
    "‚ùå **multiple sorts or complex sorts**\n",
    "\n",
    "```python\n",
    "    df.sort_values([\"col1\", \"col2\"])\n",
    "```\n",
    "\n",
    "This creates a copy in memory as large as the DataFrame itself.\n",
    "\n",
    "‚ùå **very large merge / join**<br>\n",
    "if the two tables together exceed the available RAM.\n",
    "\n",
    "‚ùå **row-by-row apply / lambda**\n",
    "\n",
    "```python\n",
    "    df.apply(lambda row: f(row.x), axis=1)\n",
    "```\n",
    "\n",
    "Very slow: they are executed in pure Python, not in C.<br>\n",
    "Better to use **vectorized** functions (`np.where`, `pd.Series.map`, etc.).\n",
    "\n",
    "‚ùå **iterative operations**<br>\n",
    "Loops like `for row in df.itertuples()` on millions of rows ‚Üí a disaster!\n",
    "\n",
    "‚ùå **Writing to CSV/parquet**\n",
    "\n",
    "```python\n",
    "    df.to_csv(\"file.csv\")\n",
    "```\n",
    "\n",
    "Not very efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54263ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "‚ö° **Alternatives and strategies**\n",
    "\n",
    "**1. Use *chunking***\n",
    "\n",
    "The following code is **risky**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dati.csv\")\n",
    "df[\"media\"] = df[\"valore\"].mean()\n",
    "```\n",
    "\n",
    "Better to read in blocks and process iteratively:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "chunksize = 500.000   # reads 500 thousand rows at a time\n",
    "risultati = []        # list where to accumulate the results\n",
    "\n",
    "for chunk in pd.read_csv(\"dati.csv\", chunksize=chunksize):\n",
    "    media_chunk = chunk[\"valore\"].mean()       # calculation on the part just read\n",
    "    risultati.append(media_chunk)              # save the partial result\n",
    "\n",
    "# after the loop you can combine the results\n",
    "media_totale = sum(risultati) / len(risultati)\n",
    "print(\"Media complessiva:\", media_totale)\n",
    "\n",
    "```\n",
    "\n",
    "**2. Use the *parquet* data format**<br>\n",
    "See the next chapter.\n",
    "\n",
    "**3. Use `cuDF`**<br>\n",
    "Uses the GPU without changes to Pandas code\n",
    "\n",
    "**4. Use `Spark`**<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd39ba5",
   "metadata": {},
   "source": [
    "# A very, very large CSV file\n",
    "\n",
    "Can we load a CSV file like [this one](https://www.kaggle.com/datasets/aadimator/nyc-realtime-traffic-speed-data/data)? it‚Äôs almost 30GB.<br>\n",
    "*Download* --> *Download dataset as zip (10 GBs)*.<br>\n",
    "Its name is **DOT_Traffic_Speeds_NBE.csv** and it refers to traffic in the city of NewYorl.\n",
    "\n",
    "Let‚Äôs see what it means:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b55a59d",
   "metadata": {},
   "source": [
    "---\n",
    "That Kaggle dataset is an **export** of NYC DOT‚Äôs **real-time traffic speed feed** (‚ÄúDOT Traffic Speeds NBE‚Äù).\n",
    "\n",
    "Each row is a **timestamped observation for one road segment (a ‚Äúlink‚Äù)** with the average **speed** and **travel time** between the segment‚Äôs start and end points. It‚Äôs maintained by NYC DOT and mirrored to Kaggle. ([Kaggle][1])\n",
    "\n",
    "Here‚Äôs what the **fields mean** (names may appear in UPPER_CASE on Kaggle):\n",
    "\n",
    "* **ID / LINK_ID**\n",
    "  Unique identifier of the road **segment** (link) from TRANSCOM (regional traffic consortium). `LINK_ID` is the same as `ID`. Use this as **the key to group or join**. ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **SPEED**\n",
    "  **Average speed (mph)** vehicles traveled **across the whole segment** during the most recent interval. It‚Äôs not spot speed at a point‚Äîthink ‚Äúsegment travel speed.‚Äù Expect missing or zero values at times. ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **TRAVEL_TIME**\n",
    "  **Seconds** the average vehicle took to traverse the segment in that interval. Roughly `TRAVEL_TIME ‚âà segment_length / SPEED` (after converting units). Useful to derive segment length if you have a stable speed sample. ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **STATUS**\n",
    "  Marked as an **artifact / not useful** in NYC DOT‚Äôs own metadata. Most people ignore it. ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **DATA_AS_OF** (a.k.a. `DataAsOf`)\n",
    "  **Timestamp** when data for that link was last received. The feed updates **every few minutes**. Timezone is local (Eastern). Use this for time-series work and resampling. ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **LINK_POINTS**\n",
    "  **Plaintext sequence of lat/long pairs** describing the link geometry (start‚Üíend polyline). **Caveat:** some values are **truncated**‚Äîdon‚Äôt rely on this alone for precise mapping. ([Medium][3])\n",
    "\n",
    "* **ENCODED_POLY_LINE**\n",
    "  **Google-encoded polyline** version of the same geometry. This is usually the better field to decode for maps. (See Google‚Äôs polyline spec referenced by DOT.) ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **ENCODED_POLY_LINE_LVLS**\n",
    "  **Polyline ‚Äúlevels‚Äù** for Google‚Äôs legacy rendering (zoom levels). Often unused in modern tooling but included for completeness. ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **OWNER**\n",
    "  Owner of the detector producing this link‚Äôs data (administrative/operational). ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **TRANSCOM_ID / TRANSCOM_ID (artifact)**\n",
    "  Marked **not useful** by the publisher (redundant with ID). ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **BOROUGH**\n",
    "  NYC borough name (**Brooklyn, Bronx, Manhattan, Queens, Staten Island**). It can be blank for some links. Handy for rollups and filtering. ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "* **LINK_NAME / DESCRIPTION**\n",
    "  Human-readable description of the segment (e.g., ‚ÄúBQE N Atlantic Ave ‚Äî BKN Bridge Manhattan Side‚Äù). Note: **links are one-way**, and not every corridor has both directions in the feed. ([Medium][3])\n",
    "\n",
    "### How to interpret the dataset (what a ‚Äúrow‚Äù is)\n",
    "\n",
    "* One **segment (link)** √ó one **timestamp** ‚Üí **avg speed & travel time** for vehicles that **completed** that segment in the interval. It‚Äôs not per-vehicle data; it‚Äôs an **aggregate**. ([Medium][3])\n",
    "* The feed is **real-time / near-real-time**, updated several times per minute, and covers **major arterials & highways** in NYC. ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "### Practical notes / gotchas\n",
    "\n",
    "* **Geometry:** Prefer **`ENCODED_POLY_LINE`** over `LINK_POINTS`; the latter can be cut off. ([Medium][3])\n",
    "* **Aggregation grain:** Links are **directional**; do not assume two-way coverage for a corridor. ([Medium][3])\n",
    "* **Units:** SPEED = mph, TRAVEL_TIME = seconds; `BOROUGH` is a label, not a geometry. ([Sito Ufficiale di New York City][2])\n",
    "* **Quality:** Occasional zeros/missing values; treat **STATUS** as ignorable. ([Sito Ufficiale di New York City][2])\n",
    "\n",
    "### Typical uses\n",
    "\n",
    "* Compute **p50/p90 speeds** by `BOROUGH`/`LINK_ID`/hour; detect slowdowns and incidents.\n",
    "* Map segments by decoding **`ENCODED_POLY_LINE`**; join with borough boundaries for choropleths.\n",
    "* Derive **segment length** via `median(SPEED)*median(TRAVEL_TIME)` (unit-converted) if length isn‚Äôt separately available.\n",
    "\n",
    "[1]: https://www.kaggle.com/datasets/aadimator/nyc-realtime-traffic-speed-data?utm_source=chatgpt.com \"NYC Real-Time Traffic Speed Data\"\n",
    "[2]: https://www.nyc.gov/html/dot/downloads/pdf/metadata-trafficspeeds.pdf?utm_source=chatgpt.com \"Traffic Sensors Metadata What does this data set describe? ...\"\n",
    "[3]: https://medium.com/qri-io/new-qri-dataset-s-nyc-real-time-traffic-speeds-c3e4c88f44be \"New Qri Dataset(s): NYC Real-Time Traffic Speeds | by Chris Whong | qri.io | Medium\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef77b9ba",
   "metadata": {},
   "source": [
    "**NOTES on this size (about 30 GB)**\n",
    "\n",
    "28 GB ‚âà 28,000,000,000 bytes ‚âà 26 GiB (if we look at it in ‚Äúcomputer‚Äù terms).\n",
    "\n",
    "A CSV file is textual, so it‚Äôs not dense: the same data in **Parquet** would often fit in **3‚Äì6 GB**.\n",
    "\n",
    "In RAM this file, <u>if read with *pandas*</u>, **takes up much more than 28 GB**: pandas in fact has to:\n",
    "\n",
    "* read the text,\n",
    "* parse it,\n",
    "* create the internal arrays.\n",
    "\n",
    "Result: 28 GB of CSV with *pandas* reading can become **50‚Äì80 GB of RAM** without trying too hard (it depends on how many string columns there are, how many NaNs, how long the labels are, etc.).\n",
    "\n",
    "**Is such a size common in a company?**\n",
    "\n",
    "* a single 28 GB CSV is not the norm in ERP/accounting/HR. In these systems we more often find **50‚Äì500 MB, at most 2‚Äì3 GB** when they export ‚Äúeverything‚Äù.\n",
    "* however it is totally normal in contexts like:\n",
    "\n",
    "  * application / web / security logs,\n",
    "  * telco,\n",
    "  * mobility / transportation (like your case),\n",
    "  * IoT,\n",
    "  * data lakes ‚Äúdumped‚Äù from a legacy system.\n",
    "\n",
    "But‚Ä¶ companies almost never want to have a single 28 GB CSV. Usually it‚Äôs a ‚Äúbig dump‚Äù made like that because ‚Äúthat was the export option‚Äù, or because someone did SELECT * over 3 years and sent it to S3. In serious production you split by date or by partition and you go with Parquet.\n",
    "\n",
    "**So: it‚Äôs not strange to have 28 GB of data. It‚Äôs a bit strange to have them all in a single CSV.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc1be43",
   "metadata": {},
   "source": [
    "## Determination of the execution environment\n",
    "\n",
    "The notebook works **indifferently** both on Jupyter Notebook / Visual Studio Code and on Google Colab, as said, <u>except for two aspects</u>:\n",
    "\n",
    "* loading the datasets into the notebook\n",
    "* including the *png* images in the individual cells\n",
    "\n",
    "It is therefore useful to **determine the execution environment**, setting a binary variable (to `True` if we are in Google Colab, to `False` if we are in Jupyter Notebook).\n",
    "\n",
    "The two operations above will be performed differently depending on the value of the binary variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the BINARY TOGGLE:\n",
    "try:\n",
    "    import google.colab                      # package available ONLY in Google Colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "print(\"Running on Colab:\", IN_COLAB)\n",
    "\n",
    "\n",
    "# IMPORT of the necessary packages (needed both in JN and in Colab):\n",
    "from IPython.display import Image, display   # import of embedding and image display packages (one time)\n",
    "                                             # Image and display are both needed in Jupyter Notebook\n",
    "                                             # Google Colab uses only Image\n",
    "import os                                    # needed in Google Colab to see from a code cell\n",
    "                                             # the contents of 'content'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8843a72f",
   "metadata": {},
   "source": [
    "## Loading the big *csv* with Google Colab Pro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc1880a",
   "metadata": {},
   "source": [
    "How much space do we have in the VM *session storage*? (with an **L4-GPU** runtime with 53GB of RAM, 22.5 GB of VRAM and 235.7 GB of disk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd305dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0737e909",
   "metadata": {},
   "source": [
    "`overlay 236G 40G 197G 17% /`: this is **the root** of the Colab environment, i.e. what in Colab we see under `/content`.<br>\n",
    "What really matters is **Avail = 197G**.<br>\n",
    "\n",
    "Translated: we can create new files up to about 197 GB (then of course it also depends on how much is used for notebooks, parquets, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38acb50f",
   "metadata": {},
   "source": [
    "What about the other lines (`/dev/root`, `tmpfs`, `/dev/shm‚Ä¶`) from the previous output?\n",
    "They are things from the Colab container.\n",
    "\n",
    "* `/dev/shm 26G` ‚Üí this is the shared memory (useful for multiprocessing, for example, but not to store 28 GB).\n",
    "* `tmpfs 27G` ‚Üí temporary memories in RAM.\n",
    "* they are not the right place to park a 28 GB CSV!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45344bca",
   "metadata": {},
   "source": [
    "So we have a lot of local space in the VM: **about 197 GB free** ‚Üí so yes, a 28 GB file fits easily.\n",
    "\n",
    "However, uploading such a large file to Google Colab *session storage* is slow and at risk of failure. Much better to put the file on Google Drive and then **mount the disk** (authorizing the Google connection with the usual steps):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b0f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\", force_remount=True)  # argument 'force_remount = True' allows multiple mount "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c08f0b",
   "metadata": {},
   "source": [
    "This way the file stays on Drive, we don‚Äôt have to ‚Äúupload‚Äù it into the session, we read it from there (`/content/drive/MyDrive/.../big.csv`), and Colab doesn‚Äôt have to keep 28 GB on the local disk.\n",
    "\n",
    "‚ö†Ô∏è Warning: reading 28 GB from Drive is slower than reading from local disk. For a huge CSV this can mean **minutes of I/O**.\n",
    "\n",
    "NB. `drive/MyDrive` is now **also available under `content` in the session storage**.\n",
    "\n",
    "In fact we can see it again by rerunning the `!df -h` command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e2023",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1757e22e",
   "metadata": {},
   "source": [
    "<u>Question</u>: but do ‚Äúoverlay‚Äù and ‚Äúdrive‚Äù have the same size (236G) ü§î?\n",
    "\n",
    "Yes, it looks like that because Colab often **shows the same backing storage or in any case two volumes with a similar size**. What matters to us is: we have ~200 GB free locally and ~187 GB free on Drive ‚Üí both > 28 GB ‚Üí we‚Äôre safe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f33105",
   "metadata": {},
   "source": [
    "First of all, as a check, let‚Äôs **list the files on the drive**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e1ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    import os\n",
    "    base = \"/content/drive/MyDrive\"\n",
    "\n",
    "    for name in os.listdir(base):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddbc4a2",
   "metadata": {},
   "source": [
    "If we also want to see the **size** of the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a980818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    for name in os.listdir(base):\n",
    "        path = os.path.join(base, name)\n",
    "        if os.path.isfile(path):\n",
    "            print(\"FILE \", name, os.path.getsize(path))\n",
    "        else:\n",
    "            print(\"DIR  \", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d9c09",
   "metadata": {},
   "source": [
    "Let‚Äôs check the size of the `DOT_Traffic_Speeds_NBE.csv` file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f7c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !ls -lh /content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f69a52",
   "metadata": {},
   "source": [
    "Now we are ready to read the csv file with pandas (`pd.read_csv`) with **cuDF**.\n",
    "\n",
    "---\n",
    "\n",
    "**`cudf` is a CUDA-accelerated version of pandas.**\n",
    "\n",
    "See [this post with video](https://www.linkedin.com/feed/update/urn:li:activity:7173982894921519105?utm_source=share&utm_medium=member_desktop) and [this article](https://www.blog.dailydoseofds.com/p/nvidias-latest-update-can-make-your).\n",
    "\n",
    "*Steve Nouri*:<br>\n",
    "**NVIDIA made Pandas 50x faster with No code change!**<br>\n",
    "\n",
    "You simply need to do this:<br>\n",
    "\n",
    "```python\n",
    "    %load_ext cudf.pandas\n",
    "    import pandas as pd\n",
    "```\n",
    "\n",
    "Now `cuDF` will be **integrated directly into Google Colab** (you obviously need to have a **GPU-enabled runtime**).\n",
    "\n",
    "Also take a look here [https://bit.ly/3XX9pgm](https://bit.ly/3XX9pgm).\n",
    "\n",
    "See also [this excellent video](https://www.youtube.com/watch?v=8X_IaCNpo7E) translated into Italian.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    %load_ext cudf.pandas\n",
    "    import pandas as pd\n",
    "    import cudf\n",
    "else:\n",
    "    import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398def29",
   "metadata": {},
   "source": [
    "The following **reading** loop takes **610 seconds** on L4-GPU.<br>\n",
    "**The general idea**:\n",
    "\n",
    "* we have a huge CSV **in blocks of 250k rows** with *pandas*,\n",
    "* we read it piece by piece (`chunksize`),\n",
    "* each piece we move to the GPU with cuDF,\n",
    "* inside here (on the GPU) we could filter the rows or transform the file,\n",
    "* if we want we save it right away or we accumulate it in a list,\n",
    "* at the end we can concatenate everything on the GPU (only if it fits in VRAM),\n",
    "* the whole code block below is timed to know how long it takes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e5157",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK = 200_000_000  # 200 MB at a time\n",
    "offset = 0\n",
    "part = 0\n",
    "\n",
    "import time\n",
    "\n",
    "# timer start-up\n",
    "start_time = time.time()\n",
    "\n",
    "# // code to be measured\n",
    "\n",
    "# 1. file path\n",
    "CSV_PATH = \"/content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv\"\n",
    "\n",
    "# 2. chunk size (start low)\n",
    "ROWS_PER_CHUNK = 250_000   # ~200-300 MB depending on the columns\n",
    "\n",
    "# 3. if you want to accumulate the chunks in GPU (only if we can keep them)\n",
    "gdf_parts = []\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(CSV_PATH, chunksize=ROWS_PER_CHUNK)):\n",
    "    print(f\"[pandas] letto chunk {i} con {len(chunk)} righe\")\n",
    "\n",
    "    # 4. convert the pandas chunk -> cuDF (here we use the GPU)\n",
    "    gdf_chunk = cudf.from_pandas(chunk)\n",
    "    print(f\"[cuDF] chunk {i} in GPU con shape {gdf_chunk.shape}\")\n",
    "\n",
    "    # ‚¨áÔ∏è here we can do our operations in GPU\n",
    "    # example: filter\n",
    "    # gdf_chunk = gdf_chunk[gdf_chunk[\"BOROUGH\"] == \"MANHATTAN\"]\n",
    "\n",
    "    # example: we save immediately to parquet so as not to keep everything in GPU\n",
    "    # gdf_chunk.to_parquet(f\"/content/out_part_{i:04d}.parquet\")\n",
    "\n",
    "    # if instead we keep them to merge later:\n",
    "    gdf_parts.append(gdf_chunk)\n",
    "\n",
    "# 5. (optional) we merge all the GPU pieces into a single cuDF DataFrame\n",
    "# ‚ö†Ô∏è do it only if it fits in VRAM\n",
    "if gdf_parts:\n",
    "    gdf_all = cudf.concat(gdf_parts, ignore_index=True)\n",
    "    print(gdf_all.shape)\n",
    "\n",
    "# // end of code to be measured\n",
    "\n",
    "# end timer and print\n",
    "end_time = time.time()\n",
    "print ('Execution Total Time: ', end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3659feb",
   "metadata": {},
   "source": [
    "Let‚Äôs go line by line to see what the code in the previous cell does.\n",
    "\n",
    "**Timer**\n",
    "\n",
    "```python\n",
    "import time\n",
    "start_time = time.time()\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "* we import the `time` package\n",
    "* we store the start instant\n",
    "* at the end we store the end instant to say ‚Äúthis whole run took X seconds‚Äù.\n",
    "\n",
    "**Reading parameters**\n",
    "\n",
    "```python\n",
    "    CSV_PATH = \"/content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv\"\n",
    "    ROWS_PER_CHUNK = 250_000\n",
    "    gdf_parts = []\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "* `CSV_PATH`: where the file is\n",
    "* `ROWS_PER_CHUNK = 250_000`: instead of reading 65 million rows in one shot, we read **250k at a time**. <u>This is the right way when the CSV is huge</u>.\n",
    "* `gdf_parts = []`: the empty list where we put the cuDF DataFrames as we convert them.\n",
    "\n",
    "**Chunked reading with pandas**\n",
    "\n",
    "```python\n",
    "    for i, chunk in enumerate(pd.read_csv(CSV_PATH, chunksize=ROWS_PER_CHUNK)):\n",
    "        print(f\"[pandas] letto chunk {i} con {len(chunk)} righe\")\n",
    "```\n",
    "\n",
    "Here something important happens:\n",
    "\n",
    "* we are not reading directly with cuDF.\n",
    "* we are using *pandas* with the option `chunksize=...` ‚Üí this makes the `read_csv` function become a **generator**: each loop of the `for` returns **only a piece of the file**.\n",
    "* `i` is the index of the `chunk` (0, 1, 2, ‚Ä¶).\n",
    "* `chunk` is a **pandas DataFrame** with ~250k rows.\n",
    "* we print how many rows we have read to see that we are progressing.\n",
    "\n",
    "Why do we do it like this? Because often pandas is more ‚Äútolerant‚Äù and chunked reading is already ready, and then we use cuDF only for the processing.\n",
    "\n",
    "> <u>Note on `enumerate`</u><br>\n",
    "> `enumerate` is a Python function that takes something **iterable** (list, generator, in this case the chunks of `read_csv`) and returns pairs:\n",
    ">\n",
    "> * the loop number (0, 1, 2, 3‚Ä¶)\n",
    "> * the actual element of the iteration<br>\n",
    ">\n",
    "> That is, with `enumerate(...)` we are saying: ‚Äúfor each piece you give us, also give us the index of the piece‚Äù.\n",
    ">\n",
    "> * `i` ‚Üí 0 for the first chunk, 1 for the second, 2 for the third‚Ä¶\n",
    "> * `chunk` ‚Üí **the pandas DataFrame with those 250,000 rows**<br>\n",
    ">\n",
    "> This way we can print:\n",
    ">\n",
    "> ```python\n",
    ">     print(f\"[pandas] letto chunk {i} ...\")\n",
    "> ```\n",
    ">\n",
    "> and we know where we are.\n",
    "\n",
    "**pandas ‚Üí cuDF conversion**\n",
    "\n",
    "```python\n",
    "    gdf_chunk = cudf.from_pandas(chunk)\n",
    "    print(f\"[cuDF] chunk {i} in GPU con shape {gdf_chunk.shape}\")\n",
    "```\n",
    "\n",
    "* we take the piece read in RAM (pandas),\n",
    "* we move it to the GPU converting it to a cuDF DataFrame.\n",
    "* we print the `shape` (the dimensions) just for checking.\n",
    "\n",
    "This is the point where we ‚Äúuse the GPU‚Äù.\n",
    "\n",
    "**Point where to do the (OPTIONAL) processing**\n",
    "\n",
    "```python\n",
    "    gdf_chunk = gdf_chunk[gdf_chunk[\"BOROUGH\"] == \"MANHATTAN\"]\n",
    "    gdf_chunk.to_parquet(...)\n",
    "```\n",
    "\n",
    "It‚Äôs the pattern ‚ÄúI read big CSVs ‚Üí I transform them in pieces ‚Üí I save them in a more convenient format‚Äù.\n",
    "\n",
    "**Accumulating the pieces**\n",
    "\n",
    "```python\n",
    "    gdf_parts.append(gdf_chunk)\n",
    "```\n",
    "\n",
    "Instead of saving right away, we put the piece in a list.<br>\n",
    "This is handy if:\n",
    "\n",
    "* we want to do a concatenation at the end,\n",
    "* or if the chunks are few and fit in memory.\n",
    "\n",
    "It is risky if the CSV is really huge and the GPU has little VRAM.\n",
    "\n",
    "**Final concatenation (optional)**\n",
    "\n",
    "```python\n",
    "    if gdf_parts:\n",
    "        gdf_all = cudf.concat(gdf_parts, ignore_index=True)\n",
    "        print(gdf_all.shape)\n",
    "```\n",
    "\n",
    "If we have at least one piece, we merge them all into a single cuDF DataFrame.\n",
    "\n",
    "```python\n",
    "    ignore_index=True because after a concat the old indexes don‚Äôt make sense.\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è we rightly put the comment: ‚Äúdo it only if it fits in VRAM‚Äù.<br>\n",
    "This is the part that often, on huge datasets, is not done, and we stop at saving per chunk.\n",
    "\n",
    "**End timer**\n",
    "\n",
    "```python\n",
    "    end_time = time.time()\n",
    "    print ('Tempo totale di esecuzione: ', end_time - start_time)\n",
    "```\n",
    "\n",
    "We print how long the whole run took: chunked reading + conversion + eventual concat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6e7f2e",
   "metadata": {},
   "source": [
    "The following version has another approach: ‚Äúdon‚Äôt accumulate, process and then discard‚Äù, it‚Äôs even safer (**770 seconds on L4-GPU**) üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af39254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# timer start-up\n",
    "start_time = time.time()\n",
    "\n",
    "CSV_PATH = \"/content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv\"\n",
    "ROWS_PER_CHUNK = 250_000\n",
    "\n",
    "# creation of the directory on session storage\n",
    "out_dir = \"/content/parquet_parts\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(CSV_PATH, chunksize=ROWS_PER_CHUNK)):\n",
    "    gdf = cudf.from_pandas(chunk)\n",
    "    # do your operations here...\n",
    "    # and then do NOT keep it in memory\n",
    "    gdf.to_parquet(f\"{out_dir}/part_{i:04d}.parquet\")\n",
    "\n",
    "# end timer and print\n",
    "end_time = time.time()\n",
    "print ('Total Execution Time: ', end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec45916",
   "metadata": {},
   "source": [
    "`chunk` ‚Üí is the **pandas dataframe** that comes from the CSV.\n",
    "\n",
    "`gdf` ‚Üí is the **cuDF dataframe** (the ‚Äúreal‚Äù one we work on in the GPU).\n",
    "\n",
    "do we want to use pandas? ‚Üí we use `chunk`<br>\n",
    "do we want to use cuDF ‚Üí we use `gdf` (this is ‚Äúthe dataframe‚Äù we care about for the GPU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc12be",
   "metadata": {},
   "source": [
    "We want to count the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7717b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !wc -l /content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68b2fb0",
   "metadata": {},
   "source": [
    "64,914,524 rows!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e975494",
   "metadata": {},
   "source": [
    "Knowing the number of rows of the file we can now write the most effective file-reading code (with `chunk` = 1,000,000):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63697f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    path = \"/content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv\"\n",
    "\n",
    "    ROWS_PER_CHUNK = 1_000_000   # 1 million: ~65 cycles\n",
    "\n",
    "    total = 0\n",
    "    for i, chunk in enumerate(pd.read_csv(path, chunksize=ROWS_PER_CHUNK)):\n",
    "        gdf = cudf.from_pandas(chunk)\n",
    "        total += len(gdf)\n",
    "        print(f\"chunk {i:03d} -> {len(gdf)} righe, totale: {total}\")\n",
    "\n",
    "    print(\"‚úÖ totale letto:\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3babcf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk.shape # the LAST chunk in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be612474",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.shape # l'ultimo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14cb021",
   "metadata": {},
   "source": [
    "First we read in chunks.<br>\n",
    "Now let‚Äôs TRY to read the whole file **into a single dataframe in memory** (in 7 minutes on L4-GPU). There is the risk that **RAM blows up**, but here we have 53GB!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005119b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    dfs = []\n",
    "    for chunk in pd.read_csv(path, chunksize=1_000_000):\n",
    "        dfs.append(chunk)\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e447f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f96af",
   "metadata": {},
   "source": [
    "If instead we wanted to read the first chunk again, we would do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ff2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "path = \"/content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv\"\n",
    "\n",
    "reader = pd.read_csv(path, chunksize=1_000_000)  # creates the iterator\n",
    "first_chunk = next(reader)                       # takes ONLY the first piece\n",
    "\n",
    "first_chunk.head()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b487a302",
   "metadata": {},
   "source": [
    "# The *parquet* format\n",
    "\n",
    "Useremo la serie storica `usa_stocks_30m.parquet`: √® una serie OHLCV di 514 titoli del Nasdaq del NYSE (dal 1998 al 2024).\n",
    "\n",
    "Il dataset con cui lavoreremo √® un sottoinsieme del dataset [**USA 514 Stocks Prices NASDAQ NYSE**](https://www.kaggle.com/datasets/olegshpagin/usa-stocks-prices-ohlcv/data), anche disponibile su [Kaggle](https://www.kaggle.com/datasets), composto da circa **36 milioni** di elementi.\n",
    "\n",
    "Scarichiamo il dataset NON da Kaggle ma dal \"Public Google Cloud Storage bucket\" di NVIDIA, per garantire velocit√† di download maggiori.\n",
    "\n",
    "Il \"Public Google Cloud Storage bucket\" di NVIDIA √® uno spazio online dove NVIDIA mette a disposizione file pubblici (come dataset, modelli, esempi di codice) che chiunque pu√≤ scaricare.\n",
    "√à un po‚Äô come un grande armadio digitale aperto a tutti, ospitato su Google Cloud.\n",
    "\n",
    "Questo download richiede **circa 60 secondi**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2bdfd95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usa_stocks_30m.parquet gi√† presente.\n"
     ]
    }
   ],
   "source": [
    "# Download of the big time series\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"usa_stocks_30m.parquet\"\n",
    "url = \"https://storage.googleapis.com/rapidsai/colab-data/usa_stocks_30m.parquet\"\n",
    "\n",
    "if not os.path.isfile(file_path):\n",
    "    print(f\"Scarico il file {file_path}...\")\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "    print(\"Download completato.\")\n",
    "else:\n",
    "    print(f\"{file_path} gi√† presente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f684f4",
   "metadata": {},
   "source": [
    "**The file `usa_stocks_30m.parquet`**<br>\n",
    "\n",
    "The `usa_stocks_30m.parquet` file is a dataset provided by the RAPIDS (NVIDIA) team to give examples of analysis on big financial time series with GPU-accelerated libraries (like `cuDF`).\n",
    "\n",
    "üìå Basically:\n",
    "\n",
    "* It is a **Parquet format** file (columnar, compressed, very efficient for big data).\n",
    "* It contains **US stock price** data (listed stocks) recorded at a **30-minute frequency**.\n",
    "* It is meant for demos: time-series analysis, manipulation with pandas/cuDF, CPU vs GPU benchmarks.\n",
    "\n",
    "üìä It typically includes:\n",
    "\n",
    "* ticker ‚Üí the stock symbol (e.g. AAPL, MSFT).\n",
    "* timestamp ‚Üí the date/time of the observation (every 30 min).\n",
    "* open, high, low, close, volume (OHLCV) ‚Üí classic trading fields.\n",
    "\n",
    "üìê Indicative sizes:\n",
    "\n",
    "* About **36 million rows**,\n",
    "* Size **~ 600‚Äì700 MB** in Parquet format,\n",
    "* **If converted to CSV it would become much heavier (even several GB)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8a92c2",
   "metadata": {},
   "source": [
    "üëâ The Parquet format **is much more efficient than CSV**:\n",
    "\n",
    "* it is binary and compressed (takes up less space);\n",
    "* it is columnar ‚Üí pandas can read only the needed columns;\n",
    "* it preserves data types (no inference every time).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2cb46435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999-11-18 17:00:00</td>\n",
       "      <td>45.56</td>\n",
       "      <td>50.00</td>\n",
       "      <td>45.50</td>\n",
       "      <td>46.00</td>\n",
       "      <td>9275000</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999-11-18 17:30:00</td>\n",
       "      <td>46.00</td>\n",
       "      <td>47.69</td>\n",
       "      <td>45.82</td>\n",
       "      <td>46.57</td>\n",
       "      <td>3200900</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999-11-18 18:00:00</td>\n",
       "      <td>46.56</td>\n",
       "      <td>46.63</td>\n",
       "      <td>41.00</td>\n",
       "      <td>41.00</td>\n",
       "      <td>3830500</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999-11-18 18:30:00</td>\n",
       "      <td>41.00</td>\n",
       "      <td>43.38</td>\n",
       "      <td>40.37</td>\n",
       "      <td>42.38</td>\n",
       "      <td>3688600</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999-11-18 19:00:00</td>\n",
       "      <td>42.31</td>\n",
       "      <td>42.44</td>\n",
       "      <td>41.56</td>\n",
       "      <td>41.69</td>\n",
       "      <td>1584300</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime   open   high    low  close   volume ticker\n",
       "0 1999-11-18 17:00:00  45.56  50.00  45.50  46.00  9275000      A\n",
       "1 1999-11-18 17:30:00  46.00  47.69  45.82  46.57  3200900      A\n",
       "2 1999-11-18 18:00:00  46.56  46.63  41.00  41.00  3830500      A\n",
       "3 1999-11-18 18:30:00  41.00  43.38  40.37  42.38  3688600      A\n",
       "4 1999-11-18 19:00:00  42.31  42.44  41.56  41.69  1584300      A"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"usa_stocks_30m.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "406ca1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36087094, 7)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ddcdb8",
   "metadata": {},
   "source": [
    "# Pickle and Feather: little used?\n",
    "\n",
    "* `Parquet`: default for large tabular data ‚Üí columnar, compressed, schema, partitionable, cross-language (Spark, DuckDB, BigQuery, etc.).\n",
    "* `CSV`: human/‚Äúuniversal‚Äù exchange, but heavy and slow.\n",
    "* `Feather (Arrow IPC file)`: super-fast for temporary passes between Python/R or for local caching; fewer features (no partitioning, no append, little ‚Äúschema evolution‚Äù).\n",
    "* `Pickle`: Python only, unsafe to load if you don‚Äôt trust the source, fragile across versions; good for Python objects (sklearn models, lists), not for long-lived tabular ‚Äúdata‚Äù.\n",
    "\n",
    "**Are `Feather` and `Pickle` ‚Äúlittle used‚Äù?**\n",
    "\n",
    "`Pickle`\n",
    "\n",
    "* üîí Security: pickle.load can execute code ‚Üí not recommended for shared files.\n",
    "* üß¨ Low portability: Python-only and sometimes tied to versions/libraries.\n",
    "* üì¶ Tabular data: it‚Äôs neither columnar nor efficiently compressed; no predicate pushdown.\n",
    "\n",
    "`Feather`\n",
    "\n",
    "* üß© Niche use: great for Python‚ÜîR/Arrow interop and ‚Äúfast‚Äù cache, but‚Ä¶\n",
    "* üß± Fewer features: no partitioning, no append, worse handling of evolving ‚Äúdata lakes‚Äù.\n",
    "* üåç Ecosystems: Spark/DB/Cloud tools push Parquet as the de facto standard.\n",
    "\n",
    "**When they make sense**:\n",
    "\n",
    "`Pickle`\n",
    "\n",
    "* Quick snapshots of Python objects (e.g. sklearn pipelines) for internal and controlled use.\n",
    "* Often better alternatives: joblib.dump for sklearn models; ONNX / PMML for portability; for tabular data ‚Üí Parquet.\n",
    "\n",
    "`Feather`\n",
    "\n",
    "* ‚ÄúFast I/O‚Äù local cache (e.g. intermediate save in a notebook).\n",
    "* Fast Python‚ÜîR exchange (Arrow): pyarrow.feather / arrow::write_feather in R.\n",
    "\n",
    "If you want maximum read/write speed on a single file and you don‚Äôt need to partition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692a7dec",
   "metadata": {},
   "source": [
    "# JSON files\n",
    "\n",
    "A **non-tabular** but **frequently used** file format in Python is JSON.\n",
    "\n",
    "JSON files (extension *.json*) are one of the most used formats today to exchange data between applications, **especially on the web and in API contexts**.\n",
    "\n",
    "üí° **In short**\n",
    "\n",
    "* JSON stands for **JavaScript Object Notation**<br>\n",
    "  It is a **textual** format, <u>human readable and easily interpretable by programs</u>, born from JavaScript but today used in **practically all languages** (Python, Java, C#, PHP, etc.).\n",
    "\n",
    "üì¶ **Structure of a JSON file**\n",
    "\n",
    "A JSON file contains data organized as [**key‚Äìvalue pairs**](https://en.wikipedia.org/wiki/Name%E2%80%93value_pair), for example:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"nome\": \"Antonio\",\n",
    "  \"eta\": 45,\n",
    "  \"iscritti\": [\"Mario\", \"Lucia\", \"Giorgio\"],\n",
    "  \"attivo\": true,\n",
    "  \"dettagli\": {\n",
    "    \"ruolo\": \"Analista\",\n",
    "    \"azienda\": \"ACI\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "üëÜ This is a JSON object, which contains:\n",
    "\n",
    "* strings (\"`Antonio`\", \"`Analista`\")\n",
    "* numbers (`45`)\n",
    "* booleans (`true`)\n",
    "* lists (arrays) ([\"`Mario`\", \"`Lucia`\", \"`Giorgio`\"])\n",
    "* nested objects (\"`dettagli\": {...}`)\n",
    "\n",
    "**In Python**<br>\n",
    "You can read or write JSON files easily with the json module:\n",
    "\n",
    "```python\n",
    "    import json\n",
    "\n",
    "    # Reading\n",
    "    with open(\"dati.json\", \"r\") as f:\n",
    "        dati = json.load(f)\n",
    "    print(dati[\"nome\"])  # -> Antonio\n",
    "\n",
    "    # Writing\n",
    "    nuovi_dati = {\"linguaggio\": \"Python\", \"versione\": 3.12}\n",
    "    with open(\"config.json\", \"w\") as f:\n",
    "        json.dump(nuovi_dati, f, indent=4)\n",
    "```\n",
    "\n",
    "**Where is JSON used?**\n",
    "\n",
    "* REST APIs (almost all modern APIs use JSON to exchange data)\n",
    "* Configurations (e.g. package.json in Node.js)\n",
    "* NoSQL databases like MongoDB (which uses BSON, a binary version of JSON)\n",
    "* Web and mobile applications to pass data between frontend and backend\n",
    "\n",
    "üÜö Quick comparison JSON vs CSV vs XML<br>\n",
    "| Quick comparison |          |                     |                                   |\n",
    "| ---------------- | -------- | ------------------- | --------------------------------- |\n",
    "| **Format**       | **Type** | **Pro**             | **Con**                           |\n",
    "| JSON             | Text     | Readable, universal | Not suitable for binary data      |\n",
    "| CSV              | Text     | Simple for tables   | Does not handle nested structures |\n",
    "| XML              | Text     | Very flexible       | More verbose and heavier          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9931522",
   "metadata": {},
   "source": [
    "## Conversion of a CSV file or a Python dictionary to JSON and vice versa (i.e. full import/export)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e447d51",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ **Python dictionary ‚Üí JSON (write)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5ae3c5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ JSON file created!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "dati = {\n",
    "    \"nome\": \"Antonio\",\n",
    "    \"eta\": 45,\n",
    "    \"linguaggi\": [\"Python\", \"SQL\", \"R\"],\n",
    "    \"attivo\": True\n",
    "}\n",
    "\n",
    "# writes on the JSON file\n",
    "with open(\"dati.json\", \"w\") as f:\n",
    "    json.dump(dati, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ JSON file created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ddb7fb",
   "metadata": {},
   "source": [
    "**Result** (`dati.json`):\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"nome\": \"Antonio\",\n",
    "    \"eta\": 45,\n",
    "    \"linguaggi\": [\"Python\", \"SQL\", \"R\"],\n",
    "    \"attivo\": true\n",
    "}\n",
    "```\n",
    "\n",
    "üî∏ `indent=4` ‚Üí makes the file readable<br>\n",
    "üî∏ `ensure_ascii=False` ‚Üí keeps accented characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7e14ab",
   "metadata": {},
   "source": [
    "2Ô∏è‚É£ **JSON ‚Üí Python dictionary (read)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fcd2fdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antonio\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"dati.json\", \"r\") as f:\n",
    "    dati_letti = json.load(f)\n",
    "\n",
    "print(dati_letti[\"nome\"])     # Antonio\n",
    "print(type(dati_letti))       # dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf969b6",
   "metadata": {},
   "source": [
    "3Ô∏è‚É£ **CSV ‚Üí JSON**\n",
    "\n",
    "Let‚Äôs use the `Credit_ISLR.csv` file.<br>\n",
    "Let‚Äôs convert it to JSON:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c6723027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CSV converted to JSON!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "with open(\"Credit_ISLR.csv\", \"r\") as f_csv:\n",
    "    reader = csv.DictReader(f_csv)\n",
    "    dati = list(reader)\n",
    "\n",
    "with open(\"Credit_ISLR.json\", \"w\") as f_json:\n",
    "    json.dump(dati, f_json, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ CSV converted to JSON!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de30abb3",
   "metadata": {},
   "source": [
    "4Ô∏è‚É£ **JSON ‚Üí CSV**\n",
    "\n",
    "Now let‚Äôs do the reverse:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "96d547be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ JSON converted to CSV!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "with open(\"Credit_ISLR.json\", \"r\") as f_json:\n",
    "    dati = json.load(f_json)\n",
    "\n",
    "with open(\"Credit_ISLR_out.csv\", \"w\", newline=\"\") as f_csv:\n",
    "    writer = csv.DictWriter(f_csv, fieldnames=dati[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(dati)\n",
    "\n",
    "print(\"‚úÖ JSON converted to CSV!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c857ef",
   "metadata": {},
   "source": [
    "| üí¨ In summary | Library       | Key method                         |\n",
    "| ------------- | ------------- | ---------------------------------- |\n",
    "| dict ‚Üí JSON   | `json`        | `json.dump()`                      |\n",
    "| JSON ‚Üí dict   | `json`        | `json.load()`                      |\n",
    "| CSV ‚Üí JSON    | `csv`, `json` | `csv.DictReader()` + `json.dump()` |\n",
    "| JSON ‚Üí CSV    | `json`, `csv` | `json.load()` + `csv.DictWriter()` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c5f09",
   "metadata": {},
   "source": [
    "# Technical note on PDFs\n",
    "\n",
    "In VSC the rendering of PDF files is different from that of Jupyter Notebook/Lab and from that of Google Colab.<br>\n",
    "The following `show_pdf` function detects which IDE is active and ‚Äúrenders‚Äù the PDF differently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "683a04f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pdf(pdf_path, width=1000, height=600):\n",
    "    \"\"\"\n",
    "    Shows a PDF in the most appropriate way for the current environment:\n",
    "    - In Jupyter: displays inline with IFrame.\n",
    "    - In Colab: uses IFrame (handles uploaded files well).\n",
    "    - In VS Code or other environments: opens in the default browser.\n",
    "    \"\"\"\n",
    "    import os, webbrowser, sys\n",
    "    from pathlib import Path\n",
    "\n",
    "    pdf_path = Path(pdf_path)\n",
    "    if not pdf_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {pdf_path}\")\n",
    "\n",
    "    # Detect environment\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "    except NameError:\n",
    "        shell = None\n",
    "\n",
    "    if shell == 'ZMQInteractiveShell':  # Jupyter or Colab\n",
    "        from IPython.display import IFrame, display\n",
    "        display(IFrame(str(pdf_path), width=width, height=height))\n",
    "    elif \"vscode\" in sys.executable.lower() or \"vscode\" in os.getcwd().lower():\n",
    "        # VS Code environment ‚Üí open in browser\n",
    "        webbrowser.open(pdf_path.resolve().as_uri())\n",
    "        print(f\"üìÇ PDF opened in browser: {pdf_path}\")\n",
    "    else:\n",
    "        # Other environments (terminals, scripts)\n",
    "        webbrowser.open(pdf_path.resolve().as_uri())\n",
    "        print(f\"üìÇ PDF opened in browser: {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec9a3c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82ee46e9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
